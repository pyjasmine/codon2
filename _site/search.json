[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! my name is Jasmine. I come from a background of Life Sciences, with an undergraduate degree in Pharmacology and a Masters of Research in Neuroscience. I have worked in the Biotech industry for a couple of years working with induced Pluripotent Stem Cells R&D. I gained an interest in data analysis from working on an collaborative RNA sequencing project, I was curious as to how so much data could be analysed quite fast and presented in visualisations that helped us make decisions reguarding cells. From there I went on to do a summer course in Data Science, learning Python coding, data analysis, and machine learning."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "codon classification project",
    "section": "",
    "text": "01 - Introduction\n\n\n\n\n\n\n\n\n\n\n02 - Data Cleaning\n\n\n\n\n\n\n\n\n\n\n03 - Exploratory Data Analysis\n\n\n\n\n\n\n\n\n\n\n04 - DNAtype Classification\n\n\n\n\n\n\n\n\n\n\n05 - Kingdom Classification\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-08-01-Introduction .html",
    "href": "posts/2022-08-01-Introduction .html",
    "title": "codon2",
    "section": "",
    "text": "Project: Predicting Kingdom and RNA Type Using Codon Frequency\n\nby: Jasmine Marzouk\n\n\n\nNotebook: Introduction\n\nWhat is a codon?\n\nDNA and RNA are genetic material that contain the code to encode for proteins which are important to the various functionings of the body. Proteins are made up of sequences of amino acids, these amino acids are encoded for by codons.\nCodons are a sequence of three nucleotides from either of the four nucleotides that make up RNA U: Uracil, A: Adenine, G: Guanine, and C: Cytosine. There are 64 possible combinations of these codons, but they only encode for ~20 amino acids. This creates a redundency in the genetic code, this gives rise to Codon Bias.\nCodon bias is a term used to refer to the fact that for a known sequence of amino acids, different species will have a bias towards certain codons to encode a certain amino acid.\n\nWhy is this important?\n\nProtein synthesis is an ever important part of the Biotechnology industry, especially for drug discovery. So it is important to synthesise proteins that fold correctly to ensure, for example, inhibitor affinity to its target. This is essential to discovering and making new therapies to manage and treat various diseases.\nMany scientists will use species of bacteria to synthesise proteins for experimentation. It is useful to understand the codon bias of the species to ensure that the plasmids transferred into the bacteria contain the sequence that is faithful to that of the host’s translation and protein synthesis machinery, this ensures better folding and higher efficiency.\n\nProblem Framing:\n\nThe aim of this project is to use codon bias, rather codon frequency to determine the classification by Kingdom or DNAtype of a species.\n\n\nModelling Method:\n\nfrom IPython.display import Image\n# you can find this image in the zip folder\nImage('../notebooks/modelling_plan.png')\n# change the path accordingly.\n\n\n\n\nAs this project was attempted before by a research group who achieved an accuracy of 0.996 for their final models. I attempted to improve upon that score by employing a PCA dimensionality reduction on the data and then fit appropriate models.\n\n\nResults:\n\nKingdom classification results:\n\n\n# you can find this image in the zip folder\nImage('../notebooks/kingdom_modelling.png')\n# change the path accordingly.\n\n\n\n\n\nDNAtype classification results:\n\n\n# you can find this image in the zip folder\nImage('../notebooks/dna_modelling.png')\n# change the path accordingly.\n\n\n\n\n\n\nConclusion\nUnfortunately, I was unable to achieve an accuracy score higher than 0.996 with dimensionality reduction.\nFurther considerations would be to try and fine tune other parameters for the models, select other models like SVM model, and consider some feature engineering. However, I do not see how feature engineering would be possible on this dataset as it is not complicated and does not contain features to which any further changes could be applied.\nPerhaps, adding more data to the dataframe by scraping the CUTG website, and using species with Ncodons < 1000 in size would be an avenue to explore."
  },
  {
    "objectID": "posts/scrapingcodonwebsite.html",
    "href": "posts/scrapingcodonwebsite.html",
    "title": "Jasmine Marzouk",
    "section": "",
    "text": "# Standard imports\nimport numpy as np\nimport pandas as pd\n\n# For web scraping\nimport requests\nfrom bs4 import BeautifulSoup\n\n# For performing regex operations\nimport re\n\n# For adding delays so that we don't spam requests\nimport time\n\n\nimport re\ndef codon_scrape(url):\n    \n\n    def first_layer_url(url):\n        response = requests.get(url)\n        soup = BeautifulSoup(response.content)\n        project_href = [i['href'] for i in soup.find_all('a', href=True)]\n        r = re.compile('codon\\/[a-zA-Z]+.html')\n        filtered_href = list(filter(r.search, project_href))\n        filtered_href = ['https://www.kazusa.or.jp'+i for i in filtered_href] #now I have a list of the urls containing each of the letters/mito and chloro\n\n        \n    def second_layer_url(filtered_href):\n        \n        for i in firsturls: #this should go through each organism url\n            response1 = requests.get(i)\n            soup1 = BeautifulSoup(response1.content)\n            project_href1 = [i['href'] for i in soup1.find_all('a', href=True)]\n            r = re.compile('ccodon\\/cgi\\-bin\\/showcodon\\.cgi\\?species=\\d{1,10}')\n            filtered_href1 = list(filter(r.search, project_href1))\n            filtered_href1.append(['https://www.kazusa.or.jp'+i for i in filtered_href1])\n    \n    def third_layer_data(filtered_href1):#this function should extract the codons and their frequencies from each organism url and assign it to a variable?\n         \n         \n         for i in filtered_href1:\n            response2 = requests.get(i)\n            soup2 = BeautifulSoup(response2.content)\n\n            codonpage = soup.find(\"strong\").find(text=True)\n\n            data = []\n\n            for info in codonpage:\n                item = {}\n\n                item['species_name'] = str(codonpage).split('<i>')[1].split('</i>')[0]\n                item['nr_of_codons'] = str(codonpage).split(' (')[1].split(')</str')[0]\n                item['kingdom'] = str(codonpage).split('>[gb')[1].split(']: ')[0]            \n\n        \n        \n    \n    \n\nwebsite has three levels\nfirst level selecting alphabet url second level selecting organism name url thrid level codons data\nokay so function that goes through first layer and finds urls puts them in a list another function goes through second layer urls and extraxts them into a list thrid goes through the urls and extracts the data into a csv file?\nproject_href = [i[‘href’] for i in soup.find_all(‘a’, href=True)]\n\n# Save the URL of the webpage we want to scrape to a variable\nurl = 'https://www.kazusa.or.jp/codon/cgi-bin/showcodon.cgi?species=100'\n\n\n# Send a get request and assign the response to a variable\nresponse = requests.get(url)\n\n\n# Turn the undecoded content into a Beautiful Soup object and assign it to a variable\nsoup = BeautifulSoup(response.content)\n\n\nprint(soup.prettify())\n\n<html>\n <head>\n  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n  <title>\n   Codon usage table\n  </title>\n </head>\n <body bgcolor=\"#F0F0F0\">\n  <strong>\n   <i>\n    Ancylobacter aquaticus\n   </i>\n   [gbbct]: 4 CDS's (1108 codons)\n  </strong>\n  <hr align=\"LEFT\" size=\"1\"/>\n  fields: [triplet] [frequency:\n  <strong>\n   per thousand\n  </strong>\n  ] ([number])\n  <hr align=\"LEFT\" size=\"1\"/>\n  <pre>\nUUU  2.7(     3)  UCU  0.9(     1)  UAU 16.2(    18)  UGU  0.0(     0)\nUUC 20.8(    23)  UCC 19.0(    21)  UAC  9.9(    11)  UGC  9.9(    11)\nUUA  0.0(     0)  UCA  2.7(     3)  UAA  0.0(     0)  UGA  2.7(     3)\nUUG  3.6(     4)  UCG 19.9(    22)  UAG  0.9(     1)  UGG  9.9(    11)\n\nCUU  6.3(     7)  CCU  1.8(     2)  CAU 13.5(    15)  CGU 15.3(    17)\nCUC 39.7(    44)  CCC 16.2(    18)  CAC 13.5(    15)  CGC 39.7(    44)\nCUA  0.0(     0)  CCA  0.9(     1)  CAA  0.9(     1)  CGA  0.9(     1)\nCUG 37.9(    42)  CCG 34.3(    38)  CAG 27.1(    30)  CGG 16.2(    18)\n\nAUU  6.3(     7)  ACU  2.7(     3)  AAU  3.6(     4)  AGU  0.9(     1)\nAUC 26.2(    29)  ACC 40.6(    45)  AAC 20.8(    23)  AGC 15.3(    17)\nAUA  0.9(     1)  ACA  0.9(     1)  AAA  2.7(     3)  AGA  0.0(     0)\nAUG 17.1(    19)  ACG 26.2(    29)  AAG 43.3(    48)  AGG  2.7(     3)\n\nGUU  9.0(    10)  GCU  9.9(    11)  GAU 18.1(    20)  GGU  6.3(     7)\nGUC 36.1(    40)  GCC 70.4(    78)  GAC 35.2(    39)  GGC 53.2(    59)\nGUA  1.8(     2)  GCA  8.1(     9)  GAA 26.2(    29)  GGA  8.1(     9)\nGUG 26.2(    29)  GCG 51.4(    57)  GAG 37.9(    42)  GGG  8.1(     9)\n</pre>\n  <hr align=\"LEFT\" size=\"1\"/>\n  Coding GC 66.52%\n1st letter GC 67.06%\n2nd letter GC 49.55%\n3rd letter GC 82.94%\n  <br/>\n  <hr align=\"LEFT\" size=\"1\"/>\n  <form action=\"./showcodon.cgi\" method=\"GET\">\n   <strong>\n    Format:\n   </strong>\n   <br/>\n   <!-- INPUT TYPE=\"HIDDEN\" NAME=\"species\" VALUE=\"Ancylobacter aquaticus [gbbct]\" -->\n   <input name=\"species\" type=\"HIDDEN\" value=\"100\"/>\n   <select name=\"aa\">\n    <option selected=\"\">\n     SELECT A CODE\n     <option value=\"1\">\n      1: Standard\n      <option value=\"2\">\n       2: Vertebrate Mitochondrial\n       <option value=\"3\">\n        3: Yeast Mitochondrial\n        <option value=\"4\">\n         4: Mold, Protozoan, Coelenterate Mitochondrial\n         <option value=\"4\">\n          (4:)  and Mycoplasma/Spiroplasma\n          <option value=\"5\">\n           5: Invertebrate Mitochondrial\n           <option value=\"6\">\n            6: Ciliate Macronuclear and Dasycladacean\n            <option value=\"9\">\n             9: Echinoderm Mitochondrial\n             <option value=\"10\">\n              10: Alternative Ciliate Macronuclear\n              <option value=\"11\">\n               11: Eubacterial\n               <option value=\"12\">\n                12: Alternative Yeast\n                <option value=\"13\">\n                 13: Ascidian Mitochondrial\n                 <option value=\"14\">\n                  14: Flatworm Mitochondrial\n                  <option value=\"15\">\n                   15: Blepharisma Nuclear Code\n                  </option>\n                 </option>\n                </option>\n               </option>\n              </option>\n             </option>\n            </option>\n           </option>\n          </option>\n         </option>\n        </option>\n       </option>\n      </option>\n     </option>\n    </option>\n   </select>\n   <a href=\"http://www3.ncbi.nlm.nih.gov/htbin-post/Taxonomy/wprintgc?mode=t\">\n    Genetic codes (NCBI)\n   </a>\n   <br/>\n   <input checked=\"\" name=\"style\" type=\"radio\" value=\"N\"/>\n   <!-- INPUT TYPE=\"hidden\" NAME=\"style\" VALUE=\"N\" -->\n   Codon Usage Table with Amino Acids\n   <br/>\n   <input name=\"style\" type=\"radio\" value=\"GCG\"/>\n   A style like CodonFrequency output in GCG\n   <a href=\"http://www.gcg.com/products/software.html\">\n    Wisconsin Package\n    <sup>\n     TM\n    </sup>\n   </a>\n   <br/>\n   <!-- INPUT TYPE=\"radio\" NAME=\"style\" VALUE=\"BT\")Back translation (comming soon)\n(BR)\n(TEXTAREA NAME=\"aa_src\" ROWS=\"2\" COLS=\"60\")(/TEXTAREA)\n(P -->\n   <input type=\"SUBMIT\" value=\"Submit\"/>\n  </form>\n  <hr align=\"LEFT\" size=\"1\"/>\n  <form action=\"srchcds.cgi\" method=\"POST\">\n   <strong>\n    CDS Search:\n   </strong>\n   <br/>\n   <input name=\"key\" size=\"40\" type=\"TEXT\"/>\n   <input type=\"SUBMIT\" value=\"Submit\"/>\n   <input name=\"abbrev\" type=\"HIDDEN\" value=\"100\"/>\n   <input name=\"entry\" type=\"HIDDEN\" value=\"bct\"/>\n   <input name=\"sponly\" type=\"HIDDEN\" value=\"Ancylobacter aquaticus\"/>\n   <br/>\n   Keyword example: ribosomal protein / MAP kinase\n  </form>\n  <!-- A HREF=\"ftp://ftp.kazusa.or.jp/pub/codon/current/species/100.bct\" -->\n  <a href=\"http://www.kazusa.or.jp/codon/current/species/100\">\n   List of codon usage for each CDS\n  </a>\n  <a href=\"http://www.kazusa.or.jp/codon/current/CODON_LABEL\">\n   (format)\n  </a>\n  <hr align=\"LEFT\" size=\"1\"/>\n  <i>\n   <a href=\"/codon/\">\n    Homepage\n   </a>\n  </i>\n </body>\n</html>\n\n\n\n\nsoup.title.text\n\n'Codon usage table'\n\n\n\n# Find a <div> tag with class='col-xs-12 col-lg-8 text-center' in our souped webpage\n#soup.find('pre', href_='<a href=\"/codon/A.html\">A</a>')\n\n\ntable = soup.find(\"pre\").find(text=True)\nprint(table)\n\n\nUUU  2.7(     3)  UCU  0.9(     1)  UAU 16.2(    18)  UGU  0.0(     0)\nUUC 20.8(    23)  UCC 19.0(    21)  UAC  9.9(    11)  UGC  9.9(    11)\nUUA  0.0(     0)  UCA  2.7(     3)  UAA  0.0(     0)  UGA  2.7(     3)\nUUG  3.6(     4)  UCG 19.9(    22)  UAG  0.9(     1)  UGG  9.9(    11)\n\nCUU  6.3(     7)  CCU  1.8(     2)  CAU 13.5(    15)  CGU 15.3(    17)\nCUC 39.7(    44)  CCC 16.2(    18)  CAC 13.5(    15)  CGC 39.7(    44)\nCUA  0.0(     0)  CCA  0.9(     1)  CAA  0.9(     1)  CGA  0.9(     1)\nCUG 37.9(    42)  CCG 34.3(    38)  CAG 27.1(    30)  CGG 16.2(    18)\n\nAUU  6.3(     7)  ACU  2.7(     3)  AAU  3.6(     4)  AGU  0.9(     1)\nAUC 26.2(    29)  ACC 40.6(    45)  AAC 20.8(    23)  AGC 15.3(    17)\nAUA  0.9(     1)  ACA  0.9(     1)  AAA  2.7(     3)  AGA  0.0(     0)\nAUG 17.1(    19)  ACG 26.2(    29)  AAG 43.3(    48)  AGG  2.7(     3)\n\nGUU  9.0(    10)  GCU  9.9(    11)  GAU 18.1(    20)  GGU  6.3(     7)\nGUC 36.1(    40)  GCC 70.4(    78)  GAC 35.2(    39)  GGC 53.2(    59)\nGUA  1.8(     2)  GCA  8.1(     9)  GAA 26.2(    29)  GGA  8.1(     9)\nGUG 26.2(    29)  GCG 51.4(    57)  GAG 37.9(    42)  GGG  8.1(     9)\n\n\n\n\nimport re\n\n\ntype(table)\n\nbs4.element.NavigableString\n\n\n\ncodons = table.text\n\ncodons\n\n'\\nUUU  2.7(     3)  UCU  0.9(     1)  UAU 16.2(    18)  UGU  0.0(     0)\\nUUC 20.8(    23)  UCC 19.0(    21)  UAC  9.9(    11)  UGC  9.9(    11)\\nUUA  0.0(     0)  UCA  2.7(     3)  UAA  0.0(     0)  UGA  2.7(     3)\\nUUG  3.6(     4)  UCG 19.9(    22)  UAG  0.9(     1)  UGG  9.9(    11)\\n\\nCUU  6.3(     7)  CCU  1.8(     2)  CAU 13.5(    15)  CGU 15.3(    17)\\nCUC 39.7(    44)  CCC 16.2(    18)  CAC 13.5(    15)  CGC 39.7(    44)\\nCUA  0.0(     0)  CCA  0.9(     1)  CAA  0.9(     1)  CGA  0.9(     1)\\nCUG 37.9(    42)  CCG 34.3(    38)  CAG 27.1(    30)  CGG 16.2(    18)\\n\\nAUU  6.3(     7)  ACU  2.7(     3)  AAU  3.6(     4)  AGU  0.9(     1)\\nAUC 26.2(    29)  ACC 40.6(    45)  AAC 20.8(    23)  AGC 15.3(    17)\\nAUA  0.9(     1)  ACA  0.9(     1)  AAA  2.7(     3)  AGA  0.0(     0)\\nAUG 17.1(    19)  ACG 26.2(    29)  AAG 43.3(    48)  AGG  2.7(     3)\\n\\nGUU  9.0(    10)  GCU  9.9(    11)  GAU 18.1(    20)  GGU  6.3(     7)\\nGUC 36.1(    40)  GCC 70.4(    78)  GAC 35.2(    39)  GGC 53.2(    59)\\nGUA  1.8(     2)  GCA  8.1(     9)  GAA 26.2(    29)  GGA  8.1(     9)\\nGUG 26.2(    29)  GCG 51.4(    57)  GAG 37.9(    42)  GGG  8.1(     9)\\n'\n\n\n\ncodons_list = re.split('\\(\\s+\\d+\\)', codons)\ncodons_list\n\n['\\nUUU  2.7',\n '  UCU  0.9',\n '  UAU 16.2',\n '  UGU  0.0',\n '\\nUUC 20.8',\n '  UCC 19.0',\n '  UAC  9.9',\n '  UGC  9.9',\n '\\nUUA  0.0',\n '  UCA  2.7',\n '  UAA  0.0',\n '  UGA  2.7',\n '\\nUUG  3.6',\n '  UCG 19.9',\n '  UAG  0.9',\n '  UGG  9.9',\n '\\n\\nCUU  6.3',\n '  CCU  1.8',\n '  CAU 13.5',\n '  CGU 15.3',\n '\\nCUC 39.7',\n '  CCC 16.2',\n '  CAC 13.5',\n '  CGC 39.7',\n '\\nCUA  0.0',\n '  CCA  0.9',\n '  CAA  0.9',\n '  CGA  0.9',\n '\\nCUG 37.9',\n '  CCG 34.3',\n '  CAG 27.1',\n '  CGG 16.2',\n '\\n\\nAUU  6.3',\n '  ACU  2.7',\n '  AAU  3.6',\n '  AGU  0.9',\n '\\nAUC 26.2',\n '  ACC 40.6',\n '  AAC 20.8',\n '  AGC 15.3',\n '\\nAUA  0.9',\n '  ACA  0.9',\n '  AAA  2.7',\n '  AGA  0.0',\n '\\nAUG 17.1',\n '  ACG 26.2',\n '  AAG 43.3',\n '  AGG  2.7',\n '\\n\\nGUU  9.0',\n '  GCU  9.9',\n '  GAU 18.1',\n '  GGU  6.3',\n '\\nGUC 36.1',\n '  GCC 70.4',\n '  GAC 35.2',\n '  GGC 53.2',\n '\\nGUA  1.8',\n '  GCA  8.1',\n '  GAA 26.2',\n '  GGA  8.1',\n '\\nGUG 26.2',\n '  GCG 51.4',\n '  GAG 37.9',\n '  GGG  8.1',\n '\\n']\n\n\n\ncodon_list = [i.strip() for i in codons_list]\n\ndel codon_list[-1]\n\ncodon_list\n\n['UUU  2.7',\n 'UCU  0.9',\n 'UAU 16.2',\n 'UGU  0.0',\n 'UUC 20.8',\n 'UCC 19.0',\n 'UAC  9.9',\n 'UGC  9.9',\n 'UUA  0.0',\n 'UCA  2.7',\n 'UAA  0.0',\n 'UGA  2.7',\n 'UUG  3.6',\n 'UCG 19.9',\n 'UAG  0.9',\n 'UGG  9.9',\n 'CUU  6.3',\n 'CCU  1.8',\n 'CAU 13.5',\n 'CGU 15.3',\n 'CUC 39.7',\n 'CCC 16.2',\n 'CAC 13.5',\n 'CGC 39.7',\n 'CUA  0.0',\n 'CCA  0.9',\n 'CAA  0.9',\n 'CGA  0.9',\n 'CUG 37.9',\n 'CCG 34.3',\n 'CAG 27.1',\n 'CGG 16.2',\n 'AUU  6.3',\n 'ACU  2.7',\n 'AAU  3.6',\n 'AGU  0.9',\n 'AUC 26.2',\n 'ACC 40.6',\n 'AAC 20.8',\n 'AGC 15.3',\n 'AUA  0.9',\n 'ACA  0.9',\n 'AAA  2.7',\n 'AGA  0.0',\n 'AUG 17.1',\n 'ACG 26.2',\n 'AAG 43.3',\n 'AGG  2.7',\n 'GUU  9.0',\n 'GCU  9.9',\n 'GAU 18.1',\n 'GGU  6.3',\n 'GUC 36.1',\n 'GCC 70.4',\n 'GAC 35.2',\n 'GGC 53.2',\n 'GUA  1.8',\n 'GCA  8.1',\n 'GAA 26.2',\n 'GGA  8.1',\n 'GUG 26.2',\n 'GCG 51.4',\n 'GAG 37.9',\n 'GGG  8.1']\n\n\n\na, b = zip(*(s.split(\"~\") for s in x))\n\ncodon_key.append(re.split('(\\w[AUCG]+)', i))\n    codon_value.append(re.split('(\\d+\\.\\d+)', i))\n\nIndentationError: unexpected indent (640607889.py, line 4)\n\n\n\ncodon_list[1]\n\n'UCU  0.9'\n\n\n\ncodon_list\n\n['UUU  2.7',\n 'UCU  0.9',\n 'UAU 16.2',\n 'UGU  0.0',\n 'UUC 20.8',\n 'UCC 19.0',\n 'UAC  9.9',\n 'UGC  9.9',\n 'UUA  0.0',\n 'UCA  2.7',\n 'UAA  0.0',\n 'UGA  2.7',\n 'UUG  3.6',\n 'UCG 19.9',\n 'UAG  0.9',\n 'UGG  9.9',\n 'CUU  6.3',\n 'CCU  1.8',\n 'CAU 13.5',\n 'CGU 15.3',\n 'CUC 39.7',\n 'CCC 16.2',\n 'CAC 13.5',\n 'CGC 39.7',\n 'CUA  0.0',\n 'CCA  0.9',\n 'CAA  0.9',\n 'CGA  0.9',\n 'CUG 37.9',\n 'CCG 34.3',\n 'CAG 27.1',\n 'CGG 16.2',\n 'AUU  6.3',\n 'ACU  2.7',\n 'AAU  3.6',\n 'AGU  0.9',\n 'AUC 26.2',\n 'ACC 40.6',\n 'AAC 20.8',\n 'AGC 15.3',\n 'AUA  0.9',\n 'ACA  0.9',\n 'AAA  2.7',\n 'AGA  0.0',\n 'AUG 17.1',\n 'ACG 26.2',\n 'AAG 43.3',\n 'AGG  2.7',\n 'GUU  9.0',\n 'GCU  9.9',\n 'GAU 18.1',\n 'GGU  6.3',\n 'GUC 36.1',\n 'GCC 70.4',\n 'GAC 35.2',\n 'GGC 53.2',\n 'GUA  1.8',\n 'GCA  8.1',\n 'GAA 26.2',\n 'GGA  8.1',\n 'GUG 26.2',\n 'GCG 51.4',\n 'GAG 37.9',\n 'GGG  8.1']\n\n\n\n# codon_dict = {}\n# codon_value = []\n# codon_key = []\n\nfor i in codon_list:\n\n    re.sub('\\\\s+',' ', i)\n\n\nprint(codon_list)\n\n['UUU  2.7', 'UCU  0.9', 'UAU 16.2', 'UGU  0.0', 'UUC 20.8', 'UCC 19.0', 'UAC  9.9', 'UGC  9.9', 'UUA  0.0', 'UCA  2.7', 'UAA  0.0', 'UGA  2.7', 'UUG  3.6', 'UCG 19.9', 'UAG  0.9', 'UGG  9.9', 'CUU  6.3', 'CCU  1.8', 'CAU 13.5', 'CGU 15.3', 'CUC 39.7', 'CCC 16.2', 'CAC 13.5', 'CGC 39.7', 'CUA  0.0', 'CCA  0.9', 'CAA  0.9', 'CGA  0.9', 'CUG 37.9', 'CCG 34.3', 'CAG 27.1', 'CGG 16.2', 'AUU  6.3', 'ACU  2.7', 'AAU  3.6', 'AGU  0.9', 'AUC 26.2', 'ACC 40.6', 'AAC 20.8', 'AGC 15.3', 'AUA  0.9', 'ACA  0.9', 'AAA  2.7', 'AGA  0.0', 'AUG 17.1', 'ACG 26.2', 'AAG 43.3', 'AGG  2.7', 'GUU  9.0', 'GCU  9.9', 'GAU 18.1', 'GGU  6.3', 'GUC 36.1', 'GCC 70.4', 'GAC 35.2', 'GGC 53.2', 'GUA  1.8', 'GCA  8.1', 'GAA 26.2', 'GGA  8.1', 'GUG 26.2', 'GCG 51.4', 'GAG 37.9', 'GGG  8.1']\n\n\n\ntype(codon_list)\n\nlist\n\n\n\ncodon_key, codon_value = zip(*(s.split(\" \") for s in codon_list))\n\n\ncodon_value = list(codon_value)\n\n\ncodon_value.remove('')\n\n\ncodon_value\n\n['',\n '16.2',\n '',\n '20.8',\n '19.0',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '19.9',\n '',\n '',\n '',\n '',\n '13.5',\n '15.3',\n '39.7',\n '16.2',\n '13.5',\n '39.7',\n '',\n '',\n '',\n '',\n '37.9',\n '34.3',\n '27.1',\n '16.2',\n '',\n '',\n '',\n '',\n '26.2',\n '40.6',\n '20.8',\n '15.3',\n '',\n '',\n '',\n '',\n '17.1',\n '26.2',\n '43.3',\n '',\n '',\n '',\n '18.1',\n '',\n '36.1',\n '70.4',\n '35.2',\n '53.2',\n '',\n '',\n '26.2',\n '',\n '26.2',\n '51.4',\n '37.9',\n '']\n\n\n\ntable = soup.find(\"strong\")\nprint(table)\n\n<strong><i>Ancylobacter aquaticus </i>[gbbct]: 4 CDS's (1108 codons)</strong>\n\n\n\nstr(table).split('<i>')[1].split('</i>')[0]\n\n'Ancylobacter aquaticus '\n\n\n\nstr(table).split(' (')[1].split(')</str')[0]\n\n'1108 codons'\n\n\n\nstr(table).split('>[gb')[1].split(']: ')[0]\n\n'bct'\n\n\n\nmatch = re.search('<i>\\w+</i>',str(table))\n\n\nmatch\n\n\n#soup = BeautifulSoup(html,\"lxml\")\n\n#('gb\\w{3}') for gbbct\ntable = soup.findAll(\"strong\", text= re.compile('\\['))\nprint(table)\n\n[]\n\n\n\nproject_href = [i['href'] for i in soup.find_all('a', href=True)]\nprint(project_href)\n\nKeyError: 'pre'\n\n\n\nimport re\nr = re.compile('codon\\/cgi\\-bin\\/showcodon\\.cgi\\?species=\\d{1,10}')\nfiltered_href = list(filter(r.search, project_href))\n\n\nlen(filtered_href)\n\n1510\n\n\n\nfiltered_href = ['https://www.kazusa.or.jp'+i for i in filtered_href]\n\nlen(filtered_href)\n\n1510"
  },
  {
    "objectID": "posts/2022-08-05-Kingdom-Classification.html",
    "href": "posts/2022-08-05-Kingdom-Classification.html",
    "title": "codon2",
    "section": "",
    "text": "# imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom IPython.display import Image\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import plot_importance\nfrom pandas import MultiIndex, Int16Dtype\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import (GridSearchCV, cross_val_score,\n                                     train_test_split)\nfrom sklearn.metrics import (accuracy_score, auc, classification_report,\n                             confusion_matrix, f1_score, plot_confusion_matrix,\n                             plot_roc_curve, precision_score, recall_score,\n                             roc_auc_score, roc_curve)\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN\nfrom numpy import hstack, unique, vstack, where\nfrom matplotlib import pyplot\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom scipy import stats\n\n%matplotlib inline\n\n\n# warnings.filterwarnings(\"ignore\")\nnp.random.seed(123)\n\n\n# warnings.filterwarnings(\"ignore\")\nnp.random.seed(123)"
  },
  {
    "objectID": "posts/2022-08-05-Kingdom-Classification.html#logistic-regression",
    "href": "posts/2022-08-05-Kingdom-Classification.html#logistic-regression",
    "title": "codon2",
    "section": "Logistic Regression:",
    "text": "Logistic Regression:\n\nsns.set_theme(style=\"darkgrid\")\n# Using SMOTE to balance the classes:\n\nsm = SMOTE(random_state=123)\nX_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n\n# Plotting distributions\nprint('Distributions before and after SMOTE upsampling:')\nplt.subplots(1, 2, figsize=(10, 3))\n\n# Plot the original data\nplt.subplot(1, 2, 1)\nplt.bar(y_train.value_counts().index, y_train.value_counts())\nplt.title('Original target classes')\nplt.xticks(ticks=[0, 1, 2, 3, 4, 5, 6])\n\n# Plot the upsampled data\nplt.subplot(1, 2, 2)\nplt.bar(y_train_sm.value_counts().index, y_train_sm.value_counts())\nplt.title('Upsampled target classes')\nplt.xticks(ticks=[0, 1, 2, 3, 4, 5, 6])\nplt.show()\n\nprint('==============================================================================')\n\n# scaling data for PCA tranform\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_sm)  # scaling the upsampled set\nX_test_scaled = scaler.transform(X_test)\n\n# PCA dimensionality reduction\npca = PCA()\n\n# fit transform train set and transform test set\nX_train_PCA = pca.fit_transform(X_train_scaled)\nX_test_PCA = pca.transform(X_test_scaled)\n# to see the number of features that account for the highest variance\nexplained_variance = pca.explained_variance_ratio_\n\n# cumulative sum of the explained variance\ncumulative_sum = np.cumsum(explained_variance)\n\n# plot\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(X_train.columns)+1), cumulative_sum, marker='.')\nplt.axhline(0.9, c='r', linestyle='--')\nplt.xlabel('Number of PCs')\nplt.ylabel('Cumulative Sum of Explained Variance')\nplt.xticks(range(1, len(X_train.columns)+1, 2))\nplt.title(\"How many features explain 90% of total variance\")\nplt.show()\n\nDistributions before and after SMOTE upsampling:\n\n\n\n\n\n==============================================================================\n\n\n\n\n\n\n# setting n_components and refitting a PCA\n\npca_0 = PCA(n_components=27)\n\n# fit transform train set and transform test set\nX_train_PCA_0 = pca_0.fit_transform(X_train_scaled)\nX_test_PCA_0 = pca_0.transform(X_test_scaled)\n\n\n# regularisation:\n\nc_params = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n\ntrain_accuracies = []\ntest_accuracies = []\n\nfor c in c_params:\n    log_reg = LogisticRegression(C=c, max_iter=10000, random_state=123)\n\n    log_reg.fit(X_train_PCA_0, y_train_sm)\n\n    train_accuracies.append(log_reg.score(X_train_PCA_0, y_train_sm))\n    test_accuracies.append(log_reg.score(X_test_PCA_0, y_test))\n\n\nplt.figure(figsize=(10, 10))\nplt.plot(c_params, train_accuracies, label='train')\nplt.plot(c_params, test_accuracies, label='test')\nplt.xscale('log')\nplt.legend()\nplt.title('Accuracies for log_reg model')\nplt.xlabel('C parameter')\nplt.ylabel('Accuracy')\nplt.show()\n\n\n\n\n\n# For C =0.01\n\nlog_reg_0 = LogisticRegression(C=0.01, max_iter=10000, random_state=123)\n\nlog_reg_0.fit(X_train_PCA_0, y_train_sm)\n\nprint(log_reg_0.score(X_train_PCA_0, y_train_sm))\nprint(log_reg_0.score(X_test_PCA_0, y_test))\n\n0.8506893517160458\n0.8218279984573853\n\n\n\nsns.set_theme(style=\"dark\")\ny_predicted = log_reg_0.predict(X_test_PCA_0)\n\n# Generate confusion matrix\ncf_matrix = confusion_matrix(y_test, y_predicted)\n\n# label rows and columns\ncf_df = pd.DataFrame(\n    cf_matrix,\n    columns=[\"Predicted vertebrate\", \"Predicted bacteria\", \"Predicted virus\",\n             \"Predicted plant\", \"Predicted invertebrate\", \"Predicted phage\", \"Predicted archaea\"],\n    index=[\"True vertebrate\", \"True bacteria\", \"True virus\", \"True plant\", \"True invertebrate\", \"True phage\", \"True archaea\"])\n\n\nplot_confusion_matrix(log_reg_0, X_test_PCA_0, y_test)\nplt.xticks(rotation=45)\n\nprint('==========================================================')\n\n# Precision, recall, and F1\nclass_report_log_0 = classification_report(y_test, y_predicted)\nprint(class_report_log_0)\n\nprint('==========================================================')\n\n# AUC score\ny_proba_train = log_reg_0.predict_proba(X_train_PCA_0)\nauc_train = np.round(roc_auc_score(\n    y_train_sm, y_proba_train, multi_class='ovo'), 3)\nprint('Logistic Regression model AUC score')\nprint(f'roc_auc_score: {auc_train}')\nprint('==========================================================')\n\ncf_df\n\n==========================================================\n              precision    recall  f1-score   support\n\n     archaea       0.41      0.92      0.57        25\n    bacteria       0.93      0.83      0.88       584\ninvertebrate       0.61      0.66      0.63       267\n       phage       0.30      0.82      0.44        44\n       plant       0.81      0.74      0.77       498\n  vertebrate       0.95      0.95      0.95       609\n       virus       0.85      0.83      0.84       566\n\n    accuracy                           0.82      2593\n   macro avg       0.70      0.82      0.73      2593\nweighted avg       0.85      0.82      0.83      2593\n\n==========================================================\nLogistic Regression model AUC score\nroc_auc_score: 0.976\n==========================================================\n\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n\n\n\n  \n    \n      \n      Predicted vertebrate\n      Predicted bacteria\n      Predicted virus\n      Predicted plant\n      Predicted invertebrate\n      Predicted phage\n      Predicted archaea\n    \n  \n  \n    \n      True vertebrate\n      23\n      2\n      0\n      0\n      0\n      0\n      0\n    \n    \n      True bacteria\n      24\n      483\n      5\n      58\n      7\n      2\n      5\n    \n    \n      True virus\n      2\n      7\n      176\n      1\n      40\n      9\n      32\n    \n    \n      True plant\n      3\n      5\n      0\n      36\n      0\n      0\n      0\n    \n    \n      True invertebrate\n      1\n      14\n      45\n      22\n      367\n      14\n      35\n    \n    \n      True phage\n      0\n      0\n      19\n      0\n      2\n      576\n      12\n    \n    \n      True archaea\n      3\n      6\n      43\n      3\n      36\n      5\n      470\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for Logistic Regression\n\nLogistic Regression model classifies using a logistic function, where the output is either 1 or 0, this is usually for binomial clasification. However, and as I assumed, Logistic regression model did not perform adequately in classifying multiclass target, with an R2 score of 0.85 for train data, and 0.82 for test data.\nI balanced the classes using SMOTE, and then performed a PCA fit on the train and transformed the test set. PCA reduces dimensionality to reduce overfitting, and use fewer features from the dataset. According to the graph above, the number of number of components needed to explain 90% of the variance is 29.\nFurthermore, hyperparameter tuning showed that a higher penalty of C=0.01 was the appropriate value to achieve a higher accuracy score.\nThe model is very good at classifying the vertebrate class which has a similar precision and recall score. The model performed well enough on the bacteria and virus classes. For the phage and archaea classes the model has: - high recall: the overall proportion of true positives over predicted results is high, the model classified the majority of these classes correctly - low precision: the proportion of true positives over actual results, the model struggled to classify the majority of the phage class and only correctly classifying 29% correctly.\nThe AUC score of 0.977 shows that the model performed well overall at distinguishing between the classes.\n\n# Feature importance\n# 5 to select coefficients for the vertebrate class.\ncoefs = pd.DataFrame(log_reg_0.coef_[5], columns=['coefficients'])\ncoefs\n\nfeat_names = pd.DataFrame(X_train.columns, columns=['feature_names'])\nfeat_names\n\nfeat_coefs = pd.concat([feat_names, coefs], axis=1, ignore_index=True)\n\nfeat_coefs = feat_coefs.sort_values(\n    by=1, ascending=False).set_index(0).head(29)\n\n# plotting coefficients\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(15, 8))\nplt.bar(feat_coefs.index, feat_coefs[1])\nplt.xticks(rotation=45)\nplt.xlabel('Features')\nplt.ylabel('Coefficient')\nplt.show()\n\n\n\n\nLogistic Regression adds coefficients (beta1) to each feature, the above bar chart shows the coefficients for the 29 features for the vertebrate class. the coefficients represent the log odds of a feature change effecting the outcome.\nfor the log_reg_0 model, the above barchart shows that with increase frequency of UUU and CUC for example, there is an increased odds of vertebrate classification. The bar grapgh shows how the model made the decision of classification, in terms of these features."
  },
  {
    "objectID": "posts/2022-08-05-Kingdom-Classification.html#knearest-neighbors-model",
    "href": "posts/2022-08-05-Kingdom-Classification.html#knearest-neighbors-model",
    "title": "codon2",
    "section": "kNearest Neighbors model:",
    "text": "kNearest Neighbors model:\n\n# using the already scaled and PCA tranformed sets from previous model\n\n# Determining the ideal n_neighbors value:\n\nn_neighbors = range(1, 100, 2)\n\ntrain_accuracies_0 = []\ntest_accuracies_0 = []\n\nfor n in n_neighbors:\n    KNN_model = KNeighborsClassifier(n_neighbors=n)\n    KNN_model.fit(X_train_PCA_0, y_train_sm)\n\n    train_accuracies_0.append(KNN_model.score(X_train_PCA_0, y_train_sm))\n    test_accuracies_0.append(KNN_model.score(X_test_PCA_0, y_test))\n\n\nplt.figure(figsize=(10, 10))\nplt.plot(n_neighbors, train_accuracies_0, label='train')\nplt.plot(n_neighbors, test_accuracies_0, label='test')\nplt.legend()\nplt.title('Accuracies for KNN_model')\nplt.xlabel('Number of neighbours')\nplt.ylabel('Accuracy')\nplt.show()\n\n\n\n\n\nKnn_accuracies_df = pd.DataFrame({'number of neighbors': n_neighbors,\n                                 'Train scores': train_accuracies_0, 'Test scores': test_accuracies_0})\nKnn_accuracies_df.sort_values(by='Test scores', ascending=False).head(5)\n\n\n\n\n\n  \n    \n      \n      number of neighbors\n      Train scores\n      Test scores\n    \n  \n  \n    \n      0\n      1\n      1.000000\n      0.939838\n    \n    \n      1\n      3\n      0.978117\n      0.934825\n    \n    \n      2\n      5\n      0.967263\n      0.928654\n    \n    \n      3\n      7\n      0.960751\n      0.924798\n    \n    \n      4\n      9\n      0.953828\n      0.922869\n    \n  \n\n\n\n\n\nKNN_model_0 = KNeighborsClassifier(n_neighbors=1)\nKNN_model_0.fit(X_train_PCA_0, y_train_sm)\n\nprint(f'KNN model train score: {KNN_model_0.score(X_train_PCA_0, y_train_sm)}')\nprint(\n    f'KNN model test score: {KNN_model_0.score(X_test_PCA_0, y_test).round(3)}')\n\nKNN model train score: 1.0\nKNN model test score: 0.94\n\n\n\ny_predicted_0 = KNN_model_0.predict(X_test_PCA_0)\n\n# Generate confusion matrix\ncf_matrix_0 = confusion_matrix(y_test, y_predicted_0)\n\n# label rows and columns\ncf_df_0 = pd.DataFrame(\n    cf_matrix_0,\n    columns=[\"Predicted vrt\", \"Predicted bct\", \"Predicted vrl\",\n             \"Predicted pln\", \"Predicted inv\", \"Predicted phg\", \"Predicted arc\"],\n    index=[\"True vrt\", \"True bct\", \"True vrl\", \"True pln\", \"True inv\", \"True phg\", \"True arc\"])\n\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(KNN_model_0, X_test_PCA_0, y_test)\nplt.xticks(rotation=45)\n\nprint('==========================================================')\n\nclass_report_knn_0 = classification_report(y_test, y_predicted_0)\nprint(class_report_knn_0)\n\nprint('==========================================================')\n\n# AUC score\ny_proba_train_0 = KNN_model_0.predict_proba(X_train_PCA_0)\nauc_train_0 = np.round(roc_auc_score(\n    y_train_sm, y_proba_train_0, multi_class='ovo'), 3)\nprint('KNN model AUC score')\nprint(f'roc_auc_score: {auc_train_0}')\nprint('==========================================================')\n\ncf_df_0\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n==========================================================\n              precision    recall  f1-score   support\n\n     archaea       0.70      0.84      0.76        25\n    bacteria       0.97      0.95      0.96       584\ninvertebrate       0.84      0.87      0.86       267\n       phage       0.61      0.82      0.70        44\n       plant       0.94      0.92      0.93       498\n  vertebrate       0.98      0.98      0.98       609\n       virus       0.97      0.95      0.96       566\n\n    accuracy                           0.94      2593\n   macro avg       0.86      0.90      0.88      2593\nweighted avg       0.94      0.94      0.94      2593\n\n==========================================================\nKNN model AUC score\nroc_auc_score: 1.0\n==========================================================\n\n\n\n\n\n\n  \n    \n      \n      Predicted vrt\n      Predicted bct\n      Predicted vrl\n      Predicted pln\n      Predicted inv\n      Predicted phg\n      Predicted arc\n    \n  \n  \n    \n      True vrt\n      21\n      3\n      1\n      0\n      0\n      0\n      0\n    \n    \n      True bct\n      5\n      554\n      3\n      17\n      4\n      0\n      1\n    \n    \n      True vrl\n      1\n      3\n      233\n      1\n      11\n      7\n      11\n    \n    \n      True pln\n      3\n      4\n      0\n      36\n      1\n      0\n      0\n    \n    \n      True inv\n      0\n      8\n      17\n      5\n      460\n      4\n      4\n    \n    \n      True phg\n      0\n      1\n      11\n      0\n      2\n      595\n      0\n    \n    \n      True arc\n      0\n      1\n      11\n      0\n      13\n      3\n      538\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for kNearest Neighbors:\n\nkNearest Neighbor model classifies by determining the distances between the data points: data points closest to each other are classified as being from the same class, depending on the number of nearest neighbors selected. Because KNN usese distances between data points, I scaled the data performed PCA and used the SMOTE transformed data.\nThe KNN_model_0 performed very well, with an accuracy score of 1.0 for the training data and 0.94 for the test data, for a value of n_neighbors = 1.\nThe model performed better on all of the classes compared with log_reg_0 (logistic Regression) model, with improvement in the precision and recall scores overall. However, the KNN model still underperformed on the phage and archaea classes with F1 scores of 0.69 and 0.76 respectively.\nOverall the KNN_model_0 performed well on distinguising between the classes with an AUC score of 1.0."
  },
  {
    "objectID": "posts/2022-08-05-Kingdom-Classification.html#xgboost-model",
    "href": "posts/2022-08-05-Kingdom-Classification.html#xgboost-model",
    "title": "codon2",
    "section": "XGBoost model:",
    "text": "XGBoost model:\nI will need to label encode the Kingdom column so the classes are integers instead of strings, the XGBoost model only takes numerical values as its input.\nI will use the LabelEncoder() to fit to the training data and transform the test data.\n\n# Instantiate the label encoder\nlabel_enc = LabelEncoder()\n\n# Fit and transform the y_train, and transform the y_test\ny_train_enc = label_enc.fit_transform(y_train)\ny_test_enc = label_enc.transform(y_test)\n\n\nmaxdepth = range(1, 10)\n\ntrain_accuracies_1 = []\ntest_accuracies_1 = []\n\nfor d in maxdepth:\n    XGB_model_ = XGBClassifier(max_depth=d, use_label_encoder=False)\n    XGB_model_.fit(X_train, y_train_enc)\n\n    train_accuracies_1.append(XGB_model_.score(X_train, y_train_enc))\n    test_accuracies_1.append(XGB_model_.score(X_test, y_test_enc))\n\n\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(10, 10))\nplt.plot(maxdepth, train_accuracies_1, label='train')\nplt.plot(maxdepth, test_accuracies_1, label='test')\nplt.xlabel('Max Depth')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracies for XGBoost model')\n\n\nXGB_accuracies_df = pd.DataFrame(\n    {'max Depth': maxdepth, 'Train scores': train_accuracies_1, 'Test scores': test_accuracies_1})\nXGB_accuracies_df.sort_values(by='Test scores', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      max Depth\n      Train scores\n      Test scores\n    \n  \n  \n    \n      5\n      6\n      1.000000\n      0.949865\n    \n    \n      3\n      4\n      1.000000\n      0.949479\n    \n    \n      8\n      9\n      1.000000\n      0.949479\n    \n    \n      4\n      5\n      1.000000\n      0.947937\n    \n    \n      6\n      7\n      1.000000\n      0.947937\n    \n    \n      7\n      8\n      1.000000\n      0.947165\n    \n    \n      2\n      3\n      0.995757\n      0.941381\n    \n    \n      1\n      2\n      0.962492\n      0.917084\n    \n    \n      0\n      1\n      0.866358\n      0.848438\n    \n  \n\n\n\n\n\n\n\n\n# XGBoost with max+depth = 4\n\nXGB_model_0 = XGBClassifier(max_depth=4, use_label_encoder=False)\nXGB_model_0.fit(X_train, y_train_enc)\n\nprint(\n    f'XGBoost train accuracy: {XGB_model_0.score(X_train, y_train_enc).round(3)}')\nprint(\n    f'XGBoost test accuracy: {XGB_model_0.score(X_test, y_test_enc).round(3)}')\n\nXGBoost train accuracy: 1.0\nXGBoost test accuracy: 0.949\n\n\n\ny_predicted_1 = XGB_model_0.predict(X_test)\n\n# Generate confusion matrix\ncf_matrix_1 = confusion_matrix(y_test_enc, y_predicted_1)\n\n# label rows and columns\ncf_df_1 = pd.DataFrame(\n    cf_matrix_1,\n    columns=[\"Predicted vrt\", \"Predicted bct\", \"Predicted vrl\",\n             \"Predicted pln\", \"Predicted inv\", \"Predicted phg\", \"Predicted arc\"],\n    index=[\"True vrt\", \"True bct\", \"True vrl\", \"True pln\", \"True inv\", \"True phg\", \"True arc\"])\n\nprint('==========================================================')\n\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(XGB_model_0, X_test, y_test_enc)\n\nprint('==========================================================')\n\n\n# Precision Recall and F1 scores\nclass_report_xgb_0 = classification_report(y_test_enc, y_predicted_1)\nprint(class_report_xgb_0)\nprint('==========================================================')\n\n# AUC score\ny_proba_train_1 = XGB_model_0.predict_proba(X_train)\nauc_train_1 = np.round(roc_auc_score(\n    y_train, y_proba_train_1, multi_class='ovo'), 3)\nprint('XGBoost model AUC score')\nprint(f'roc_auc_score: {auc_train_1}')\nprint('==========================================================')\n\ncf_df_1\n\n==========================================================\n==========================================================\n              precision    recall  f1-score   support\n\n           0       0.81      0.68      0.74        25\n           1       0.94      0.98      0.96       584\n           2       0.93      0.84      0.88       267\n           3       0.90      0.64      0.75        44\n           4       0.93      0.94      0.94       498\n           5       0.99      0.99      0.99       609\n           6       0.95      0.97      0.96       566\n\n    accuracy                           0.95      2593\n   macro avg       0.92      0.86      0.89      2593\nweighted avg       0.95      0.95      0.95      2593\n\n==========================================================\n\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\nXGBoost model AUC score\nroc_auc_score: 1.0\n==========================================================\n\n\n\n\n\n\n  \n    \n      \n      Predicted vrt\n      Predicted bct\n      Predicted vrl\n      Predicted pln\n      Predicted inv\n      Predicted phg\n      Predicted arc\n    \n  \n  \n    \n      True vrt\n      17\n      6\n      1\n      0\n      0\n      0\n      1\n    \n    \n      True bct\n      1\n      574\n      1\n      2\n      2\n      0\n      4\n    \n    \n      True vrl\n      0\n      4\n      224\n      0\n      20\n      4\n      15\n    \n    \n      True pln\n      2\n      12\n      0\n      28\n      1\n      0\n      1\n    \n    \n      True inv\n      0\n      13\n      5\n      1\n      469\n      2\n      8\n    \n    \n      True phg\n      0\n      0\n      6\n      0\n      0\n      602\n      1\n    \n    \n      True arc\n      1\n      0\n      5\n      0\n      10\n      2\n      548\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for XGBoost model:\n\nXGBoost model is an ensemble model, it classifies data using decision trees in ensembles. Using max_depth as the hyperparameter, which determines the maximium depth of the trees.\nFor XGB_model_0 the appropriate value for max_depth was determined to be 3, this gave an accuracy score of 0.996 for training data and 0.941 for test data. There is slight overfitting here, this is a disadvantage of decision tree type classification.\nI did not use the scaled, oversamples, and PCA transformed data for this model, as this is not needed.\nXGB_model_0 performed better than KNN_model_0 and log_reg_0 in terms of precision for phage and archaea classes with scores of 0.83 and 0.89 respectively. Although, there is a decrease in recall score for both of these classes, meaning that this model underperformed in classifying the\nHowever, in terms of accuracy it is as good as KNN_model_0 in terms of accuracy, but KNN_model_0 had less overfitting on the training data.\nFurthermore, this model performed well on distinguishing between the classes with an AUC score of 1.0.\n\n# plot feature importance\nplt.figure(figsize=(15, 25))\nplot_importance(XGB_model_0, max_num_features=20,\n                importance_type='gain', show_values=False)\nplt.show()\n\n<Figure size 1080x1800 with 0 Axes>\n\n\n\n\n\n\nfeat_imprt = pd.DataFrame(\n    XGB_model_0.feature_importances_).sort_values(by=0, ascending=False)\n\nfeat_names_0 = pd.DataFrame(X_train.columns, columns=['feature_names'])\n\n# join inner because the ndarray lengths are different\nfeat_importance_0 = pd.concat(\n    [feat_names_0, feat_imprt], axis=1, ignore_index=True, join='inner')\n\nfeat_importance_0 = feat_importance_0.sort_values(\n    by=1, ascending=False).set_index(0).head(20)\n\n# plotting coefficients\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(15, 8))\nplt.bar(feat_importance_0.index, feat_imprt[0][:20])\nplt.xticks(rotation=45)\nplt.xlabel('Features')\nplt.ylabel('Importance')\nplt.show()\n\n\n\n\nFeature importance for XGBoost models shows the features that the model deemed most important in making the classification decision.\nthe above bar chart shows the codons importance according to gain metric, which is the relative contribution of each feature for each tree in the ensemble (XGBoost) model. This means that CUA and GCG having the highest values of importance were the most important in generating a prediction of the outcome classification."
  },
  {
    "objectID": "posts/2022-08-05-Kingdom-Classification.html#ensemble-model",
    "href": "posts/2022-08-05-Kingdom-Classification.html#ensemble-model",
    "title": "codon2",
    "section": "Ensemble Model",
    "text": "Ensemble Model\n\n# We instantiate the base models, along with their names\nbase_models = [('KNN', KNN_model_0),\n               ('XGBoost', XGB_model_0)\n               ]\n\n\n# building the stacked model\nstacked_model = StackingClassifier(\n    estimators=base_models,\n    final_estimator=LogisticRegression())\n\nstacked_model.fit(X_train, y_train)\n\nStackingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=1)),\n                               ('XGBoost',\n                                XGBClassifier(base_score=0.5, booster='gbtree',\n                                              callbacks=None,\n                                              colsample_bylevel=1,\n                                              colsample_bynode=1,\n                                              colsample_bytree=1,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None, gamma=0,\n                                              gpu_id=-1,\n                                              grow_policy='depthwise',\n                                              importance_type=None,\n                                              interaction_constraints='',\n                                              learning_rate=0.300000012,\n                                              max_bin=256, max_cat_to_onehot=4,\n                                              max_delta_step=0, max_depth=4,\n                                              max_leaves=0, min_child_weight=1,\n                                              missing=nan,\n                                              monotone_constraints='()',\n                                              n_estimators=100, n_jobs=0,\n                                              num_parallel_tree=1,\n                                              objective='multi:softprob',\n                                              predictor='auto', random_state=0,\n                                              reg_alpha=0, ...))],\n                   final_estimator=LogisticRegression())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StackingClassifierStackingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=1)),\n                               ('XGBoost',\n                                XGBClassifier(base_score=0.5, booster='gbtree',\n                                              callbacks=None,\n                                              colsample_bylevel=1,\n                                              colsample_bynode=1,\n                                              colsample_bytree=1,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None, gamma=0,\n                                              gpu_id=-1,\n                                              grow_policy='depthwise',\n                                              importance_type=None,\n                                              interaction_constraints='',\n                                              learning_rate=0.300000012,\n                                              max_bin=256, max_cat_to_onehot=4,\n                                              max_delta_step=0, max_depth=4,\n                                              max_leaves=0, min_child_weight=1,\n                                              missing=nan,\n                                              monotone_constraints='()',\n                                              n_estimators=100, n_jobs=0,\n                                              num_parallel_tree=1,\n                                              objective='multi:softprob',\n                                              predictor='auto', random_state=0,\n                                              reg_alpha=0, ...))],\n                   final_estimator=LogisticRegression())KNNKNeighborsClassifierKNeighborsClassifier(n_neighbors=1)XGBoostXGBClassifierXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n              predictor='auto', random_state=0, reg_alpha=0, ...)final_estimatorLogisticRegressionLogisticRegression()\n\n\n\nprint(\n    f'Stacked model train accuracy: {stacked_model.score(X_train, y_train).round(3)}')\nprint(\n    f'Stacked model test accuracy: {stacked_model.score(X_test, y_test).round(3)}')\n\nStacked model train accuracy: 1.0\nStacked model test accuracy: 0.949\n\n\n\ny_predicted_2 = stacked_model.predict(X_test)\n\n# Generate confusion matrix\ncf_matrix_2 = confusion_matrix(y_test, y_predicted_2)\n\n# label rows and columns\ncf_df_2 = pd.DataFrame(\n    cf_matrix_2,\n    columns=[\"Predicted vrt\", \"Predicted bct\", \"Predicted vrl\",\n             \"Predicted pln\", \"Predicted inv\", \"Predicted phg\", \"Predicted arc\"],\n    index=[\"True vrt\", \"True bct\", \"True vrl\", \"True pln\", \"True inv\", \"True phg\", \"True arc\"])\n\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(stacked_model, X_test, y_test)\nplt.xticks(rotation=45)\n\nprint('==========================================================')\n\n\n# Precision Recall and F1 scores\nclass_report_stack_0 = classification_report(y_test, y_predicted_2)\nprint(class_report_stack_0)\nprint('==========================================================')\n\n# AUC score\ny_proba_train_2 = stacked_model.predict_proba(X_train)\nauc_train_2 = np.round(roc_auc_score(\n    y_train, y_proba_train_2, multi_class='ovo'), 3)\nprint('Ensemble model AUC score')\nprint(f'roc_auc_score: {auc_train_2}')\nprint('==========================================================')\n\ncf_df_2\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n==========================================================\n              precision    recall  f1-score   support\n\n     archaea       0.81      0.68      0.74        25\n    bacteria       0.94      0.98      0.96       584\ninvertebrate       0.91      0.85      0.88       267\n       phage       0.90      0.64      0.75        44\n       plant       0.94      0.94      0.94       498\n  vertebrate       0.99      0.99      0.99       609\n       virus       0.95      0.97      0.96       566\n\n    accuracy                           0.95      2593\n   macro avg       0.92      0.86      0.89      2593\nweighted avg       0.95      0.95      0.95      2593\n\n==========================================================\nEnsemble model AUC score\nroc_auc_score: 1.0\n==========================================================\n\n\n\n\n\n\n  \n    \n      \n      Predicted vrt\n      Predicted bct\n      Predicted vrl\n      Predicted pln\n      Predicted inv\n      Predicted phg\n      Predicted arc\n    \n  \n  \n    \n      True vrt\n      17\n      6\n      1\n      0\n      0\n      0\n      1\n    \n    \n      True bct\n      1\n      574\n      1\n      2\n      3\n      0\n      3\n    \n    \n      True vrl\n      0\n      4\n      228\n      0\n      17\n      4\n      14\n    \n    \n      True pln\n      2\n      12\n      0\n      28\n      1\n      0\n      1\n    \n    \n      True inv\n      0\n      14\n      8\n      1\n      466\n      1\n      8\n    \n    \n      True phg\n      0\n      0\n      6\n      0\n      1\n      600\n      2\n    \n    \n      True arc\n      1\n      0\n      6\n      0\n      8\n      2\n      549\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for the sctacked (ensemble) model:\n\nFor the ensemble or stacked model, I used the models I already trained and tuned, with the exception of the logistic regression model log_reg_0; I decided to exclude this model as it was the model that underperformed on classifying the target variable.\nI only included XGB_model_0 and KNN_model_0 as base models, with a final estimator being a default Logistic Regression model. Each of the base estimators is different in the way it approaches the classification.\nThe ensemble model classifies by taking the outputs from both the XGBoost model and KNN model as probabilites of each class as an input, the final estimator being a logistic regression then uses logistic function to classify each class.\nstacked_model achieved an accuracy score of 0.998 for the train data and 0.946 for the test data, suggesting slight overfitting, which makes sense as one of the base models is the XGBoost.\nThe stacked model did well in distinguishing between the classes with an AUC of 1.0."
  },
  {
    "objectID": "posts/2022-08-05-Kingdom-Classification.html#comparing-models",
    "href": "posts/2022-08-05-Kingdom-Classification.html#comparing-models",
    "title": "codon2",
    "section": "Comparing Models:",
    "text": "Comparing Models:\n\ncls = classification_report(y_test, y_predicted, output_dict=True)\ncls_0 = classification_report(y_test, y_predicted_0, output_dict=True)\ncls_1 = classification_report(y_test_enc, y_predicted_1, output_dict=True)\ncls_2 = classification_report(y_test, y_predicted_2, output_dict=True)\n\n\ncomparison_df = pd.DataFrame({'Precision':\n                             [cls['weighted avg']['precision'],\n                              cls_0['weighted avg']['precision'],\n                              cls_1['weighted avg']['precision'],\n                              cls_2['weighted avg']['precision']],\n                              'Recall':\n                             [cls['weighted avg']['recall'],\n                              cls_0['weighted avg']['recall'],\n                              cls_1['weighted avg']['recall'],\n                              cls_2['weighted avg']['recall']],\n                             'F1 score':\n                              [cls['weighted avg']['f1-score'],\n                              cls_0['weighted avg']['f1-score'],\n                               cls_1['weighted avg']['f1-score'],\n                              cls_2['weighted avg']['f1-score']],\n                              'Accuracy':\n                              [cls['accuracy'],\n                               cls_0['accuracy'],\n                               cls_1['accuracy'],\n                               cls_2['accuracy']],\n                              'AUC score':\n                              [auc_train,\n                               auc_train_0,\n                               auc_train_1,\n                               auc_train_2]\n                              },\n                             index=['Logistic Regression', 'kNearest Neighbors', 'XGBoost', 'Stacked model'])\n\ncomparison_df.round(3).sort_values(by='F1 score', ascending=False)\n\n\n\n\n\n  \n    \n      \n      Precision\n      Recall\n      F1 score\n      Accuracy\n      AUC score\n    \n  \n  \n    \n      Stacked model\n      0.949\n      0.949\n      0.949\n      0.949\n      1.000\n    \n    \n      XGBoost\n      0.949\n      0.949\n      0.948\n      0.949\n      1.000\n    \n    \n      kNearest Neighbors\n      0.943\n      0.940\n      0.941\n      0.940\n      1.000\n    \n    \n      Logistic Regression\n      0.847\n      0.822\n      0.830\n      0.822\n      0.976\n    \n  \n\n\n\n\nAccording to the F1 score the model that performed the best overall is the stacked model, but by not much as the KNN model did just as well.\nLooking at the vanilla models performed at the beginning the accuracy score for the kNearest Neighbor performed was 93%, so I only achived an improvement of ~1.6% for the Kingdom classses classification.\nThis could be because the dataset is not complicated enough for the models to be able to learn by combining the models, that is why the stacked model isn’t far off from the results of the KNN and XGBoost models.\n\nplt.figure(figsize=(20, 25))\ncomparison_df.plot(kind='bar')\nplt.xticks(rotation=90)\nplt.legend()\nplt.show()\n\n<Figure size 1440x1800 with 0 Axes>"
  },
  {
    "objectID": "posts/2022-08-05-Kingdom-Classification.html#saving-models",
    "href": "posts/2022-08-05-Kingdom-Classification.html#saving-models",
    "title": "codon2",
    "section": "Saving Models:",
    "text": "Saving Models:\n\n# Logistic Regression model\nlog_reg_kingdom = 'finalized_log_reg_kingdom.sav'\njoblib.dump(log_reg_0, log_reg_kingdom)\n\n\n# kNearest Neighbor model\nknn_kingdom = 'finalized_knn_kingdom.sav'\njoblib.dump(KNN_model_0, knn_kingdom)\n\n# XGBoost model\nxgboost_kingdom = 'finalized_xgboost_kingdom.sav'\njoblib.dump(XGB_model_0, xgboost_kingdom)\n\n# Ensemble model\nstacked_kingdom = 'finalized_stacked_kingdom.sav'\njoblib.dump(stacked_model, stacked_kingdom)\n\n['finalized_stacked_kingdom.sav']"
  },
  {
    "objectID": "posts/2022-08-04-DnaType-Classification.html",
    "href": "posts/2022-08-04-DnaType-Classification.html",
    "title": "codon2",
    "section": "",
    "text": "With modeling I will start with a Logistic Regression\n\n# imports\nfrom IPython.display import Image\nfrom xgboost import plot_importance\nfrom sklearn.metrics import roc_auc_score\nfrom pandas import MultiIndex, Int16Dtype\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import (GridSearchCV, cross_val_score,\n                                     train_test_split)\nfrom sklearn.metrics import (accuracy_score, auc, classification_report,\n                             confusion_matrix, f1_score, plot_confusion_matrix,\n                             plot_roc_curve, precision_score, recall_score,\n                             roc_auc_score, roc_curve)\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN\nfrom numpy import hstack, unique, vstack, where\nfrom matplotlib import pyplot\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom scipy import stats\n\n%matplotlib inline\n\n\n# warnings.filterwarnings(\"ignore\")\nnp.random.seed(123)"
  },
  {
    "objectID": "posts/2022-08-04-DnaType-Classification.html#logistic-regression",
    "href": "posts/2022-08-04-DnaType-Classification.html#logistic-regression",
    "title": "codon2",
    "section": "Logistic Regression:",
    "text": "Logistic Regression:\n\n# Using SMOTE to balance the classes:\n\nsm = SMOTE(random_state=123)\nX_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n\n# Plotting distributions\nprint('Distributions before and after SMOTE upsampling:')\nplt.subplots(1, 2, figsize=(10, 3))\n\n# Plot the original data\nplt.subplot(1, 2, 1)\nplt.bar(y_train.value_counts().index, y_train.value_counts())\nplt.title('Original target classes')\nplt.xticks(ticks=[0, 1, 2])\n\n# Plot the upsampled data\nplt.subplot(1, 2, 2)\nplt.bar(y_train_sm.value_counts().index, y_train_sm.value_counts())\nplt.title('Upsampled target classes')\nplt.xticks(ticks=[0, 1, 2])\n\nplt.show()\nprint('==============================================================================')\n\n# scaling data for PCA tranform\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_sm)  # scaling the upsampled set\nX_test_scaled = scaler.transform(X_test)\n\n# PCA dimensionality reduction\npca = PCA()\n\n# fit transform train set and transform test set\nX_train_PCA = pca.fit_transform(X_train_scaled)\nX_test_PCA = pca.transform(X_test_scaled)\n# to see the number of features that account for the highest variance\nexplained_variance = pca.explained_variance_ratio_\n\n# cumulative sum of the explained variance\ncumulative_sum = np.cumsum(explained_variance)\n\n# plot\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(X_train.columns)+1), cumulative_sum, marker='.')\nplt.axhline(0.9, c='r', linestyle='--')\nplt.xlabel('Number of PCs')\nplt.ylabel('Cumulative Sum of Explained Variance')\nplt.xticks(range(1, len(X_train.columns)+1, 2))\nplt.title(\"How many features explain 90% of total variance\")\nplt.show()\n\nDistributions before and after SMOTE upsampling:\n\n\n\n\n\n==============================================================================\n\n\n\n\n\n\n# setting n_components and refitting a PCA\n\npca_0 = PCA(n_components=26)\n\n# fit transform train set and transform test set\nX_train_PCA_0 = pca_0.fit_transform(X_train_scaled)\nX_test_PCA_0 = pca_0.transform(X_test_scaled)\n\n\n# regularisation:\n\nc_params = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n\ntrain_accuracies = []\ntest_accuracies = []\n\nfor c in c_params:\n    logi_reg = LogisticRegression(C=c, max_iter=10000, random_state=123)\n\n    logi_reg.fit(X_train_PCA_0, y_train_sm)\n\n    train_accuracies.append(logi_reg.score(X_train_PCA_0, y_train_sm))\n    test_accuracies.append(logi_reg.score(X_test_PCA_0, y_test))\n\n\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(10, 10))\nplt.plot(c_params, train_accuracies, label='train')\nplt.plot(c_params, test_accuracies, label='test')\nplt.xscale('log')\nplt.legend()\nplt.title('Accuracies for logi_reg model')\n\nText(0.5, 1.0, 'Accuracies for logi_reg model')\n\n\n\n\n\n\n# For C =0.1\n\nlog_reg_0 = LogisticRegression(C=0.1, max_iter=10000, random_state=123)\n\nlog_reg_0.fit(X_train_PCA_0, y_train_sm)\n\nprint(\n    f'log_reg_0 model train score: {log_reg_0.score(X_train_PCA_0, y_train_sm).round(3)}')\nprint(\n    f'log_reg_0 model test score: {log_reg_0.score(X_test_PCA_0, y_test).round(3)}')\n\nlog_reg_0 model train score: 0.99\nlog_reg_0 model test score: 0.985\n\n\n\ny_predicted = log_reg_0.predict(X_test_PCA_0)\n\n# Generate confusion matrix\ncf_matrix = confusion_matrix(y_test, y_predicted)\n\n# label rows and columns\ncf_df = pd.DataFrame(\n    cf_matrix,\n    columns=[\"Predicted 0\", \"Predicted 1\", \"Predicted 2\"],\n    index=[\"True 0\", \"True 1\", \"True 2\"])\n\n\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(log_reg_0, X_test_PCA_0, y_test)\nprint('==========================================================')\n\n# Precision, recall, and F1 score:\nclass_report_log_0 = classification_report(y_test, y_predicted)\nprint(class_report_log_0)\nprint('==========================================================')\n\n# AUC score\ny_proba_train = log_reg_0.predict_proba(X_train_PCA_0)\nauc_train = np.round(roc_auc_score(\n    y_train_sm, y_proba_train, multi_class='ovo'), 3)\nprint('Logistic Regression model AUC score')\nprint(f'roc_auc_score: {auc_train}')\nprint('==========================================================')\n\ncf_df\n\n==========================================================\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      0.99      1850\n           1       0.98      0.98      0.98       580\n           2       0.88      0.96      0.92       163\n\n    accuracy                           0.98      2593\n   macro avg       0.95      0.98      0.96      2593\nweighted avg       0.99      0.98      0.98      2593\n\n==========================================================\nLogistic Regression model AUC score\nroc_auc_score: 1.0\n==========================================================\n\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n\n\n\n  \n    \n      \n      Predicted 0\n      Predicted 1\n      Predicted 2\n    \n  \n  \n    \n      True 0\n      1828\n      8\n      14\n    \n    \n      True 1\n      4\n      569\n      7\n    \n    \n      True 2\n      4\n      3\n      156\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for Logistic Regression\n\nLogistic Regression model classifies using a logistic function, where the output is either 1 or 0, this is usually for binomial classification. I balanced the classes using SMOTE, and then performed a PCA fit on the train and transformed the test set. PCA reduces dimensionality to reduce overfitting, and use fewer features from the dataset. According to the graph above, the number of number of components needed to explain 90% of the variance is 29.\nThe Logistic Regression model log_reg_0 has performed well on the dataset with an accuracy score of 0.99 on training data and 0.985 on the test data.\nFurthermore, hyperparameter tuning showed that a high penalty of C=0.1 was the appropriate value to achieve a higher accuracy score.\nThe model performed well on classifying the DNA types, with an F1 score of 0.99, 0.98, and 0.89 for types 0 (nuclear), 1 (mitochondrial), and 2(chloroplast) respectively. This shows that the model has equivently good precision and recall, meaning that the model classified the classes correctly the majority of time with low percentage of false positives overall.\nThe AUC score of 0.999 shows that the model performed well overall at distinguishing between the classes.\n\n# Visualising the PCA components in a scatter graph\nplt.figure(figsize=(10, 10))\nscatter = plt.scatter(\n    X_train_PCA_0[:, 0], X_train_PCA_0[:, 1], alpha=0.3, c=y_train_sm)\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nclasses = ['0: nuclear DNA', '1: mitochondrial DNA', '2: chloroplast DNA']\nplt.legend(handles=scatter.legend_elements()[0], labels=classes)\nplt.show()\n\n\n\n\nThe principal components analysis of the features shows good separation between the DNAtype classes, this could be why the logistic regression is performing well on this dataset,\n\n# Feature importance\ncoefs = pd.DataFrame(log_reg_0.coef_[0], columns=['coefficients'])\ncoefs\n\nfeat_names = pd.DataFrame(X_train.columns, columns=['feature_names'])\nfeat_names\n\nfeat_coefs = pd.concat([feat_names, coefs], axis=1, ignore_index=True)\n\nfeat_coefs = feat_coefs.sort_values(\n    by=1, ascending=False).set_index(0).head(26)\n\n# plotting coefficients\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(15, 8))\nplt.bar(feat_coefs.index, feat_coefs[1])\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nLogistic Regression adds coefficients (beta1) to each feature, the above bar chart shows the coefficients for the 26 features for the 0: nuclear. the coefficients represent the log odds of a feature change effecting the outcome.\nfor the log_reg_0 model, the above barchart shows that with increase frequency of UUU and UUG for example, there is an increased odds of 0: nuclear classification. The bar grapgh shows how the model made the decision of classification, in terms of these features.\nNcodons appears to be a feature with a positive and high coefficient for the nuclear DNA (0) class, this makes sense because Ncodons represents the size of the sequence, nuclear DNA is the larger of the DNAtypes so the number of codons would reflect this."
  },
  {
    "objectID": "posts/2022-08-04-DnaType-Classification.html#knearest-neighbors-model",
    "href": "posts/2022-08-04-DnaType-Classification.html#knearest-neighbors-model",
    "title": "codon2",
    "section": "kNearest Neighbors model:",
    "text": "kNearest Neighbors model:\n\n# using the already scaled and PCA tranformed sets from previous model\n\n# Determining the ideal n_neighbors value:\n\nn_neighbors = range(1, 100, 2)\n\ntrain_accuracies_0 = []\ntest_accuracies_0 = []\n\nfor n in n_neighbors:\n    KNN_model = KNeighborsClassifier(n_neighbors=n)\n    KNN_model.fit(X_train_PCA_0, y_train_sm)\n\n    train_accuracies_0.append(KNN_model.score(X_train_PCA_0, y_train_sm))\n    test_accuracies_0.append(KNN_model.score(X_test_PCA_0, y_test))\n\n\nplt.figure(figsize=(10, 10))\nplt.plot(n_neighbors, train_accuracies_0, label='train')\nplt.plot(n_neighbors, test_accuracies_0, label='test')\nplt.legend()\nplt.title('Accuracies for KNN_model')\n\nText(0.5, 1.0, 'Accuracies for KNN_model')\n\n\n\n\n\n\nKnn_accuracies_df = pd.DataFrame({'number of neighbors': n_neighbors,\n                                 'Train scores': train_accuracies_0, 'Test scores': test_accuracies_0})\nKnn_accuracies_df.head(5)\n\n\n\n\n\n  \n    \n      \n      number of neighbors\n      Train scores\n      Test scores\n    \n  \n  \n    \n      0\n      1\n      1.000000\n      0.993444\n    \n    \n      1\n      3\n      0.999054\n      0.992673\n    \n    \n      2\n      5\n      0.997883\n      0.993444\n    \n    \n      3\n      7\n      0.996711\n      0.990359\n    \n    \n      4\n      9\n      0.996171\n      0.989202\n    \n  \n\n\n\n\n\nKNN_model_0 = KNeighborsClassifier(n_neighbors=3)\nKNN_model_0.fit(X_train_PCA_0, y_train_sm)\n\nprint(\n    f'KNN_model train score: {KNN_model_0.score(X_train_PCA_0, y_train_sm).round(3)}')\nprint(\n    f'KNN_model test score: {KNN_model_0.score(X_test_PCA_0, y_test).round(3)}')\n\nKNN_model train score: 0.999\nKNN_model test score: 0.993\n\n\n\ny_predicted_0 = KNN_model_0.predict(X_test_PCA_0)\n\n# Generate confusion matrix\ncf_matrix_0 = confusion_matrix(y_test, y_predicted_0)\n\n# label rows and columns\ncf_df_0 = pd.DataFrame(\n    cf_matrix_0,\n    columns=[\"Predicted 0\", \"Predicted 1\", \"Predicted 2\"],\n    index=[\"True 0\", \"True 1\", \"True 2\"])\n\n\nprint('==========================================================')\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(KNN_model_0, X_test_PCA_0, y_test)\n\n# Precision, recall, and F1 scores\nclass_report_knn_0 = classification_report(y_test, y_predicted_0)\nprint(class_report_knn_0)\nprint('==========================================================')\n\n# AUC score\ny_proba_train_0 = KNN_model_0.predict_proba(X_train_PCA_0)\nauc_train_0 = np.round(roc_auc_score(\n    y_train_sm, y_proba_train_0, multi_class='ovo'), 3)\nprint('KNN model AUC score')\nprint(f'roc_auc_score: {auc_train_0}')\nprint('==========================================================')\n\n\ncf_df_0\n\n==========================================================\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      1.00      1850\n           1       0.99      0.99      0.99       580\n           2       0.96      0.99      0.97       163\n\n    accuracy                           0.99      2593\n   macro avg       0.98      0.99      0.99      2593\nweighted avg       0.99      0.99      0.99      2593\n\n==========================================================\n\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\nKNN model AUC score\nroc_auc_score: 1.0\n==========================================================\n\n\n\n\n\n\n  \n    \n      \n      Predicted 0\n      Predicted 1\n      Predicted 2\n    \n  \n  \n    \n      True 0\n      1839\n      7\n      4\n    \n    \n      True 1\n      3\n      574\n      3\n    \n    \n      True 2\n      1\n      1\n      161\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for kNearest Neighbors:\n\nkNearest Neighbor model classifies by determining the distances between the data points: data points closest to each other are classified as being from the same class, depending on the number of nearest neighbors selected. Because KNN usese distances between data points, I scaled the data performed PCA and used the SMOTE transformed data.\nThe KNN_model_0 performed very well, with an accuracy score of 0.999 for the training data and 0.993 for the test data, for a value of n_neighbors = 3.\nThe model performed well on all of the classes, with improvement in the precision and recall scores: F1 scores of 1.0, 0.99, and 0.97 for 0 (nuclear), 1 (mitochondrial), and 2 (chloroplast) respectively.\nOverall the KNN_model_0 performed well on distinguising between the classes with an AUC score of 1.0."
  },
  {
    "objectID": "posts/2022-08-04-DnaType-Classification.html#xgboost-model",
    "href": "posts/2022-08-04-DnaType-Classification.html#xgboost-model",
    "title": "codon2",
    "section": "XGBoost model:",
    "text": "XGBoost model:\n\nmaxdepth = range(1, 10)\n\ntrain_accuracies_1 = []\ntest_accuracies_1 = []\n\nfor d in maxdepth:\n    XGB_model = XGBClassifier(max_depth=d, use_label_encoder=False)\n    XGB_model.fit(X_train, y_train)\n\n    train_accuracies_1.append(XGB_model.score(X_train, y_train))\n    test_accuracies_1.append(XGB_model.score(X_test, y_test))\n\n\nplt.figure(figsize=(10, 10))\nplt.plot(maxdepth, train_accuracies_1, label='train')\nplt.plot(maxdepth, test_accuracies_1, label='test')\nplt.legend()\nplt.title('Accuracies for XGBoost model')\n\n\nXGB_accuracies_df = pd.DataFrame(\n    {'max Depth': maxdepth, 'Train scores': train_accuracies_1, 'Test scores': test_accuracies_1})\nXGB_accuracies_df.sort_values(by='Test scores', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      max Depth\n      Train scores\n      Test scores\n    \n  \n  \n    \n      2\n      3\n      1.000000\n      0.994601\n    \n    \n      4\n      5\n      1.000000\n      0.994601\n    \n    \n      5\n      6\n      1.000000\n      0.994601\n    \n    \n      6\n      7\n      1.000000\n      0.994601\n    \n    \n      8\n      9\n      1.000000\n      0.994601\n    \n    \n      3\n      4\n      1.000000\n      0.994215\n    \n    \n      1\n      2\n      0.999807\n      0.993444\n    \n    \n      7\n      8\n      1.000000\n      0.993444\n    \n    \n      0\n      1\n      0.995565\n      0.991516\n    \n  \n\n\n\n\n\n\n\n\n# XGBoost with max+depth = 3\n\nXGB_model_0 = XGBClassifier(max_depth=3, use_label_encoder=False)\nXGB_model_0.fit(X_train, y_train)\n\nprint(\n    f'XGBoost train accuracy: {XGB_model_0.score(X_train, y_train).round(3)}')\nprint(f'XGBoost test accuracy: {XGB_model_0.score(X_test, y_test).round(3)}')\n\nXGBoost train accuracy: 1.0\nXGBoost test accuracy: 0.995\n\n\n\ny_predicted_1 = XGB_model_0.predict(X_test)\n\n# Generate confusion matrix\ncf_matrix_1 = confusion_matrix(y_test, y_predicted_1)\n\n# label rows and columns\ncf_df_1 = pd.DataFrame(\n    cf_matrix_1,\n    columns=[\"Predicted 0\", \"Predicted 1\", \"Predicted 2\"],\n    index=[\"True 0\", \"True 1\", \"True 2\"])\n\n\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(XGB_model, X_test, y_test)\n\nprint('==========================================================')\n\n\n# Precision Recall and F1 scores\nclass_report_xgb_0 = classification_report(y_test, y_predicted_1)\nprint(class_report_xgb_0)\nprint('==========================================================')\n\n\n# AUC score\ny_proba_train_1 = XGB_model_0.predict_proba(X_train)\nauc_train_1 = np.round(roc_auc_score(\n    y_train, y_proba_train_1, multi_class='ovo'), 3)\nprint('XGBoost model AUC score')\nprint(f'roc_auc_score: {auc_train_1}')\n\nprint('==========================================================')\ncf_df_1\n\n==========================================================\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      1850\n           1       0.99      0.99      0.99       580\n           2       0.99      0.96      0.97       163\n\n    accuracy                           0.99      2593\n   macro avg       0.99      0.98      0.99      2593\nweighted avg       0.99      0.99      0.99      2593\n\n==========================================================\nXGBoost model AUC score\nroc_auc_score: 1.0\n==========================================================\n\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n\n\n\n  \n    \n      \n      Predicted 0\n      Predicted 1\n      Predicted 2\n    \n  \n  \n    \n      True 0\n      1848\n      1\n      1\n    \n    \n      True 1\n      4\n      575\n      1\n    \n    \n      True 2\n      5\n      2\n      156\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for XGBoost model:\n\nXGBoost model is an ensemble model, it classifies data using decision trees in ensembles. Using max_depth as the hyperparameter, which determines the maximium depth of the trees. I did not use the scaled, oversamples, and PCA transformed data for this model, as this is not needed.\nFor XGB_model_0 the appropriate value for max_depth was determined to be 3, this gave an accuracy score of 1.0 for training data and 0.995 for test data. The model did not overfit on the training data, the model is performing just as well on the training data and on unseen data (test set).\nThe model performed very well on the data, showing high F1 scores, identical to the KNN_model_0, which shows that the use of two different classification models with different approaches are able to classify the target data really well.\nFurthermore, this model performed well on distinguishing between the classes with an AUC score of 1.0.\n\nfeat_imprt = pd.DataFrame(\n    XGB_model_0.feature_importances_).sort_values(by=0, ascending=False)\n\nfeat_names_0 = pd.DataFrame(X_train.columns, columns=['feature_names'])\n\n# join inner because the ndarray lengths are different\nfeat_coefs_0 = pd.concat([feat_names_0, feat_imprt],\n                         axis=1, ignore_index=True, join='inner')\n\nfeat_coefs_0 = feat_coefs_0.sort_values(\n    by=1, ascending=False).set_index(0).head(20)\n\n# plotting coefficients\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(15, 8))\nplt.bar(feat_coefs_0.index, feat_imprt[0][:20])\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nFeature importance for XGBoost models shows the features that the model deemed most important in making the classification decision.\nthe above bar chart shows the codons importance according to gain metric, which is the relative contribution of each feature for each tree in the ensemble (XGBoost) model. This means that UGA having the highest values of importance were the most important in generating a prediction of the outcome classification."
  },
  {
    "objectID": "posts/2022-08-04-DnaType-Classification.html#ensemble-model",
    "href": "posts/2022-08-04-DnaType-Classification.html#ensemble-model",
    "title": "codon2",
    "section": "Ensemble Model",
    "text": "Ensemble Model\n\n# We instantiate the base models, along with their names\nbase_models = [('KNN', KNN_model_0),\n               ('XGBoost', XGB_model_0)\n               ]\n\n\n# building the stacked model\nstacked_model = StackingClassifier(\n    estimators=base_models,\n    final_estimator=LogisticRegression(random_state=123, max_iter=10000),\n    passthrough=True)\n\nstacked_model.fit(X_train, y_train)\n\nStackingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=3)),\n                               ('XGBoost',\n                                XGBClassifier(base_score=0.5, booster='gbtree',\n                                              callbacks=None,\n                                              colsample_bylevel=1,\n                                              colsample_bynode=1,\n                                              colsample_bytree=1,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None, gamma=0,\n                                              gpu_id=-1,\n                                              grow_policy='depthwise',\n                                              importance_type=None,\n                                              interaction_c...\n                                              learning_rate=0.300000012,\n                                              max_bin=256, max_cat_to_onehot=4,\n                                              max_delta_step=0, max_depth=3,\n                                              max_leaves=0, min_child_weight=1,\n                                              missing=nan,\n                                              monotone_constraints='()',\n                                              n_estimators=100, n_jobs=0,\n                                              num_parallel_tree=1,\n                                              objective='multi:softprob',\n                                              predictor='auto', random_state=0,\n                                              reg_alpha=0, ...))],\n                   final_estimator=LogisticRegression(max_iter=10000,\n                                                      random_state=123),\n                   passthrough=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StackingClassifierStackingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=3)),\n                               ('XGBoost',\n                                XGBClassifier(base_score=0.5, booster='gbtree',\n                                              callbacks=None,\n                                              colsample_bylevel=1,\n                                              colsample_bynode=1,\n                                              colsample_bytree=1,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None, gamma=0,\n                                              gpu_id=-1,\n                                              grow_policy='depthwise',\n                                              importance_type=None,\n                                              interaction_c...\n                                              learning_rate=0.300000012,\n                                              max_bin=256, max_cat_to_onehot=4,\n                                              max_delta_step=0, max_depth=3,\n                                              max_leaves=0, min_child_weight=1,\n                                              missing=nan,\n                                              monotone_constraints='()',\n                                              n_estimators=100, n_jobs=0,\n                                              num_parallel_tree=1,\n                                              objective='multi:softprob',\n                                              predictor='auto', random_state=0,\n                                              reg_alpha=0, ...))],\n                   final_estimator=LogisticRegression(max_iter=10000,\n                                                      random_state=123),\n                   passthrough=True)KNNKNeighborsClassifierKNeighborsClassifier(n_neighbors=3)XGBoostXGBClassifierXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n              predictor='auto', random_state=0, reg_alpha=0, ...)final_estimatorLogisticRegressionLogisticRegression(max_iter=10000, random_state=123)\n\n\n\nprint(\n    f'Stacked model train accuracy: {stacked_model.score(X_train, y_train).round(3)}')\nprint(\n    f'Stacked model test accuracy: {stacked_model.score(X_test, y_test).round(3)}')\n\nStacked model train accuracy: 1.0\nStacked model test accuracy: 0.995\n\n\n\ny_predicted_2 = stacked_model.predict(X_test)\n\n# Generate confusion matrix\ncf_matrix_2 = confusion_matrix(y_test, y_predicted_2)\n\n# label rows and columns\ncf_df_2 = pd.DataFrame(\n    cf_matrix_2,\n    columns=[\"Predicted 0\", \"Predicted 1\", \"Predicted 2\"],\n    index=[\"True 0\", \"True 1\", \"True 2\"])\n\n\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(stacked_model, X_test, y_test)\n\nprint('==========================================================')\n\n\n# Precision Recall and F1 scores\nclass_report_stack_0 = classification_report(y_test, y_predicted_2)\nprint(class_report_stack_0)\nprint('==========================================================')\n\n\n# AUC score\ny_proba_train_2 = stacked_model.predict_proba(X_train)\nauc_train_2 = np.round(roc_auc_score(\n    y_train, y_proba_train_2, multi_class='ovo'), 3)\nprint('AUC score for Stacked model')\nprint(f'roc_auc_score: {auc_train_2}')\nprint('==========================================================')\n\ncf_df_2\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n==========================================================\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      1850\n           1       0.99      0.99      0.99       580\n           2       0.99      0.96      0.97       163\n\n    accuracy                           0.99      2593\n   macro avg       0.99      0.98      0.99      2593\nweighted avg       0.99      0.99      0.99      2593\n\n==========================================================\nAUC score for Stacked model\nroc_auc_score: 1.0\n==========================================================\n\n\n\n\n\n\n  \n    \n      \n      Predicted 0\n      Predicted 1\n      Predicted 2\n    \n  \n  \n    \n      True 0\n      1848\n      1\n      1\n    \n    \n      True 1\n      4\n      575\n      1\n    \n    \n      True 2\n      5\n      2\n      156\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for the sctacked (ensemble) model:\n\nFor the ensemble or stacked model, I used the models I already trained and tuned, with the exception of the logistic regression model log_reg_0; I decided to exclude this model as it was the model that underperformed on classifying the target variable.\nI only included XGB_model_0 and KNN_model_0 as base models, with a final estimator being a default Logistic Regression model. Each of the base estimators is different in the way it approaches the classification.\nThe ensemble model classifies by taking the outputs from both the XGBoost model and KNN model as probabilites of each class as an input, the final estimator being a logistic regression then uses logistic function to classify each class.\nstacked_model achieved an accuracy score of 1.0 for the train data and 0.995 for the test data.\nThe stacked model did well in distinguishing between the classes with an AUC of 1.0."
  },
  {
    "objectID": "posts/2022-08-04-DnaType-Classification.html#comparing-models",
    "href": "posts/2022-08-04-DnaType-Classification.html#comparing-models",
    "title": "codon2",
    "section": "Comparing Models",
    "text": "Comparing Models\n\ncls = classification_report(y_test, y_predicted, output_dict=True)\ncls_0 = classification_report(y_test, y_predicted_0, output_dict=True)\ncls_1 = classification_report(y_test, y_predicted_1, output_dict=True)\ncls_2 = classification_report(y_test, y_predicted_2, output_dict=True)\n\n\ncomparison_df = pd.DataFrame({'Precision':\n                             [cls['weighted avg']['precision'],\n                              cls_0['weighted avg']['precision'],\n                              cls_1['weighted avg']['precision'],\n                              cls_2['weighted avg']['precision']],\n                              'Recall':\n                             [cls['weighted avg']['recall'],\n                              cls_0['weighted avg']['recall'],\n                              cls_1['weighted avg']['recall'],\n                              cls_2['weighted avg']['recall']],\n                             'F1 score':\n                              [cls['weighted avg']['f1-score'],\n                              cls_0['weighted avg']['f1-score'],\n                               cls_1['weighted avg']['f1-score'],\n                              cls_2['weighted avg']['f1-score']],\n                              'Accuracy':\n                              [cls['accuracy'],\n                               cls_0['accuracy'],\n                               cls_1['accuracy'],\n                               cls_2['accuracy']],\n                              'AUC score':\n                              [auc_train,\n                               auc_train_0,\n                               auc_train_1,\n                               auc_train_2]\n                              },\n                             index=['Logistic Regression', 'kNearest Neighbors', 'XGBoost', 'Stacked model'])\n\ncomparison_df.round(3).sort_values(by='F1 score', ascending=False)\n\n\n\n\n\n  \n    \n      \n      Precision\n      Recall\n      F1 score\n      Accuracy\n      AUC score\n    \n  \n  \n    \n      XGBoost\n      0.995\n      0.995\n      0.995\n      0.995\n      1.0\n    \n    \n      Stacked model\n      0.995\n      0.995\n      0.995\n      0.995\n      1.0\n    \n    \n      kNearest Neighbors\n      0.993\n      0.993\n      0.993\n      0.993\n      1.0\n    \n    \n      Logistic Regression\n      0.985\n      0.985\n      0.985\n      0.985\n      1.0\n    \n  \n\n\n\n\nAccording to the F1 score XGBoost model and the Stacked model performed the best and similarly. This makes sense, contrary to the Kingdom classification, the DNAtype only has three classes, which means that the XGBoost would perform better in this classification as it can get a pure node easier.\nLooking at the vanilla models performed at the beginning the accuracy score for the kNearest Neighbor performed was 99.3%, so I only achived an improvement of %0.2 for the DNAtype classses classification."
  },
  {
    "objectID": "posts/2022-08-04-DnaType-Classification.html#saving-models",
    "href": "posts/2022-08-04-DnaType-Classification.html#saving-models",
    "title": "codon2",
    "section": "Saving Models:",
    "text": "Saving Models:\n\n# Logistic Regression model\nlog_reg_dna = 'finalized_log_reg_dna.sav'\njoblib.dump(log_reg_0, log_reg_dna)\n\n\n# kNearest Neighbor model\nknn_dna = 'finalized_knn_dna.sav'\njoblib.dump(KNN_model_0, knn_dna)\n\n# XGBoost model\nxgboost_dna = 'finalized_xgboost_dna.sav'\njoblib.dump(XGB_model_0, xgboost_dna)\n\n# Ensemble model\nstacked_dna = 'finalized_stacked_dna.sav'\njoblib.dump(stacked_model, stacked_dna)\n\n['finalized_stacked_dna.sav']"
  },
  {
    "objectID": "posts/2022-08-03-Exploratory-Data-Analysis.html#exploratory-data-analysis-method",
    "href": "posts/2022-08-03-Exploratory-Data-Analysis.html#exploratory-data-analysis-method",
    "title": "codon2",
    "section": "Exploratory Data Analysis Method:",
    "text": "Exploratory Data Analysis Method:\nIn order to explore the data set and understand what it contains I will be aiming to answer the following questions:\n\nWhat is the distribution of the target classes for Kingdom?\nWhat is the distribution of the target classes for DNAtype?\nWhat does the distribution for the Ncodons column look like?\nAre there codons that have a higher occurance in certain DNA types?\nAre there codons that have a higher occurence in certain Kingdoms?\n\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import (GridSearchCV, cross_val_score,\n                                     train_test_split)\nfrom sklearn.metrics import (accuracy_score, auc, classification_report,\n                             confusion_matrix, f1_score, plot_confusion_matrix,\n                             plot_roc_curve, precision_score, recall_score,\n                             roc_auc_score, roc_curve)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nimport warnings\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom scipy import stats\n\n%matplotlib inline\n\n\n# warnings.filterwarnings(\"ignore\")\nnp.random.seed(123)\n\n\n# importing the cleaned dataframe dfcodon1 and renaming it to codon\n\ncodon = joblib.load('../data/dfcodon1.pkl')\n\n\n# Dataframe Shape\nprint(\n    f'The dataframe contains {codon.shape[0]} rows and {codon.shape[1]} columns')\n\nThe dataframe contains 13028 rows and 69 columns\n\n\n\n# peeking at the dataframe\ncodon.sample(5)\n\n\n\n\n\n  \n    \n      \n      Kingdom\n      DNAtype\n      SpeciesID\n      Ncodons\n      SpeciesName\n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      11544\n      vertebrate\n      1\n      71258\n      4460\n      mitochondrion Hypopomus occidentalis\n      0.01951\n      0.02623\n      0.02354\n      0.01076\n      0.03274\n      ...\n      0.00740\n      0.00000\n      0.00000\n      0.00090\n      0.00269\n      0.01726\n      0.00359\n      0.00404\n      0.00314\n      0.03139\n    \n    \n      12258\n      mammal\n      0\n      30521\n      14077\n      Bos grunniens\n      0.01698\n      0.01954\n      0.00789\n      0.01435\n      0.01279\n      ...\n      0.00902\n      0.01179\n      0.00980\n      0.02579\n      0.02941\n      0.02998\n      0.03467\n      0.00107\n      0.00078\n      0.00199\n    \n    \n      2177\n      virus\n      0\n      394337\n      1359\n      Tomato leaf curl New Delhi virus-[Multan;Luffa]\n      0.02134\n      0.02428\n      0.00736\n      0.01177\n      0.01545\n      ...\n      0.00736\n      0.01472\n      0.01325\n      0.02943\n      0.01840\n      0.02796\n      0.01545\n      0.00221\n      0.00147\n      0.00221\n    \n    \n      11615\n      vertebrate\n      0\n      7792\n      11800\n      Heterodontus francisci\n      0.01661\n      0.01720\n      0.00712\n      0.01025\n      0.00941\n      ...\n      0.00890\n      0.01517\n      0.01025\n      0.02017\n      0.02339\n      0.02881\n      0.02661\n      0.00068\n      0.00042\n      0.00144\n    \n    \n      3172\n      bacteria\n      0\n      320122\n      13771\n      Clostridium phage phi CD119\n      0.03544\n      0.00588\n      0.04480\n      0.01089\n      0.01343\n      ...\n      0.00029\n      0.02926\n      0.00508\n      0.04647\n      0.01017\n      0.06739\n      0.01808\n      0.00334\n      0.00182\n      0.00058\n    \n  \n\n5 rows × 69 columns\n\n\n\n\n# Describing the dataframe\ncodon.describe()\n\n\n\n\n\n  \n    \n      \n      DNAtype\n      SpeciesID\n      Ncodons\n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      CUC\n      CUA\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      count\n      13028.000000\n      13028.000000\n      1.302800e+04\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      ...\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n    \n    \n      mean\n      0.367209\n      130451.105926\n      7.960576e+04\n      0.024819\n      0.023440\n      0.020635\n      0.014104\n      0.017822\n      0.018286\n      0.019043\n      ...\n      0.005452\n      0.009929\n      0.006422\n      0.024183\n      0.021164\n      0.028292\n      0.021683\n      0.001640\n      0.000590\n      0.006178\n    \n    \n      std\n      0.688726\n      124787.086107\n      7.197010e+05\n      0.017627\n      0.011597\n      0.020709\n      0.009279\n      0.010587\n      0.014572\n      0.024251\n      ...\n      0.006601\n      0.008574\n      0.006387\n      0.013826\n      0.013038\n      0.014343\n      0.015018\n      0.001785\n      0.000882\n      0.010344\n    \n    \n      min\n      0.000000\n      7.000000\n      1.000000e+03\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.000000\n      28850.750000\n      1.602000e+03\n      0.013910\n      0.015380\n      0.005610\n      0.007108\n      0.010890\n      0.007830\n      0.005300\n      ...\n      0.001220\n      0.001690\n      0.001168\n      0.012390\n      0.011860\n      0.017360\n      0.009710\n      0.000560\n      0.000000\n      0.000410\n    \n    \n      50%\n      0.000000\n      81971.500000\n      2.927500e+03\n      0.021750\n      0.021905\n      0.015260\n      0.013360\n      0.016130\n      0.014560\n      0.009680\n      ...\n      0.003530\n      0.009270\n      0.004540\n      0.025435\n      0.019070\n      0.026085\n      0.020540\n      0.001380\n      0.000420\n      0.001130\n    \n    \n      75%\n      1.000000\n      222891.250000\n      9.120000e+03\n      0.031310\n      0.029210\n      0.029485\n      0.019803\n      0.022730\n      0.025110\n      0.017245\n      ...\n      0.007150\n      0.015922\n      0.010250\n      0.034190\n      0.027690\n      0.036800\n      0.031122\n      0.002370\n      0.000830\n      0.002890\n    \n    \n      max\n      12.000000\n      465364.000000\n      4.066258e+07\n      0.217300\n      0.091690\n      0.151330\n      0.101190\n      0.089780\n      0.100350\n      0.163920\n      ...\n      0.055540\n      0.098830\n      0.058430\n      0.185660\n      0.113840\n      0.144890\n      0.158550\n      0.045200\n      0.025610\n      0.106700\n    \n  \n\n8 rows × 67 columns\n\n\n\n\n# Setting a theme for all the visualisations\nsns.set_theme(style=\"darkgrid\")\nsns.set(rc={'figure.figsize': (11.7, 8.27)})\n\n\nPlan for EDA: - Univariate alalysis - Bivariate analysis - Check codon distribution for each Kingdom - Check codon distribution for each DNAtype"
  },
  {
    "objectID": "posts/2022-08-03-Exploratory-Data-Analysis.html#univariate-analysis",
    "href": "posts/2022-08-03-Exploratory-Data-Analysis.html#univariate-analysis",
    "title": "codon2",
    "section": "Univariate Analysis",
    "text": "Univariate Analysis\nDue to the dataframe containing 69 columns, this will be hard to visualise without breaking up the dataframe into focus points. So, I will isolate the codon columns into a separate dataframe and the rest of the columns into another one, and then do univariate analysis.\n\nSeparating the codon only columns\n\n\njustcodon = codon.drop(\n    columns=['Kingdom', 'DNAtype', 'SpeciesID', 'Ncodons', 'SpeciesName'], axis=1)\njustcodon.sample(3)\n\n\n\n\n\n  \n    \n      \n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      CUC\n      CUA\n      CUG\n      AUU\n      AUC\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      1830\n      0.02869\n      0.01967\n      0.01742\n      0.01527\n      0.02859\n      0.01312\n      0.01199\n      0.00932\n      0.02664\n      0.01178\n      ...\n      0.00113\n      0.01445\n      0.00451\n      0.03043\n      0.02244\n      0.02613\n      0.02152\n      0.00092\n      0.00000\n      0.00020\n    \n    \n      2592\n      0.00948\n      0.02212\n      0.00316\n      0.01422\n      0.00790\n      0.01106\n      0.00790\n      0.02291\n      0.01106\n      0.02212\n      ...\n      0.00316\n      0.01422\n      0.00790\n      0.00711\n      0.02765\n      0.01896\n      0.02291\n      0.00079\n      0.00000\n      0.00000\n    \n    \n      1004\n      0.01659\n      0.02461\n      0.01495\n      0.00981\n      0.01207\n      0.01433\n      0.01130\n      0.00940\n      0.01644\n      0.02250\n      ...\n      0.00817\n      0.01521\n      0.01433\n      0.02666\n      0.02214\n      0.02764\n      0.01741\n      0.00319\n      0.00108\n      0.00057\n    \n  \n\n3 rows × 64 columns\n\n\n\n\nSeparating the non codon into dataframe\n\n\nnon_codon = codon.drop(columns=codon.columns[5:], axis=1)\nnon_codon.head(3)\n\n\n\n\n\n  \n    \n      \n      Kingdom\n      DNAtype\n      SpeciesID\n      Ncodons\n      SpeciesName\n    \n  \n  \n    \n      0\n      virus\n      0\n      100217\n      1995\n      Epizootic haematopoietic necrosis virus\n    \n    \n      1\n      virus\n      0\n      100220\n      1474\n      Bohle iridovirus\n    \n    \n      2\n      virus\n      0\n      100755\n      4862\n      Sweet potato leaf curl virus\n    \n  \n\n\n\n\n\nVisualising non_codon\n\nWhat is the distribution of the target classes for Kingdom?\nWhat is the distribution of the target classes for DNAtype?\n\nI will not be looking at SepciesID or SpeciesName as these columns won’t have a distribution that offers any noteworthy insights.\n\nfor col in non_codon[['Kingdom', 'DNAtype', 'Ncodons']]:\n    plt.figure()\n    plt.title(f'Feature: {col}')\n    plt.hist(codon[col], bins=20)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nKingdom shows the counts of the different kingdom, with bacterial and viral being the ones with the most data points in this dataset.\nDNAtype has 12 unique values, however there only seems to show 0, 1, and 2. I will have to apply a log to this graph to see the counts of the other dna types and they could be muted by the high counts from the top three.\nNcodons another distribution that is heavily skewed, I will visualise this one with a log transformation.\n\n\n# Looking at the Kingdom column distribution using log transformation:\nnon_codon['Kingdom'].value_counts().plot(kind='bar', log=True)\nplt.title('Kingdom Column Distribution')\nplt.xlabel('Kingdoms')\nplt.ylabel('Log Counts')\n\n# Proportions of the Kingdom counts:\nnon_codon['Kingdom'].value_counts(normalize=True)*100\n\nbacteria        22.413264\nvirus           21.737796\nplant           19.365981\nvertebrate      15.942585\ninvertebrate    10.323918\nmammal           4.390543\nphage            1.688671\nrodent           1.650292\nprimate          1.381640\narchaea          0.967148\nplasmid          0.138164\nName: Kingdom, dtype: float64\n\n\n\n\n\nThe dataset does contain rows for kingdoms beside the top three, however there is a high imbalance here. with some of the classes having less than 5%. I will drop plm since it is <2% (18 entries).\n\n# Looking at DNAtype column distribution with log transformation:\nnon_codon['DNAtype'].value_counts().plot(kind='bar', log=True)\nplt.title('DNAtype column distribution')\nplt.xlabel('DNA Type')\nplt.ylabel('Log Counts')\n\nprint('Proportions of DNAtype counts')\n# Proportions of the DNAtype counts:\nnon_codon['DNAtype'].value_counts(normalize=True)*100\n\nProportions of DNAtype counts\n\n\n0     71.131409\n1     22.252072\n2      6.263433\n4      0.237949\n12     0.038379\n3      0.015352\n9      0.015352\n5      0.015352\n11     0.015352\n6      0.007676\n7      0.007676\nName: DNAtype, dtype: float64\n\n\n\n\n\n\nThoughts:\n\nThe columns Kingdoms and DNAtype will be the targets (separately) that I will be aiming to classify. Looking at the distributions, and the imbalances between the classes I will aim to drop some classes and the following is my reasoning.\nIn terms of kingdoms, the Dataset contains the following: - pri: primate - rod: rodent - mam: mammalian - vrt: vertebrate - inv: invertebrate - pln: plant - bct: bacteria - vrl: virus - phg :bacteriophage - arc: archaea - plm: plasmid\nprimate ,rodent, and mammalian can be grouped and renamed under the vertebrate kingdom, because they are vertebrates and are all mammals - vrt = vrt + pri + rod + mam\nThe column will then have the following classes: - vrt: vertebrate - inv: invertebrate - pln: plant - bct: bacteria - vrl: virus - phg :bacteriophage - arc: archaea - plm: plasmid\nI will further disregard the rows that are class plm, as there are far too little entries (18 only).\nFinal classes will be:\n\nvrt: vertebrate\ninv: invertebrate\npln: plant\nbct: bacteria\nvrl: virus\nphg :bacteriophage\narc: archaea\n\nFinally I will look at the distribution after these changes.\nFor the column DNAtype, it contains the following classes: - 0: nuclear - 1: mitochondrion - 2: chloroplast - 3: cyanelle - 4: plastid - 5: nucleomorph - 6: secondary endosymbiont - 7: chromoplast - 8: leukoplast - 9: NA - 10: proplastid - 11: apicoplast - 12: kinetoplast\nAs seen in the DNAtype distribution, there is a high imbalance between these classes. I will focus here on the top three which are 0 (nuclear), 1 (mitochonrion), and 2(chloroplast). These three DNA types that are present in all of the kingdoms in our dataset, so I will go ahead with these as they are representative. I will drop all rows that do not contain 0, 1, or 2, as the are less than 1% of the dataset.\n\nKingdom Column\n\n\n# changing the value names as stated above:\nnon_codon = non_codon.replace(['primate', 'rodend', 'mammal'], 'vertebrate')\n\n# dropping rows with value plm\nnon_codon = non_codon.drop(\n    non_codon.loc[non_codon['Kingdom'] == 'plasmid'].index)\n\n\nnon_codon['Kingdom'].value_counts().plot(\n    kind='bar', log=True)  # no log applied\nplt.title('Kingdom Column Distribution')\nplt.xlabel('Kingdoms')\nplt.ylabel('Log Counts')\n\nText(0, 0.5, 'Log Counts')\n\n\n\n\n\n\nnon_codon['Kingdom'].value_counts()\n\nbacteria        2920\nvirus           2832\nvertebrate      2829\nplant           2523\ninvertebrate    1345\nphage            220\nrodent           215\narchaea          126\nName: Kingdom, dtype: int64\n\n\nAs there are still imbalances in the data, this makes sense since genome lengths differ between species, so this imbalance is inevitable. For example for archea there are only 209 species,\n\nNote: these changes have currently been made to the dataframe non_codon I will now apply these changes to the main dataframe codon\n\n\n# changing the value names as stated above:\ncodon = codon.replace(['primate', 'rodent', 'mammal'], 'vertebrate')\n\n# dropping rows with value plm\ncodon = codon.drop(codon.loc[codon['Kingdom'] == 'plasmid'].index)\n\n\nDNAtype Column\n\n\n# Dropping all rows that do not contain 0, 1, or 2 as DNAtype:\nnon_codon = non_codon.drop(non_codon.loc[non_codon['DNAtype'] > 2].index)\n\n\n# Checking distribution:\nnon_codon['DNAtype'].value_counts().plot(\n    kind='bar', log=True)  # log transformation not applied\nplt.title('DNAtype column distribution')\nplt.xlabel('DNA Type')\nplt.ylabel('Log Counts')\n\nText(0, 0.5, 'Log Counts')\n\n\n\n\n\nThe classes are highly imbalanced, but this can be adjusted later on during the modeling section.\nAgain, I will apply these changes to the original codon dataframe for consistency.\n\n# Dropping rows in DNAtype with values larger than 2:\ncodon = codon.drop(codon.loc[codon['DNAtype'] > 2].index)\n\n\n# Checking distribution in codon dataframe:\ncodon['DNAtype'].value_counts().plot(kind='bar')\n\n<AxesSubplot:>\n\n\n\n\n\n\nWhat does the distribution for the Ncodons column look like?\n\n\n# checking log transformed distribution\nplt.figure()\nplt.hist(non_codon['Ncodons'], bins=100)\nplt.show()\n\nprint('===============================================')\nnon_codon['Ncodons'] = np.log(non_codon['Ncodons']+1)\nnon_codon['Ncodons']\n\n\n\n\n===============================================\n\n\n0         7.598900\n1         7.296413\n2         8.489411\n3         7.557995\n4        10.035918\n           ...    \n13023     7.001246\n13024     7.634337\n13025     7.430707\n13026    17.520819\n13027    16.012624\nName: Ncodons, Length: 12964, dtype: float64\n\n\nNcodons is highly skewed, this is because there are more species with smaller genomes and so smaller number of codons in the sequence.\nApplying the log transformation change to the original dataframe codon\n\ncodon['Ncodons'] = np.log(codon['Ncodons']+1)\ncodon['Ncodons']\n\n0         7.598900\n1         7.296413\n2         8.489411\n3         7.557995\n4        10.035918\n           ...    \n13023     7.001246\n13024     7.634337\n13025     7.430707\n13026    17.520819\n13027    16.012624\nName: Ncodons, Length: 12964, dtype: float64\n\n\nFinally, the dataframe shape after transformation.\n\n# the dataframe containing only the non_codon columns\nprint(\n    f'The non_codon dataframe contains {non_codon.shape[0]} rows and {non_codon.shape[1]}')\n# the original codon dataframe\nprint(\n    f'The codon dataframe contains {codon.shape[0]} rows and {codon.shape[1]}')\n\nThe non_codon dataframe contains 12964 rows and 5\nThe codon dataframe contains 12964 rows and 69\n\n\n\n\n\nVisualising non_codon\n\njustcodon.describe()\n\n\n\n\n\n  \n    \n      \n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      CUC\n      CUA\n      CUG\n      AUU\n      AUC\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      count\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      ...\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n    \n    \n      mean\n      0.024819\n      0.023440\n      0.020635\n      0.014104\n      0.017822\n      0.018286\n      0.019043\n      0.018455\n      0.028354\n      0.025036\n      ...\n      0.005452\n      0.009929\n      0.006422\n      0.024183\n      0.021164\n      0.028292\n      0.021683\n      0.001640\n      0.000590\n      0.006178\n    \n    \n      std\n      0.017627\n      0.011597\n      0.020709\n      0.009279\n      0.010587\n      0.014572\n      0.024251\n      0.016583\n      0.017505\n      0.014595\n      ...\n      0.006601\n      0.008574\n      0.006387\n      0.013826\n      0.013038\n      0.014343\n      0.015018\n      0.001785\n      0.000882\n      0.010344\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.013910\n      0.015380\n      0.005610\n      0.007108\n      0.010890\n      0.007830\n      0.005300\n      0.007180\n      0.016370\n      0.015130\n      ...\n      0.001220\n      0.001690\n      0.001168\n      0.012390\n      0.011860\n      0.017360\n      0.009710\n      0.000560\n      0.000000\n      0.000410\n    \n    \n      50%\n      0.021750\n      0.021905\n      0.015260\n      0.013360\n      0.016130\n      0.014560\n      0.009680\n      0.012800\n      0.025480\n      0.021540\n      ...\n      0.003530\n      0.009270\n      0.004540\n      0.025435\n      0.019070\n      0.026085\n      0.020540\n      0.001380\n      0.000420\n      0.001130\n    \n    \n      75%\n      0.031310\n      0.029210\n      0.029485\n      0.019803\n      0.022730\n      0.025110\n      0.017245\n      0.024330\n      0.038113\n      0.031860\n      ...\n      0.007150\n      0.015922\n      0.010250\n      0.034190\n      0.027690\n      0.036800\n      0.031122\n      0.002370\n      0.000830\n      0.002890\n    \n    \n      max\n      0.217300\n      0.091690\n      0.151330\n      0.101190\n      0.089780\n      0.100350\n      0.163920\n      0.107370\n      0.154060\n      0.088600\n      ...\n      0.055540\n      0.098830\n      0.058430\n      0.185660\n      0.113840\n      0.144890\n      0.158550\n      0.045200\n      0.025610\n      0.106700\n    \n  \n\n8 rows × 64 columns\n\n\n\nThe justcodon dataframe created from the original codon dataframe contains all the codon columns which are 64 columns in total, each column representing one of the 64 codons.\nThe values in these columns are frequencies of the codon occurrence in each organism’s genome (from nuclear, mitochondrial, or chloroplast RNA)\nThe frequencies are caculated by taking a count of the total number of codons in the sequenced genome, counting the individual codon occurance, and then dividing this number by the total number of codons.\nfor example : for Enterobacteria phage P1, the total number of codons is 71879, and the number of UGU occurances is 362, so the codon frequency is 0.00503.\n\nCodon columns distributions:\n\n\nplt.figure(figsize=(30, 10))\nsns.boxplot(data=justcodon, palette='Blues')\nplt.title('Codon Distributions')\nplt.axhline(y=justcodon.median().mean(), color='r')\nplt.xlabel('Codons')\nplt.show()\n\n\n\n\nA few observations - The stop codons UAA, UAG UGA have the lowest occurance, this makes sense since stop codons are what tells the translation machinery when to stop translation, so they will not be as frequent in the RNA/DNA sequence as the rest of the codons.\n\nThe start codon AUG occurance is quite similar accross all species as the distribution is tending towards the median.\n\nUUU UUC:\n\nThese codons encode for the amino acid Phenylalanine. As this amino acid only has two codons that can encode for it, the distributions have a similar median, but UUU has slightly more recorded occurances among the different species.\n\nAUU AUC AUA:\n\nThese three codons encode for the amino acid Isoleucine. From the distributions AUU and AUC has higher occurances in the species. So they tend to be the ones found most among species to encode for the amino acid, with AUU having a higher median so it is even more favoured over AUC.\n\nACU ACC ACA ACG\n\nThese encode for the amino acid Theronine. The distributions show that ACC and ACA have a wider range of occurances among the species while ACU codon appears to be the higher occuring one for this amino acid.\n\nCAA CAG\n\nThese codons encode for the amino acid Glutamine. From the distributions it is apparent that CAA is more frequent in its occurance among species.\n\nAAU AAC\n\nThese codons encode for the amino acid Asparagine. Seeing that this amino acid only has two type of codons that can encode it, the distributions show a similar median for both however AAC tends to be favoured.\n\nThese frequencies are highly dependent on the size of the sequence, DNA length that was sequenced. Therefore, it is important to view their distributions in relation to either DNA type or Kingdom."
  },
  {
    "objectID": "posts/2022-08-03-Exploratory-Data-Analysis.html#bivariate-analysis",
    "href": "posts/2022-08-03-Exploratory-Data-Analysis.html#bivariate-analysis",
    "title": "codon2",
    "section": "Bivariate Analysis",
    "text": "Bivariate Analysis\n\nKingdom classes:\n\nAre there codons that have a higher occurence in certain Kingdoms?\n\n\nclustergraph_kingdom = codon.drop(\n    columns=['DNAtype', 'SpeciesID', 'Ncodons', 'SpeciesName'], axis=1)\nclustergraph_kingdom.set_index('Kingdom', inplace=True)\n\n\nfig = px.imshow(clustergraph_kingdom.groupby(level='Kingdom').mean(\n),  color_continuous_scale='Cividis', origin='lower')  # or RdBu_r\nfig.update_layout()\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nFrom the heatmap it appears that CUA has a high occurance in the vertebrate kingdom. furthermore the Aspartic Acid (GAU and GAC) and Glutamic Acid (GAA and GAG) codons seem to have high occurances in all of the kingdoms but not as much in the vertebrate kingdom in comparison with the rest.\nCAG has a higher occurance for archea kingdom.\nCUA is highly favoured in vertebrates, this codon encodes for Leucine.\n\n\nDNAtype classes:\n\nAre there codons that have a higher occurance in certain DNA types?\n\n\nclustergraph_dna = codon.drop(\n    columns=['Kingdom', 'SpeciesID', 'Ncodons', 'SpeciesName'], axis=1)\nclustergraph_dna.set_index('DNAtype', inplace=True)\n\n\nfig = px.imshow(clustergraph_dna.groupby(level='DNAtype').mean(),\n                color_continuous_scale='Cividis', origin='lower')  # or RdBu_r\nfig.update_layout()\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nIn the heatmap for DNAtype codon frequencies, there are some codons that have frequencies. It appears that for DNAtype 1 which is mitochondria, there is a higher frequency of CUA which codes for the amino acid Leucine. While for Nuclear DNA there is almost an avoidance of the CUA codon. Moreover, in terms of the stop codons, it appears that mitochondrial DNA (1) seems to favour UGA stop codon over the over two.\nAnother noteworthy pattern, mitochondrial DNA shows obvious order of favouring for the Theronine amino acid encoding codons; ACG, ACA, ACC, and ACU. There is a higher occurance of ACA for this amino acid.\nNuclear DNA (0) seems to have a different pattern, in terms of frequency there are obvious codons that are more occurent for amino acids, however it isn’t as pronounced."
  },
  {
    "objectID": "posts/05-Kingdom-Classification.html",
    "href": "posts/05-Kingdom-Classification.html",
    "title": "Jasmine Marzouk",
    "section": "",
    "text": "# imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom IPython.display import Image\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import plot_importance\nfrom pandas import MultiIndex, Int16Dtype\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import (GridSearchCV, cross_val_score,\n                                     train_test_split)\nfrom sklearn.metrics import (accuracy_score, auc, classification_report,\n                             confusion_matrix, f1_score, plot_confusion_matrix,\n                             plot_roc_curve, precision_score, recall_score,\n                             roc_auc_score, roc_curve)\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN\nfrom numpy import hstack, unique, vstack, where\nfrom matplotlib import pyplot\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom scipy import stats\n\n%matplotlib inline\n\n\n# warnings.filterwarnings(\"ignore\")\nnp.random.seed(123)\n\n\n# warnings.filterwarnings(\"ignore\")\nnp.random.seed(123)"
  },
  {
    "objectID": "posts/05-Kingdom-Classification.html#logistic-regression",
    "href": "posts/05-Kingdom-Classification.html#logistic-regression",
    "title": "Jasmine Marzouk",
    "section": "Logistic Regression:",
    "text": "Logistic Regression:\n\nsns.set_theme(style=\"darkgrid\")\n# Using SMOTE to balance the classes:\n\nsm = SMOTE(random_state=123)\nX_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n\n# Plotting distributions\nprint('Distributions before and after SMOTE upsampling:')\nplt.subplots(1, 2, figsize=(10, 3))\n\n# Plot the original data\nplt.subplot(1, 2, 1)\nplt.bar(y_train.value_counts().index, y_train.value_counts())\nplt.title('Original target classes')\nplt.xticks(ticks=[0, 1, 2, 3, 4, 5, 6])\n\n# Plot the upsampled data\nplt.subplot(1, 2, 2)\nplt.bar(y_train_sm.value_counts().index, y_train_sm.value_counts())\nplt.title('Upsampled target classes')\nplt.xticks(ticks=[0, 1, 2, 3, 4, 5, 6])\nplt.show()\n\nprint('==============================================================================')\n\n# scaling data for PCA tranform\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_sm)  # scaling the upsampled set\nX_test_scaled = scaler.transform(X_test)\n\n# PCA dimensionality reduction\npca = PCA()\n\n# fit transform train set and transform test set\nX_train_PCA = pca.fit_transform(X_train_scaled)\nX_test_PCA = pca.transform(X_test_scaled)\n# to see the number of features that account for the highest variance\nexplained_variance = pca.explained_variance_ratio_\n\n# cumulative sum of the explained variance\ncumulative_sum = np.cumsum(explained_variance)\n\n# plot\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(X_train.columns)+1), cumulative_sum, marker='.')\nplt.axhline(0.9, c='r', linestyle='--')\nplt.xlabel('Number of PCs')\nplt.ylabel('Cumulative Sum of Explained Variance')\nplt.xticks(range(1, len(X_train.columns)+1, 2))\nplt.title(\"How many features explain 90% of total variance\")\nplt.show()\n\nDistributions before and after SMOTE upsampling:\n\n\n\n\n\n==============================================================================\n\n\n\n\n\n\n# setting n_components and refitting a PCA\n\npca_0 = PCA(n_components=27)\n\n# fit transform train set and transform test set\nX_train_PCA_0 = pca_0.fit_transform(X_train_scaled)\nX_test_PCA_0 = pca_0.transform(X_test_scaled)\n\n\n# regularisation:\n\nc_params = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n\ntrain_accuracies = []\ntest_accuracies = []\n\nfor c in c_params:\n    log_reg = LogisticRegression(C=c, max_iter=10000, random_state=123)\n\n    log_reg.fit(X_train_PCA_0, y_train_sm)\n\n    train_accuracies.append(log_reg.score(X_train_PCA_0, y_train_sm))\n    test_accuracies.append(log_reg.score(X_test_PCA_0, y_test))\n\n\nplt.figure(figsize=(10, 10))\nplt.plot(c_params, train_accuracies, label='train')\nplt.plot(c_params, test_accuracies, label='test')\nplt.xscale('log')\nplt.legend()\nplt.title('Accuracies for log_reg model')\nplt.xlabel('C parameter')\nplt.ylabel('Accuracy')\nplt.show()\n\n\n\n\n\n# For C =0.01\n\nlog_reg_0 = LogisticRegression(C=0.01, max_iter=10000, random_state=123)\n\nlog_reg_0.fit(X_train_PCA_0, y_train_sm)\n\nprint(log_reg_0.score(X_train_PCA_0, y_train_sm))\nprint(log_reg_0.score(X_test_PCA_0, y_test))\n\n0.8506893517160458\n0.8218279984573853\n\n\n\nsns.set_theme(style=\"dark\")\ny_predicted = log_reg_0.predict(X_test_PCA_0)\n\n# Generate confusion matrix\ncf_matrix = confusion_matrix(y_test, y_predicted)\n\n# label rows and columns\ncf_df = pd.DataFrame(\n    cf_matrix,\n    columns=[\"Predicted vertebrate\", \"Predicted bacteria\", \"Predicted virus\",\n             \"Predicted plant\", \"Predicted invertebrate\", \"Predicted phage\", \"Predicted archaea\"],\n    index=[\"True vertebrate\", \"True bacteria\", \"True virus\", \"True plant\", \"True invertebrate\", \"True phage\", \"True archaea\"])\n\n\nplot_confusion_matrix(log_reg_0, X_test_PCA_0, y_test)\nplt.xticks(rotation=45)\n\nprint('==========================================================')\n\n# Precision, recall, and F1\nclass_report_log_0 = classification_report(y_test, y_predicted)\nprint(class_report_log_0)\n\nprint('==========================================================')\n\n# AUC score\ny_proba_train = log_reg_0.predict_proba(X_train_PCA_0)\nauc_train = np.round(roc_auc_score(\n    y_train_sm, y_proba_train, multi_class='ovo'), 3)\nprint('Logistic Regression model AUC score')\nprint(f'roc_auc_score: {auc_train}')\nprint('==========================================================')\n\ncf_df\n\n==========================================================\n              precision    recall  f1-score   support\n\n     archaea       0.41      0.92      0.57        25\n    bacteria       0.93      0.83      0.88       584\ninvertebrate       0.61      0.66      0.63       267\n       phage       0.30      0.82      0.44        44\n       plant       0.81      0.74      0.77       498\n  vertebrate       0.95      0.95      0.95       609\n       virus       0.85      0.83      0.84       566\n\n    accuracy                           0.82      2593\n   macro avg       0.70      0.82      0.73      2593\nweighted avg       0.85      0.82      0.83      2593\n\n==========================================================\nLogistic Regression model AUC score\nroc_auc_score: 0.976\n==========================================================\n\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n\n\n\n  \n    \n      \n      Predicted vertebrate\n      Predicted bacteria\n      Predicted virus\n      Predicted plant\n      Predicted invertebrate\n      Predicted phage\n      Predicted archaea\n    \n  \n  \n    \n      True vertebrate\n      23\n      2\n      0\n      0\n      0\n      0\n      0\n    \n    \n      True bacteria\n      24\n      483\n      5\n      58\n      7\n      2\n      5\n    \n    \n      True virus\n      2\n      7\n      176\n      1\n      40\n      9\n      32\n    \n    \n      True plant\n      3\n      5\n      0\n      36\n      0\n      0\n      0\n    \n    \n      True invertebrate\n      1\n      14\n      45\n      22\n      367\n      14\n      35\n    \n    \n      True phage\n      0\n      0\n      19\n      0\n      2\n      576\n      12\n    \n    \n      True archaea\n      3\n      6\n      43\n      3\n      36\n      5\n      470\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for Logistic Regression\n\nLogistic Regression model classifies using a logistic function, where the output is either 1 or 0, this is usually for binomial clasification. However, and as I assumed, Logistic regression model did not perform adequately in classifying multiclass target, with an R2 score of 0.85 for train data, and 0.82 for test data.\nI balanced the classes using SMOTE, and then performed a PCA fit on the train and transformed the test set. PCA reduces dimensionality to reduce overfitting, and use fewer features from the dataset. According to the graph above, the number of number of components needed to explain 90% of the variance is 29.\nFurthermore, hyperparameter tuning showed that a higher penalty of C=0.01 was the appropriate value to achieve a higher accuracy score.\nThe model is very good at classifying the vertebrate class which has a similar precision and recall score. The model performed well enough on the bacteria and virus classes. For the phage and archaea classes the model has: - high recall: the overall proportion of true positives over predicted results is high, the model classified the majority of these classes correctly - low precision: the proportion of true positives over actual results, the model struggled to classify the majority of the phage class and only correctly classifying 29% correctly.\nThe AUC score of 0.977 shows that the model performed well overall at distinguishing between the classes.\n\n# Feature importance\n# 5 to select coefficients for the vertebrate class.\ncoefs = pd.DataFrame(log_reg_0.coef_[5], columns=['coefficients'])\ncoefs\n\nfeat_names = pd.DataFrame(X_train.columns, columns=['feature_names'])\nfeat_names\n\nfeat_coefs = pd.concat([feat_names, coefs], axis=1, ignore_index=True)\n\nfeat_coefs = feat_coefs.sort_values(\n    by=1, ascending=False).set_index(0).head(29)\n\n# plotting coefficients\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(15, 8))\nplt.bar(feat_coefs.index, feat_coefs[1])\nplt.xticks(rotation=45)\nplt.xlabel('Features')\nplt.ylabel('Coefficient')\nplt.show()\n\n\n\n\nLogistic Regression adds coefficients (beta1) to each feature, the above bar chart shows the coefficients for the 29 features for the vertebrate class. the coefficients represent the log odds of a feature change effecting the outcome.\nfor the log_reg_0 model, the above barchart shows that with increase frequency of UUU and CUC for example, there is an increased odds of vertebrate classification. The bar grapgh shows how the model made the decision of classification, in terms of these features."
  },
  {
    "objectID": "posts/05-Kingdom-Classification.html#knearest-neighbors-model",
    "href": "posts/05-Kingdom-Classification.html#knearest-neighbors-model",
    "title": "Jasmine Marzouk",
    "section": "kNearest Neighbors model:",
    "text": "kNearest Neighbors model:\n\n# using the already scaled and PCA tranformed sets from previous model\n\n# Determining the ideal n_neighbors value:\n\nn_neighbors = range(1, 100, 2)\n\ntrain_accuracies_0 = []\ntest_accuracies_0 = []\n\nfor n in n_neighbors:\n    KNN_model = KNeighborsClassifier(n_neighbors=n)\n    KNN_model.fit(X_train_PCA_0, y_train_sm)\n\n    train_accuracies_0.append(KNN_model.score(X_train_PCA_0, y_train_sm))\n    test_accuracies_0.append(KNN_model.score(X_test_PCA_0, y_test))\n\n\nplt.figure(figsize=(10, 10))\nplt.plot(n_neighbors, train_accuracies_0, label='train')\nplt.plot(n_neighbors, test_accuracies_0, label='test')\nplt.legend()\nplt.title('Accuracies for KNN_model')\nplt.xlabel('Number of neighbours')\nplt.ylabel('Accuracy')\nplt.show()\n\n\n\n\n\nKnn_accuracies_df = pd.DataFrame({'number of neighbors': n_neighbors,\n                                 'Train scores': train_accuracies_0, 'Test scores': test_accuracies_0})\nKnn_accuracies_df.sort_values(by='Test scores', ascending=False).head(5)\n\n\n\n\n\n  \n    \n      \n      number of neighbors\n      Train scores\n      Test scores\n    \n  \n  \n    \n      0\n      1\n      1.000000\n      0.939838\n    \n    \n      1\n      3\n      0.978117\n      0.934825\n    \n    \n      2\n      5\n      0.967263\n      0.928654\n    \n    \n      3\n      7\n      0.960751\n      0.924798\n    \n    \n      4\n      9\n      0.953828\n      0.922869\n    \n  \n\n\n\n\n\nKNN_model_0 = KNeighborsClassifier(n_neighbors=1)\nKNN_model_0.fit(X_train_PCA_0, y_train_sm)\n\nprint(f'KNN model train score: {KNN_model_0.score(X_train_PCA_0, y_train_sm)}')\nprint(\n    f'KNN model test score: {KNN_model_0.score(X_test_PCA_0, y_test).round(3)}')\n\nKNN model train score: 1.0\nKNN model test score: 0.94\n\n\n\ny_predicted_0 = KNN_model_0.predict(X_test_PCA_0)\n\n# Generate confusion matrix\ncf_matrix_0 = confusion_matrix(y_test, y_predicted_0)\n\n# label rows and columns\ncf_df_0 = pd.DataFrame(\n    cf_matrix_0,\n    columns=[\"Predicted vrt\", \"Predicted bct\", \"Predicted vrl\",\n             \"Predicted pln\", \"Predicted inv\", \"Predicted phg\", \"Predicted arc\"],\n    index=[\"True vrt\", \"True bct\", \"True vrl\", \"True pln\", \"True inv\", \"True phg\", \"True arc\"])\n\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(KNN_model_0, X_test_PCA_0, y_test)\nplt.xticks(rotation=45)\n\nprint('==========================================================')\n\nclass_report_knn_0 = classification_report(y_test, y_predicted_0)\nprint(class_report_knn_0)\n\nprint('==========================================================')\n\n# AUC score\ny_proba_train_0 = KNN_model_0.predict_proba(X_train_PCA_0)\nauc_train_0 = np.round(roc_auc_score(\n    y_train_sm, y_proba_train_0, multi_class='ovo'), 3)\nprint('KNN model AUC score')\nprint(f'roc_auc_score: {auc_train_0}')\nprint('==========================================================')\n\ncf_df_0\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n==========================================================\n              precision    recall  f1-score   support\n\n     archaea       0.70      0.84      0.76        25\n    bacteria       0.97      0.95      0.96       584\ninvertebrate       0.84      0.87      0.86       267\n       phage       0.61      0.82      0.70        44\n       plant       0.94      0.92      0.93       498\n  vertebrate       0.98      0.98      0.98       609\n       virus       0.97      0.95      0.96       566\n\n    accuracy                           0.94      2593\n   macro avg       0.86      0.90      0.88      2593\nweighted avg       0.94      0.94      0.94      2593\n\n==========================================================\nKNN model AUC score\nroc_auc_score: 1.0\n==========================================================\n\n\n\n\n\n\n  \n    \n      \n      Predicted vrt\n      Predicted bct\n      Predicted vrl\n      Predicted pln\n      Predicted inv\n      Predicted phg\n      Predicted arc\n    \n  \n  \n    \n      True vrt\n      21\n      3\n      1\n      0\n      0\n      0\n      0\n    \n    \n      True bct\n      5\n      554\n      3\n      17\n      4\n      0\n      1\n    \n    \n      True vrl\n      1\n      3\n      233\n      1\n      11\n      7\n      11\n    \n    \n      True pln\n      3\n      4\n      0\n      36\n      1\n      0\n      0\n    \n    \n      True inv\n      0\n      8\n      17\n      5\n      460\n      4\n      4\n    \n    \n      True phg\n      0\n      1\n      11\n      0\n      2\n      595\n      0\n    \n    \n      True arc\n      0\n      1\n      11\n      0\n      13\n      3\n      538\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for kNearest Neighbors:\n\nkNearest Neighbor model classifies by determining the distances between the data points: data points closest to each other are classified as being from the same class, depending on the number of nearest neighbors selected. Because KNN usese distances between data points, I scaled the data performed PCA and used the SMOTE transformed data.\nThe KNN_model_0 performed very well, with an accuracy score of 1.0 for the training data and 0.94 for the test data, for a value of n_neighbors = 1.\nThe model performed better on all of the classes compared with log_reg_0 (logistic Regression) model, with improvement in the precision and recall scores overall. However, the KNN model still underperformed on the phage and archaea classes with F1 scores of 0.69 and 0.76 respectively.\nOverall the KNN_model_0 performed well on distinguising between the classes with an AUC score of 1.0."
  },
  {
    "objectID": "posts/05-Kingdom-Classification.html#xgboost-model",
    "href": "posts/05-Kingdom-Classification.html#xgboost-model",
    "title": "Jasmine Marzouk",
    "section": "XGBoost model:",
    "text": "XGBoost model:\nI will need to label encode the Kingdom column so the classes are integers instead of strings, the XGBoost model only takes numerical values as its input.\nI will use the LabelEncoder() to fit to the training data and transform the test data.\n\n# Instantiate the label encoder\nlabel_enc = LabelEncoder()\n\n# Fit and transform the y_train, and transform the y_test\ny_train_enc = label_enc.fit_transform(y_train)\ny_test_enc = label_enc.transform(y_test)\n\n\nmaxdepth = range(1, 10)\n\ntrain_accuracies_1 = []\ntest_accuracies_1 = []\n\nfor d in maxdepth:\n    XGB_model_ = XGBClassifier(max_depth=d, use_label_encoder=False)\n    XGB_model_.fit(X_train, y_train_enc)\n\n    train_accuracies_1.append(XGB_model_.score(X_train, y_train_enc))\n    test_accuracies_1.append(XGB_model_.score(X_test, y_test_enc))\n\n\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(10, 10))\nplt.plot(maxdepth, train_accuracies_1, label='train')\nplt.plot(maxdepth, test_accuracies_1, label='test')\nplt.xlabel('Max Depth')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracies for XGBoost model')\n\n\nXGB_accuracies_df = pd.DataFrame(\n    {'max Depth': maxdepth, 'Train scores': train_accuracies_1, 'Test scores': test_accuracies_1})\nXGB_accuracies_df.sort_values(by='Test scores', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      max Depth\n      Train scores\n      Test scores\n    \n  \n  \n    \n      5\n      6\n      1.000000\n      0.949865\n    \n    \n      3\n      4\n      1.000000\n      0.949479\n    \n    \n      8\n      9\n      1.000000\n      0.949479\n    \n    \n      4\n      5\n      1.000000\n      0.947937\n    \n    \n      6\n      7\n      1.000000\n      0.947937\n    \n    \n      7\n      8\n      1.000000\n      0.947165\n    \n    \n      2\n      3\n      0.995757\n      0.941381\n    \n    \n      1\n      2\n      0.962492\n      0.917084\n    \n    \n      0\n      1\n      0.866358\n      0.848438\n    \n  \n\n\n\n\n\n\n\n\n# XGBoost with max+depth = 4\n\nXGB_model_0 = XGBClassifier(max_depth=4, use_label_encoder=False)\nXGB_model_0.fit(X_train, y_train_enc)\n\nprint(\n    f'XGBoost train accuracy: {XGB_model_0.score(X_train, y_train_enc).round(3)}')\nprint(\n    f'XGBoost test accuracy: {XGB_model_0.score(X_test, y_test_enc).round(3)}')\n\nXGBoost train accuracy: 1.0\nXGBoost test accuracy: 0.949\n\n\n\ny_predicted_1 = XGB_model_0.predict(X_test)\n\n# Generate confusion matrix\ncf_matrix_1 = confusion_matrix(y_test_enc, y_predicted_1)\n\n# label rows and columns\ncf_df_1 = pd.DataFrame(\n    cf_matrix_1,\n    columns=[\"Predicted vrt\", \"Predicted bct\", \"Predicted vrl\",\n             \"Predicted pln\", \"Predicted inv\", \"Predicted phg\", \"Predicted arc\"],\n    index=[\"True vrt\", \"True bct\", \"True vrl\", \"True pln\", \"True inv\", \"True phg\", \"True arc\"])\n\nprint('==========================================================')\n\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(XGB_model_0, X_test, y_test_enc)\n\nprint('==========================================================')\n\n\n# Precision Recall and F1 scores\nclass_report_xgb_0 = classification_report(y_test_enc, y_predicted_1)\nprint(class_report_xgb_0)\nprint('==========================================================')\n\n# AUC score\ny_proba_train_1 = XGB_model_0.predict_proba(X_train)\nauc_train_1 = np.round(roc_auc_score(\n    y_train, y_proba_train_1, multi_class='ovo'), 3)\nprint('XGBoost model AUC score')\nprint(f'roc_auc_score: {auc_train_1}')\nprint('==========================================================')\n\ncf_df_1\n\n==========================================================\n==========================================================\n              precision    recall  f1-score   support\n\n           0       0.81      0.68      0.74        25\n           1       0.94      0.98      0.96       584\n           2       0.93      0.84      0.88       267\n           3       0.90      0.64      0.75        44\n           4       0.93      0.94      0.94       498\n           5       0.99      0.99      0.99       609\n           6       0.95      0.97      0.96       566\n\n    accuracy                           0.95      2593\n   macro avg       0.92      0.86      0.89      2593\nweighted avg       0.95      0.95      0.95      2593\n\n==========================================================\n\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\nXGBoost model AUC score\nroc_auc_score: 1.0\n==========================================================\n\n\n\n\n\n\n  \n    \n      \n      Predicted vrt\n      Predicted bct\n      Predicted vrl\n      Predicted pln\n      Predicted inv\n      Predicted phg\n      Predicted arc\n    \n  \n  \n    \n      True vrt\n      17\n      6\n      1\n      0\n      0\n      0\n      1\n    \n    \n      True bct\n      1\n      574\n      1\n      2\n      2\n      0\n      4\n    \n    \n      True vrl\n      0\n      4\n      224\n      0\n      20\n      4\n      15\n    \n    \n      True pln\n      2\n      12\n      0\n      28\n      1\n      0\n      1\n    \n    \n      True inv\n      0\n      13\n      5\n      1\n      469\n      2\n      8\n    \n    \n      True phg\n      0\n      0\n      6\n      0\n      0\n      602\n      1\n    \n    \n      True arc\n      1\n      0\n      5\n      0\n      10\n      2\n      548\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for XGBoost model:\n\nXGBoost model is an ensemble model, it classifies data using decision trees in ensembles. Using max_depth as the hyperparameter, which determines the maximium depth of the trees.\nFor XGB_model_0 the appropriate value for max_depth was determined to be 3, this gave an accuracy score of 0.996 for training data and 0.941 for test data. There is slight overfitting here, this is a disadvantage of decision tree type classification.\nI did not use the scaled, oversamples, and PCA transformed data for this model, as this is not needed.\nXGB_model_0 performed better than KNN_model_0 and log_reg_0 in terms of precision for phage and archaea classes with scores of 0.83 and 0.89 respectively. Although, there is a decrease in recall score for both of these classes, meaning that this model underperformed in classifying the\nHowever, in terms of accuracy it is as good as KNN_model_0 in terms of accuracy, but KNN_model_0 had less overfitting on the training data.\nFurthermore, this model performed well on distinguishing between the classes with an AUC score of 1.0.\n\n# plot feature importance\nplt.figure(figsize=(15, 25))\nplot_importance(XGB_model_0, max_num_features=20,\n                importance_type='gain', show_values=False)\nplt.show()\n\n<Figure size 1080x1800 with 0 Axes>\n\n\n\n\n\n\nfeat_imprt = pd.DataFrame(\n    XGB_model_0.feature_importances_).sort_values(by=0, ascending=False)\n\nfeat_names_0 = pd.DataFrame(X_train.columns, columns=['feature_names'])\n\n# join inner because the ndarray lengths are different\nfeat_importance_0 = pd.concat(\n    [feat_names_0, feat_imprt], axis=1, ignore_index=True, join='inner')\n\nfeat_importance_0 = feat_importance_0.sort_values(\n    by=1, ascending=False).set_index(0).head(20)\n\n# plotting coefficients\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(15, 8))\nplt.bar(feat_importance_0.index, feat_imprt[0][:20])\nplt.xticks(rotation=45)\nplt.xlabel('Features')\nplt.ylabel('Importance')\nplt.show()\n\n\n\n\nFeature importance for XGBoost models shows the features that the model deemed most important in making the classification decision.\nthe above bar chart shows the codons importance according to gain metric, which is the relative contribution of each feature for each tree in the ensemble (XGBoost) model. This means that CUA and GCG having the highest values of importance were the most important in generating a prediction of the outcome classification."
  },
  {
    "objectID": "posts/05-Kingdom-Classification.html#ensemble-model",
    "href": "posts/05-Kingdom-Classification.html#ensemble-model",
    "title": "Jasmine Marzouk",
    "section": "Ensemble Model",
    "text": "Ensemble Model\n\n# We instantiate the base models, along with their names\nbase_models = [('KNN', KNN_model_0),\n               ('XGBoost', XGB_model_0)\n               ]\n\n\n# building the stacked model\nstacked_model = StackingClassifier(\n    estimators=base_models,\n    final_estimator=LogisticRegression())\n\nstacked_model.fit(X_train, y_train)\n\nStackingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=1)),\n                               ('XGBoost',\n                                XGBClassifier(base_score=0.5, booster='gbtree',\n                                              callbacks=None,\n                                              colsample_bylevel=1,\n                                              colsample_bynode=1,\n                                              colsample_bytree=1,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None, gamma=0,\n                                              gpu_id=-1,\n                                              grow_policy='depthwise',\n                                              importance_type=None,\n                                              interaction_constraints='',\n                                              learning_rate=0.300000012,\n                                              max_bin=256, max_cat_to_onehot=4,\n                                              max_delta_step=0, max_depth=4,\n                                              max_leaves=0, min_child_weight=1,\n                                              missing=nan,\n                                              monotone_constraints='()',\n                                              n_estimators=100, n_jobs=0,\n                                              num_parallel_tree=1,\n                                              objective='multi:softprob',\n                                              predictor='auto', random_state=0,\n                                              reg_alpha=0, ...))],\n                   final_estimator=LogisticRegression())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StackingClassifierStackingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=1)),\n                               ('XGBoost',\n                                XGBClassifier(base_score=0.5, booster='gbtree',\n                                              callbacks=None,\n                                              colsample_bylevel=1,\n                                              colsample_bynode=1,\n                                              colsample_bytree=1,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None, gamma=0,\n                                              gpu_id=-1,\n                                              grow_policy='depthwise',\n                                              importance_type=None,\n                                              interaction_constraints='',\n                                              learning_rate=0.300000012,\n                                              max_bin=256, max_cat_to_onehot=4,\n                                              max_delta_step=0, max_depth=4,\n                                              max_leaves=0, min_child_weight=1,\n                                              missing=nan,\n                                              monotone_constraints='()',\n                                              n_estimators=100, n_jobs=0,\n                                              num_parallel_tree=1,\n                                              objective='multi:softprob',\n                                              predictor='auto', random_state=0,\n                                              reg_alpha=0, ...))],\n                   final_estimator=LogisticRegression())KNNKNeighborsClassifierKNeighborsClassifier(n_neighbors=1)XGBoostXGBClassifierXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n              predictor='auto', random_state=0, reg_alpha=0, ...)final_estimatorLogisticRegressionLogisticRegression()\n\n\n\nprint(\n    f'Stacked model train accuracy: {stacked_model.score(X_train, y_train).round(3)}')\nprint(\n    f'Stacked model test accuracy: {stacked_model.score(X_test, y_test).round(3)}')\n\nStacked model train accuracy: 1.0\nStacked model test accuracy: 0.949\n\n\n\ny_predicted_2 = stacked_model.predict(X_test)\n\n# Generate confusion matrix\ncf_matrix_2 = confusion_matrix(y_test, y_predicted_2)\n\n# label rows and columns\ncf_df_2 = pd.DataFrame(\n    cf_matrix_2,\n    columns=[\"Predicted vrt\", \"Predicted bct\", \"Predicted vrl\",\n             \"Predicted pln\", \"Predicted inv\", \"Predicted phg\", \"Predicted arc\"],\n    index=[\"True vrt\", \"True bct\", \"True vrl\", \"True pln\", \"True inv\", \"True phg\", \"True arc\"])\n\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(stacked_model, X_test, y_test)\nplt.xticks(rotation=45)\n\nprint('==========================================================')\n\n\n# Precision Recall and F1 scores\nclass_report_stack_0 = classification_report(y_test, y_predicted_2)\nprint(class_report_stack_0)\nprint('==========================================================')\n\n# AUC score\ny_proba_train_2 = stacked_model.predict_proba(X_train)\nauc_train_2 = np.round(roc_auc_score(\n    y_train, y_proba_train_2, multi_class='ovo'), 3)\nprint('Ensemble model AUC score')\nprint(f'roc_auc_score: {auc_train_2}')\nprint('==========================================================')\n\ncf_df_2\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n==========================================================\n              precision    recall  f1-score   support\n\n     archaea       0.81      0.68      0.74        25\n    bacteria       0.94      0.98      0.96       584\ninvertebrate       0.91      0.85      0.88       267\n       phage       0.90      0.64      0.75        44\n       plant       0.94      0.94      0.94       498\n  vertebrate       0.99      0.99      0.99       609\n       virus       0.95      0.97      0.96       566\n\n    accuracy                           0.95      2593\n   macro avg       0.92      0.86      0.89      2593\nweighted avg       0.95      0.95      0.95      2593\n\n==========================================================\nEnsemble model AUC score\nroc_auc_score: 1.0\n==========================================================\n\n\n\n\n\n\n  \n    \n      \n      Predicted vrt\n      Predicted bct\n      Predicted vrl\n      Predicted pln\n      Predicted inv\n      Predicted phg\n      Predicted arc\n    \n  \n  \n    \n      True vrt\n      17\n      6\n      1\n      0\n      0\n      0\n      1\n    \n    \n      True bct\n      1\n      574\n      1\n      2\n      3\n      0\n      3\n    \n    \n      True vrl\n      0\n      4\n      228\n      0\n      17\n      4\n      14\n    \n    \n      True pln\n      2\n      12\n      0\n      28\n      1\n      0\n      1\n    \n    \n      True inv\n      0\n      14\n      8\n      1\n      466\n      1\n      8\n    \n    \n      True phg\n      0\n      0\n      6\n      0\n      1\n      600\n      2\n    \n    \n      True arc\n      1\n      0\n      6\n      0\n      8\n      2\n      549\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for the sctacked (ensemble) model:\n\nFor the ensemble or stacked model, I used the models I already trained and tuned, with the exception of the logistic regression model log_reg_0; I decided to exclude this model as it was the model that underperformed on classifying the target variable.\nI only included XGB_model_0 and KNN_model_0 as base models, with a final estimator being a default Logistic Regression model. Each of the base estimators is different in the way it approaches the classification.\nThe ensemble model classifies by taking the outputs from both the XGBoost model and KNN model as probabilites of each class as an input, the final estimator being a logistic regression then uses logistic function to classify each class.\nstacked_model achieved an accuracy score of 0.998 for the train data and 0.946 for the test data, suggesting slight overfitting, which makes sense as one of the base models is the XGBoost.\nThe stacked model did well in distinguishing between the classes with an AUC of 1.0."
  },
  {
    "objectID": "posts/05-Kingdom-Classification.html#comparing-models",
    "href": "posts/05-Kingdom-Classification.html#comparing-models",
    "title": "Jasmine Marzouk",
    "section": "Comparing Models:",
    "text": "Comparing Models:\n\ncls = classification_report(y_test, y_predicted, output_dict=True)\ncls_0 = classification_report(y_test, y_predicted_0, output_dict=True)\ncls_1 = classification_report(y_test_enc, y_predicted_1, output_dict=True)\ncls_2 = classification_report(y_test, y_predicted_2, output_dict=True)\n\n\ncomparison_df = pd.DataFrame({'Precision':\n                             [cls['weighted avg']['precision'],\n                              cls_0['weighted avg']['precision'],\n                              cls_1['weighted avg']['precision'],\n                              cls_2['weighted avg']['precision']],\n                              'Recall':\n                             [cls['weighted avg']['recall'],\n                              cls_0['weighted avg']['recall'],\n                              cls_1['weighted avg']['recall'],\n                              cls_2['weighted avg']['recall']],\n                             'F1 score':\n                              [cls['weighted avg']['f1-score'],\n                              cls_0['weighted avg']['f1-score'],\n                               cls_1['weighted avg']['f1-score'],\n                              cls_2['weighted avg']['f1-score']],\n                              'Accuracy':\n                              [cls['accuracy'],\n                               cls_0['accuracy'],\n                               cls_1['accuracy'],\n                               cls_2['accuracy']],\n                              'AUC score':\n                              [auc_train,\n                               auc_train_0,\n                               auc_train_1,\n                               auc_train_2]\n                              },\n                             index=['Logistic Regression', 'kNearest Neighbors', 'XGBoost', 'Stacked model'])\n\ncomparison_df.round(3).sort_values(by='F1 score', ascending=False)\n\n\n\n\n\n  \n    \n      \n      Precision\n      Recall\n      F1 score\n      Accuracy\n      AUC score\n    \n  \n  \n    \n      Stacked model\n      0.949\n      0.949\n      0.949\n      0.949\n      1.000\n    \n    \n      XGBoost\n      0.949\n      0.949\n      0.948\n      0.949\n      1.000\n    \n    \n      kNearest Neighbors\n      0.943\n      0.940\n      0.941\n      0.940\n      1.000\n    \n    \n      Logistic Regression\n      0.847\n      0.822\n      0.830\n      0.822\n      0.976\n    \n  \n\n\n\n\nAccording to the F1 score the model that performed the best overall is the stacked model, but by not much as the KNN model did just as well.\nLooking at the vanilla models performed at the beginning the accuracy score for the kNearest Neighbor performed was 93%, so I only achived an improvement of ~1.6% for the Kingdom classses classification.\nThis could be because the dataset is not complicated enough for the models to be able to learn by combining the models, that is why the stacked model isn’t far off from the results of the KNN and XGBoost models.\n\nplt.figure(figsize=(20, 25))\ncomparison_df.plot(kind='bar')\nplt.xticks(rotation=90)\nplt.legend()\nplt.show()\n\n<Figure size 1440x1800 with 0 Axes>"
  },
  {
    "objectID": "posts/05-Kingdom-Classification.html#saving-models",
    "href": "posts/05-Kingdom-Classification.html#saving-models",
    "title": "Jasmine Marzouk",
    "section": "Saving Models:",
    "text": "Saving Models:\n\n# Logistic Regression model\nlog_reg_kingdom = 'finalized_log_reg_kingdom.sav'\njoblib.dump(log_reg_0, log_reg_kingdom)\n\n\n# kNearest Neighbor model\nknn_kingdom = 'finalized_knn_kingdom.sav'\njoblib.dump(KNN_model_0, knn_kingdom)\n\n# XGBoost model\nxgboost_kingdom = 'finalized_xgboost_kingdom.sav'\njoblib.dump(XGB_model_0, xgboost_kingdom)\n\n# Ensemble model\nstacked_kingdom = 'finalized_stacked_kingdom.sav'\njoblib.dump(stacked_model, stacked_kingdom)\n\n['finalized_stacked_kingdom.sav']"
  },
  {
    "objectID": "posts/04-DnaType-Classification.html",
    "href": "posts/04-DnaType-Classification.html",
    "title": "Jasmine Marzouk",
    "section": "",
    "text": "With modeling I will start with a Logistic Regression\n\n# imports\nfrom IPython.display import Image\nfrom xgboost import plot_importance\nfrom sklearn.metrics import roc_auc_score\nfrom pandas import MultiIndex, Int16Dtype\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import (GridSearchCV, cross_val_score,\n                                     train_test_split)\nfrom sklearn.metrics import (accuracy_score, auc, classification_report,\n                             confusion_matrix, f1_score, plot_confusion_matrix,\n                             plot_roc_curve, precision_score, recall_score,\n                             roc_auc_score, roc_curve)\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import DBSCAN\nfrom numpy import hstack, unique, vstack, where\nfrom matplotlib import pyplot\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom scipy import stats\n\n%matplotlib inline\n\n\n# warnings.filterwarnings(\"ignore\")\nnp.random.seed(123)"
  },
  {
    "objectID": "posts/04-DnaType-Classification.html#logistic-regression",
    "href": "posts/04-DnaType-Classification.html#logistic-regression",
    "title": "Jasmine Marzouk",
    "section": "Logistic Regression:",
    "text": "Logistic Regression:\n\n# Using SMOTE to balance the classes:\n\nsm = SMOTE(random_state=123)\nX_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n\n# Plotting distributions\nprint('Distributions before and after SMOTE upsampling:')\nplt.subplots(1, 2, figsize=(10, 3))\n\n# Plot the original data\nplt.subplot(1, 2, 1)\nplt.bar(y_train.value_counts().index, y_train.value_counts())\nplt.title('Original target classes')\nplt.xticks(ticks=[0, 1, 2])\n\n# Plot the upsampled data\nplt.subplot(1, 2, 2)\nplt.bar(y_train_sm.value_counts().index, y_train_sm.value_counts())\nplt.title('Upsampled target classes')\nplt.xticks(ticks=[0, 1, 2])\n\nplt.show()\nprint('==============================================================================')\n\n# scaling data for PCA tranform\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_sm)  # scaling the upsampled set\nX_test_scaled = scaler.transform(X_test)\n\n# PCA dimensionality reduction\npca = PCA()\n\n# fit transform train set and transform test set\nX_train_PCA = pca.fit_transform(X_train_scaled)\nX_test_PCA = pca.transform(X_test_scaled)\n# to see the number of features that account for the highest variance\nexplained_variance = pca.explained_variance_ratio_\n\n# cumulative sum of the explained variance\ncumulative_sum = np.cumsum(explained_variance)\n\n# plot\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(X_train.columns)+1), cumulative_sum, marker='.')\nplt.axhline(0.9, c='r', linestyle='--')\nplt.xlabel('Number of PCs')\nplt.ylabel('Cumulative Sum of Explained Variance')\nplt.xticks(range(1, len(X_train.columns)+1, 2))\nplt.title(\"How many features explain 90% of total variance\")\nplt.show()\n\nDistributions before and after SMOTE upsampling:\n\n\n\n\n\n==============================================================================\n\n\n\n\n\n\n# setting n_components and refitting a PCA\n\npca_0 = PCA(n_components=26)\n\n# fit transform train set and transform test set\nX_train_PCA_0 = pca_0.fit_transform(X_train_scaled)\nX_test_PCA_0 = pca_0.transform(X_test_scaled)\n\n\n# regularisation:\n\nc_params = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n\ntrain_accuracies = []\ntest_accuracies = []\n\nfor c in c_params:\n    logi_reg = LogisticRegression(C=c, max_iter=10000, random_state=123)\n\n    logi_reg.fit(X_train_PCA_0, y_train_sm)\n\n    train_accuracies.append(logi_reg.score(X_train_PCA_0, y_train_sm))\n    test_accuracies.append(logi_reg.score(X_test_PCA_0, y_test))\n\n\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(10, 10))\nplt.plot(c_params, train_accuracies, label='train')\nplt.plot(c_params, test_accuracies, label='test')\nplt.xscale('log')\nplt.legend()\nplt.title('Accuracies for logi_reg model')\n\nText(0.5, 1.0, 'Accuracies for logi_reg model')\n\n\n\n\n\n\n# For C =0.1\n\nlog_reg_0 = LogisticRegression(C=0.1, max_iter=10000, random_state=123)\n\nlog_reg_0.fit(X_train_PCA_0, y_train_sm)\n\nprint(\n    f'log_reg_0 model train score: {log_reg_0.score(X_train_PCA_0, y_train_sm).round(3)}')\nprint(\n    f'log_reg_0 model test score: {log_reg_0.score(X_test_PCA_0, y_test).round(3)}')\n\nlog_reg_0 model train score: 0.99\nlog_reg_0 model test score: 0.985\n\n\n\ny_predicted = log_reg_0.predict(X_test_PCA_0)\n\n# Generate confusion matrix\ncf_matrix = confusion_matrix(y_test, y_predicted)\n\n# label rows and columns\ncf_df = pd.DataFrame(\n    cf_matrix,\n    columns=[\"Predicted 0\", \"Predicted 1\", \"Predicted 2\"],\n    index=[\"True 0\", \"True 1\", \"True 2\"])\n\n\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(log_reg_0, X_test_PCA_0, y_test)\nprint('==========================================================')\n\n# Precision, recall, and F1 score:\nclass_report_log_0 = classification_report(y_test, y_predicted)\nprint(class_report_log_0)\nprint('==========================================================')\n\n# AUC score\ny_proba_train = log_reg_0.predict_proba(X_train_PCA_0)\nauc_train = np.round(roc_auc_score(\n    y_train_sm, y_proba_train, multi_class='ovo'), 3)\nprint('Logistic Regression model AUC score')\nprint(f'roc_auc_score: {auc_train}')\nprint('==========================================================')\n\ncf_df\n\n==========================================================\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      0.99      1850\n           1       0.98      0.98      0.98       580\n           2       0.88      0.96      0.92       163\n\n    accuracy                           0.98      2593\n   macro avg       0.95      0.98      0.96      2593\nweighted avg       0.99      0.98      0.98      2593\n\n==========================================================\nLogistic Regression model AUC score\nroc_auc_score: 1.0\n==========================================================\n\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n\n\n\n  \n    \n      \n      Predicted 0\n      Predicted 1\n      Predicted 2\n    \n  \n  \n    \n      True 0\n      1828\n      8\n      14\n    \n    \n      True 1\n      4\n      569\n      7\n    \n    \n      True 2\n      4\n      3\n      156\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for Logistic Regression\n\nLogistic Regression model classifies using a logistic function, where the output is either 1 or 0, this is usually for binomial classification. I balanced the classes using SMOTE, and then performed a PCA fit on the train and transformed the test set. PCA reduces dimensionality to reduce overfitting, and use fewer features from the dataset. According to the graph above, the number of number of components needed to explain 90% of the variance is 29.\nThe Logistic Regression model log_reg_0 has performed well on the dataset with an accuracy score of 0.99 on training data and 0.985 on the test data.\nFurthermore, hyperparameter tuning showed that a high penalty of C=0.1 was the appropriate value to achieve a higher accuracy score.\nThe model performed well on classifying the DNA types, with an F1 score of 0.99, 0.98, and 0.89 for types 0 (nuclear), 1 (mitochondrial), and 2(chloroplast) respectively. This shows that the model has equivently good precision and recall, meaning that the model classified the classes correctly the majority of time with low percentage of false positives overall.\nThe AUC score of 0.999 shows that the model performed well overall at distinguishing between the classes.\n\n# Visualising the PCA components in a scatter graph\nplt.figure(figsize=(10, 10))\nscatter = plt.scatter(\n    X_train_PCA_0[:, 0], X_train_PCA_0[:, 1], alpha=0.3, c=y_train_sm)\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nclasses = ['0: nuclear DNA', '1: mitochondrial DNA', '2: chloroplast DNA']\nplt.legend(handles=scatter.legend_elements()[0], labels=classes)\nplt.show()\n\n\n\n\nThe principal components analysis of the features shows good separation between the DNAtype classes, this could be why the logistic regression is performing well on this dataset,\n\n# Feature importance\ncoefs = pd.DataFrame(log_reg_0.coef_[0], columns=['coefficients'])\ncoefs\n\nfeat_names = pd.DataFrame(X_train.columns, columns=['feature_names'])\nfeat_names\n\nfeat_coefs = pd.concat([feat_names, coefs], axis=1, ignore_index=True)\n\nfeat_coefs = feat_coefs.sort_values(\n    by=1, ascending=False).set_index(0).head(26)\n\n# plotting coefficients\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(15, 8))\nplt.bar(feat_coefs.index, feat_coefs[1])\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nLogistic Regression adds coefficients (beta1) to each feature, the above bar chart shows the coefficients for the 26 features for the 0: nuclear. the coefficients represent the log odds of a feature change effecting the outcome.\nfor the log_reg_0 model, the above barchart shows that with increase frequency of UUU and UUG for example, there is an increased odds of 0: nuclear classification. The bar grapgh shows how the model made the decision of classification, in terms of these features.\nNcodons appears to be a feature with a positive and high coefficient for the nuclear DNA (0) class, this makes sense because Ncodons represents the size of the sequence, nuclear DNA is the larger of the DNAtypes so the number of codons would reflect this."
  },
  {
    "objectID": "posts/04-DnaType-Classification.html#knearest-neighbors-model",
    "href": "posts/04-DnaType-Classification.html#knearest-neighbors-model",
    "title": "Jasmine Marzouk",
    "section": "kNearest Neighbors model:",
    "text": "kNearest Neighbors model:\n\n# using the already scaled and PCA tranformed sets from previous model\n\n# Determining the ideal n_neighbors value:\n\nn_neighbors = range(1, 100, 2)\n\ntrain_accuracies_0 = []\ntest_accuracies_0 = []\n\nfor n in n_neighbors:\n    KNN_model = KNeighborsClassifier(n_neighbors=n)\n    KNN_model.fit(X_train_PCA_0, y_train_sm)\n\n    train_accuracies_0.append(KNN_model.score(X_train_PCA_0, y_train_sm))\n    test_accuracies_0.append(KNN_model.score(X_test_PCA_0, y_test))\n\n\nplt.figure(figsize=(10, 10))\nplt.plot(n_neighbors, train_accuracies_0, label='train')\nplt.plot(n_neighbors, test_accuracies_0, label='test')\nplt.legend()\nplt.title('Accuracies for KNN_model')\n\nText(0.5, 1.0, 'Accuracies for KNN_model')\n\n\n\n\n\n\nKnn_accuracies_df = pd.DataFrame({'number of neighbors': n_neighbors,\n                                 'Train scores': train_accuracies_0, 'Test scores': test_accuracies_0})\nKnn_accuracies_df.head(5)\n\n\n\n\n\n  \n    \n      \n      number of neighbors\n      Train scores\n      Test scores\n    \n  \n  \n    \n      0\n      1\n      1.000000\n      0.993444\n    \n    \n      1\n      3\n      0.999054\n      0.992673\n    \n    \n      2\n      5\n      0.997883\n      0.993444\n    \n    \n      3\n      7\n      0.996711\n      0.990359\n    \n    \n      4\n      9\n      0.996171\n      0.989202\n    \n  \n\n\n\n\n\nKNN_model_0 = KNeighborsClassifier(n_neighbors=3)\nKNN_model_0.fit(X_train_PCA_0, y_train_sm)\n\nprint(\n    f'KNN_model train score: {KNN_model_0.score(X_train_PCA_0, y_train_sm).round(3)}')\nprint(\n    f'KNN_model test score: {KNN_model_0.score(X_test_PCA_0, y_test).round(3)}')\n\nKNN_model train score: 0.999\nKNN_model test score: 0.993\n\n\n\ny_predicted_0 = KNN_model_0.predict(X_test_PCA_0)\n\n# Generate confusion matrix\ncf_matrix_0 = confusion_matrix(y_test, y_predicted_0)\n\n# label rows and columns\ncf_df_0 = pd.DataFrame(\n    cf_matrix_0,\n    columns=[\"Predicted 0\", \"Predicted 1\", \"Predicted 2\"],\n    index=[\"True 0\", \"True 1\", \"True 2\"])\n\n\nprint('==========================================================')\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(KNN_model_0, X_test_PCA_0, y_test)\n\n# Precision, recall, and F1 scores\nclass_report_knn_0 = classification_report(y_test, y_predicted_0)\nprint(class_report_knn_0)\nprint('==========================================================')\n\n# AUC score\ny_proba_train_0 = KNN_model_0.predict_proba(X_train_PCA_0)\nauc_train_0 = np.round(roc_auc_score(\n    y_train_sm, y_proba_train_0, multi_class='ovo'), 3)\nprint('KNN model AUC score')\nprint(f'roc_auc_score: {auc_train_0}')\nprint('==========================================================')\n\n\ncf_df_0\n\n==========================================================\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      1.00      1850\n           1       0.99      0.99      0.99       580\n           2       0.96      0.99      0.97       163\n\n    accuracy                           0.99      2593\n   macro avg       0.98      0.99      0.99      2593\nweighted avg       0.99      0.99      0.99      2593\n\n==========================================================\n\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\nKNN model AUC score\nroc_auc_score: 1.0\n==========================================================\n\n\n\n\n\n\n  \n    \n      \n      Predicted 0\n      Predicted 1\n      Predicted 2\n    \n  \n  \n    \n      True 0\n      1839\n      7\n      4\n    \n    \n      True 1\n      3\n      574\n      3\n    \n    \n      True 2\n      1\n      1\n      161\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for kNearest Neighbors:\n\nkNearest Neighbor model classifies by determining the distances between the data points: data points closest to each other are classified as being from the same class, depending on the number of nearest neighbors selected. Because KNN usese distances between data points, I scaled the data performed PCA and used the SMOTE transformed data.\nThe KNN_model_0 performed very well, with an accuracy score of 0.999 for the training data and 0.993 for the test data, for a value of n_neighbors = 3.\nThe model performed well on all of the classes, with improvement in the precision and recall scores: F1 scores of 1.0, 0.99, and 0.97 for 0 (nuclear), 1 (mitochondrial), and 2 (chloroplast) respectively.\nOverall the KNN_model_0 performed well on distinguising between the classes with an AUC score of 1.0."
  },
  {
    "objectID": "posts/04-DnaType-Classification.html#xgboost-model",
    "href": "posts/04-DnaType-Classification.html#xgboost-model",
    "title": "Jasmine Marzouk",
    "section": "XGBoost model:",
    "text": "XGBoost model:\n\nmaxdepth = range(1, 10)\n\ntrain_accuracies_1 = []\ntest_accuracies_1 = []\n\nfor d in maxdepth:\n    XGB_model = XGBClassifier(max_depth=d, use_label_encoder=False)\n    XGB_model.fit(X_train, y_train)\n\n    train_accuracies_1.append(XGB_model.score(X_train, y_train))\n    test_accuracies_1.append(XGB_model.score(X_test, y_test))\n\n\nplt.figure(figsize=(10, 10))\nplt.plot(maxdepth, train_accuracies_1, label='train')\nplt.plot(maxdepth, test_accuracies_1, label='test')\nplt.legend()\nplt.title('Accuracies for XGBoost model')\n\n\nXGB_accuracies_df = pd.DataFrame(\n    {'max Depth': maxdepth, 'Train scores': train_accuracies_1, 'Test scores': test_accuracies_1})\nXGB_accuracies_df.sort_values(by='Test scores', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      max Depth\n      Train scores\n      Test scores\n    \n  \n  \n    \n      2\n      3\n      1.000000\n      0.994601\n    \n    \n      4\n      5\n      1.000000\n      0.994601\n    \n    \n      5\n      6\n      1.000000\n      0.994601\n    \n    \n      6\n      7\n      1.000000\n      0.994601\n    \n    \n      8\n      9\n      1.000000\n      0.994601\n    \n    \n      3\n      4\n      1.000000\n      0.994215\n    \n    \n      1\n      2\n      0.999807\n      0.993444\n    \n    \n      7\n      8\n      1.000000\n      0.993444\n    \n    \n      0\n      1\n      0.995565\n      0.991516\n    \n  \n\n\n\n\n\n\n\n\n# XGBoost with max+depth = 3\n\nXGB_model_0 = XGBClassifier(max_depth=3, use_label_encoder=False)\nXGB_model_0.fit(X_train, y_train)\n\nprint(\n    f'XGBoost train accuracy: {XGB_model_0.score(X_train, y_train).round(3)}')\nprint(f'XGBoost test accuracy: {XGB_model_0.score(X_test, y_test).round(3)}')\n\nXGBoost train accuracy: 1.0\nXGBoost test accuracy: 0.995\n\n\n\ny_predicted_1 = XGB_model_0.predict(X_test)\n\n# Generate confusion matrix\ncf_matrix_1 = confusion_matrix(y_test, y_predicted_1)\n\n# label rows and columns\ncf_df_1 = pd.DataFrame(\n    cf_matrix_1,\n    columns=[\"Predicted 0\", \"Predicted 1\", \"Predicted 2\"],\n    index=[\"True 0\", \"True 1\", \"True 2\"])\n\n\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(XGB_model, X_test, y_test)\n\nprint('==========================================================')\n\n\n# Precision Recall and F1 scores\nclass_report_xgb_0 = classification_report(y_test, y_predicted_1)\nprint(class_report_xgb_0)\nprint('==========================================================')\n\n\n# AUC score\ny_proba_train_1 = XGB_model_0.predict_proba(X_train)\nauc_train_1 = np.round(roc_auc_score(\n    y_train, y_proba_train_1, multi_class='ovo'), 3)\nprint('XGBoost model AUC score')\nprint(f'roc_auc_score: {auc_train_1}')\n\nprint('==========================================================')\ncf_df_1\n\n==========================================================\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      1850\n           1       0.99      0.99      0.99       580\n           2       0.99      0.96      0.97       163\n\n    accuracy                           0.99      2593\n   macro avg       0.99      0.98      0.99      2593\nweighted avg       0.99      0.99      0.99      2593\n\n==========================================================\nXGBoost model AUC score\nroc_auc_score: 1.0\n==========================================================\n\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n\n\n\n  \n    \n      \n      Predicted 0\n      Predicted 1\n      Predicted 2\n    \n  \n  \n    \n      True 0\n      1848\n      1\n      1\n    \n    \n      True 1\n      4\n      575\n      1\n    \n    \n      True 2\n      5\n      2\n      156\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for XGBoost model:\n\nXGBoost model is an ensemble model, it classifies data using decision trees in ensembles. Using max_depth as the hyperparameter, which determines the maximium depth of the trees. I did not use the scaled, oversamples, and PCA transformed data for this model, as this is not needed.\nFor XGB_model_0 the appropriate value for max_depth was determined to be 3, this gave an accuracy score of 1.0 for training data and 0.995 for test data. The model did not overfit on the training data, the model is performing just as well on the training data and on unseen data (test set).\nThe model performed very well on the data, showing high F1 scores, identical to the KNN_model_0, which shows that the use of two different classification models with different approaches are able to classify the target data really well.\nFurthermore, this model performed well on distinguishing between the classes with an AUC score of 1.0.\n\nfeat_imprt = pd.DataFrame(\n    XGB_model_0.feature_importances_).sort_values(by=0, ascending=False)\n\nfeat_names_0 = pd.DataFrame(X_train.columns, columns=['feature_names'])\n\n# join inner because the ndarray lengths are different\nfeat_coefs_0 = pd.concat([feat_names_0, feat_imprt],\n                         axis=1, ignore_index=True, join='inner')\n\nfeat_coefs_0 = feat_coefs_0.sort_values(\n    by=1, ascending=False).set_index(0).head(20)\n\n# plotting coefficients\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(15, 8))\nplt.bar(feat_coefs_0.index, feat_imprt[0][:20])\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nFeature importance for XGBoost models shows the features that the model deemed most important in making the classification decision.\nthe above bar chart shows the codons importance according to gain metric, which is the relative contribution of each feature for each tree in the ensemble (XGBoost) model. This means that UGA having the highest values of importance were the most important in generating a prediction of the outcome classification."
  },
  {
    "objectID": "posts/04-DnaType-Classification.html#ensemble-model",
    "href": "posts/04-DnaType-Classification.html#ensemble-model",
    "title": "Jasmine Marzouk",
    "section": "Ensemble Model",
    "text": "Ensemble Model\n\n# We instantiate the base models, along with their names\nbase_models = [('KNN', KNN_model_0),\n               ('XGBoost', XGB_model_0)\n               ]\n\n\n# building the stacked model\nstacked_model = StackingClassifier(\n    estimators=base_models,\n    final_estimator=LogisticRegression(random_state=123, max_iter=10000),\n    passthrough=True)\n\nstacked_model.fit(X_train, y_train)\n\nStackingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=3)),\n                               ('XGBoost',\n                                XGBClassifier(base_score=0.5, booster='gbtree',\n                                              callbacks=None,\n                                              colsample_bylevel=1,\n                                              colsample_bynode=1,\n                                              colsample_bytree=1,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None, gamma=0,\n                                              gpu_id=-1,\n                                              grow_policy='depthwise',\n                                              importance_type=None,\n                                              interaction_c...\n                                              learning_rate=0.300000012,\n                                              max_bin=256, max_cat_to_onehot=4,\n                                              max_delta_step=0, max_depth=3,\n                                              max_leaves=0, min_child_weight=1,\n                                              missing=nan,\n                                              monotone_constraints='()',\n                                              n_estimators=100, n_jobs=0,\n                                              num_parallel_tree=1,\n                                              objective='multi:softprob',\n                                              predictor='auto', random_state=0,\n                                              reg_alpha=0, ...))],\n                   final_estimator=LogisticRegression(max_iter=10000,\n                                                      random_state=123),\n                   passthrough=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StackingClassifierStackingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=3)),\n                               ('XGBoost',\n                                XGBClassifier(base_score=0.5, booster='gbtree',\n                                              callbacks=None,\n                                              colsample_bylevel=1,\n                                              colsample_bynode=1,\n                                              colsample_bytree=1,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None, gamma=0,\n                                              gpu_id=-1,\n                                              grow_policy='depthwise',\n                                              importance_type=None,\n                                              interaction_c...\n                                              learning_rate=0.300000012,\n                                              max_bin=256, max_cat_to_onehot=4,\n                                              max_delta_step=0, max_depth=3,\n                                              max_leaves=0, min_child_weight=1,\n                                              missing=nan,\n                                              monotone_constraints='()',\n                                              n_estimators=100, n_jobs=0,\n                                              num_parallel_tree=1,\n                                              objective='multi:softprob',\n                                              predictor='auto', random_state=0,\n                                              reg_alpha=0, ...))],\n                   final_estimator=LogisticRegression(max_iter=10000,\n                                                      random_state=123),\n                   passthrough=True)KNNKNeighborsClassifierKNeighborsClassifier(n_neighbors=3)XGBoostXGBClassifierXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=100,\n              n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n              predictor='auto', random_state=0, reg_alpha=0, ...)final_estimatorLogisticRegressionLogisticRegression(max_iter=10000, random_state=123)\n\n\n\nprint(\n    f'Stacked model train accuracy: {stacked_model.score(X_train, y_train).round(3)}')\nprint(\n    f'Stacked model test accuracy: {stacked_model.score(X_test, y_test).round(3)}')\n\nStacked model train accuracy: 1.0\nStacked model test accuracy: 0.995\n\n\n\ny_predicted_2 = stacked_model.predict(X_test)\n\n# Generate confusion matrix\ncf_matrix_2 = confusion_matrix(y_test, y_predicted_2)\n\n# label rows and columns\ncf_df_2 = pd.DataFrame(\n    cf_matrix_2,\n    columns=[\"Predicted 0\", \"Predicted 1\", \"Predicted 2\"],\n    index=[\"True 0\", \"True 1\", \"True 2\"])\n\n\nsns.set_theme(style=\"dark\")\nplot_confusion_matrix(stacked_model, X_test, y_test)\n\nprint('==========================================================')\n\n\n# Precision Recall and F1 scores\nclass_report_stack_0 = classification_report(y_test, y_predicted_2)\nprint(class_report_stack_0)\nprint('==========================================================')\n\n\n# AUC score\ny_proba_train_2 = stacked_model.predict_proba(X_train)\nauc_train_2 = np.round(roc_auc_score(\n    y_train, y_proba_train_2, multi_class='ovo'), 3)\nprint('AUC score for Stacked model')\nprint(f'roc_auc_score: {auc_train_2}')\nprint('==========================================================')\n\ncf_df_2\n\n/home/amina/anaconda3/envs/plotly_bokeh/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n  warnings.warn(msg, category=FutureWarning)\n\n\n==========================================================\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      1850\n           1       0.99      0.99      0.99       580\n           2       0.99      0.96      0.97       163\n\n    accuracy                           0.99      2593\n   macro avg       0.99      0.98      0.99      2593\nweighted avg       0.99      0.99      0.99      2593\n\n==========================================================\nAUC score for Stacked model\nroc_auc_score: 1.0\n==========================================================\n\n\n\n\n\n\n  \n    \n      \n      Predicted 0\n      Predicted 1\n      Predicted 2\n    \n  \n  \n    \n      True 0\n      1848\n      1\n      1\n    \n    \n      True 1\n      4\n      575\n      1\n    \n    \n      True 2\n      5\n      2\n      156\n    \n  \n\n\n\n\n\n\n\n\nInterpreting Results for the sctacked (ensemble) model:\n\nFor the ensemble or stacked model, I used the models I already trained and tuned, with the exception of the logistic regression model log_reg_0; I decided to exclude this model as it was the model that underperformed on classifying the target variable.\nI only included XGB_model_0 and KNN_model_0 as base models, with a final estimator being a default Logistic Regression model. Each of the base estimators is different in the way it approaches the classification.\nThe ensemble model classifies by taking the outputs from both the XGBoost model and KNN model as probabilites of each class as an input, the final estimator being a logistic regression then uses logistic function to classify each class.\nstacked_model achieved an accuracy score of 1.0 for the train data and 0.995 for the test data.\nThe stacked model did well in distinguishing between the classes with an AUC of 1.0."
  },
  {
    "objectID": "posts/04-DnaType-Classification.html#comparing-models",
    "href": "posts/04-DnaType-Classification.html#comparing-models",
    "title": "Jasmine Marzouk",
    "section": "Comparing Models",
    "text": "Comparing Models\n\ncls = classification_report(y_test, y_predicted, output_dict=True)\ncls_0 = classification_report(y_test, y_predicted_0, output_dict=True)\ncls_1 = classification_report(y_test, y_predicted_1, output_dict=True)\ncls_2 = classification_report(y_test, y_predicted_2, output_dict=True)\n\n\ncomparison_df = pd.DataFrame({'Precision':\n                             [cls['weighted avg']['precision'],\n                              cls_0['weighted avg']['precision'],\n                              cls_1['weighted avg']['precision'],\n                              cls_2['weighted avg']['precision']],\n                              'Recall':\n                             [cls['weighted avg']['recall'],\n                              cls_0['weighted avg']['recall'],\n                              cls_1['weighted avg']['recall'],\n                              cls_2['weighted avg']['recall']],\n                             'F1 score':\n                              [cls['weighted avg']['f1-score'],\n                              cls_0['weighted avg']['f1-score'],\n                               cls_1['weighted avg']['f1-score'],\n                              cls_2['weighted avg']['f1-score']],\n                              'Accuracy':\n                              [cls['accuracy'],\n                               cls_0['accuracy'],\n                               cls_1['accuracy'],\n                               cls_2['accuracy']],\n                              'AUC score':\n                              [auc_train,\n                               auc_train_0,\n                               auc_train_1,\n                               auc_train_2]\n                              },\n                             index=['Logistic Regression', 'kNearest Neighbors', 'XGBoost', 'Stacked model'])\n\ncomparison_df.round(3).sort_values(by='F1 score', ascending=False)\n\n\n\n\n\n  \n    \n      \n      Precision\n      Recall\n      F1 score\n      Accuracy\n      AUC score\n    \n  \n  \n    \n      XGBoost\n      0.995\n      0.995\n      0.995\n      0.995\n      1.0\n    \n    \n      Stacked model\n      0.995\n      0.995\n      0.995\n      0.995\n      1.0\n    \n    \n      kNearest Neighbors\n      0.993\n      0.993\n      0.993\n      0.993\n      1.0\n    \n    \n      Logistic Regression\n      0.985\n      0.985\n      0.985\n      0.985\n      1.0\n    \n  \n\n\n\n\nAccording to the F1 score XGBoost model and the Stacked model performed the best and similarly. This makes sense, contrary to the Kingdom classification, the DNAtype only has three classes, which means that the XGBoost would perform better in this classification as it can get a pure node easier.\nLooking at the vanilla models performed at the beginning the accuracy score for the kNearest Neighbor performed was 99.3%, so I only achived an improvement of %0.2 for the DNAtype classses classification."
  },
  {
    "objectID": "posts/04-DnaType-Classification.html#saving-models",
    "href": "posts/04-DnaType-Classification.html#saving-models",
    "title": "Jasmine Marzouk",
    "section": "Saving Models:",
    "text": "Saving Models:\n\n# Logistic Regression model\nlog_reg_dna = 'finalized_log_reg_dna.sav'\njoblib.dump(log_reg_0, log_reg_dna)\n\n\n# kNearest Neighbor model\nknn_dna = 'finalized_knn_dna.sav'\njoblib.dump(KNN_model_0, knn_dna)\n\n# XGBoost model\nxgboost_dna = 'finalized_xgboost_dna.sav'\njoblib.dump(XGB_model_0, xgboost_dna)\n\n# Ensemble model\nstacked_dna = 'finalized_stacked_dna.sav'\njoblib.dump(stacked_model, stacked_dna)\n\n['finalized_stacked_dna.sav']"
  },
  {
    "objectID": "posts/03-Exploratory-Data-Analysis.html#exploratory-data-analysis-method",
    "href": "posts/03-Exploratory-Data-Analysis.html#exploratory-data-analysis-method",
    "title": "Jasmine Marzouk",
    "section": "Exploratory Data Analysis Method:",
    "text": "Exploratory Data Analysis Method:\nIn order to explore the data set and understand what it contains I will be aiming to answer the following questions:\n\nWhat is the distribution of the target classes for Kingdom?\nWhat is the distribution of the target classes for DNAtype?\nWhat does the distribution for the Ncodons column look like?\nAre there codons that have a higher occurance in certain DNA types?\nAre there codons that have a higher occurence in certain Kingdoms?\n\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import (GridSearchCV, cross_val_score,\n                                     train_test_split)\nfrom sklearn.metrics import (accuracy_score, auc, classification_report,\n                             confusion_matrix, f1_score, plot_confusion_matrix,\n                             plot_roc_curve, precision_score, recall_score,\n                             roc_auc_score, roc_curve)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nimport warnings\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom scipy import stats\n\n%matplotlib inline\n\n\n# warnings.filterwarnings(\"ignore\")\nnp.random.seed(123)\n\n\n# importing the cleaned dataframe dfcodon1 and renaming it to codon\n\ncodon = joblib.load('../data/dfcodon1.pkl')\n\n\n# Dataframe Shape\nprint(\n    f'The dataframe contains {codon.shape[0]} rows and {codon.shape[1]} columns')\n\nThe dataframe contains 13028 rows and 69 columns\n\n\n\n# peeking at the dataframe\ncodon.sample(5)\n\n\n\n\n\n  \n    \n      \n      Kingdom\n      DNAtype\n      SpeciesID\n      Ncodons\n      SpeciesName\n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      11544\n      vertebrate\n      1\n      71258\n      4460\n      mitochondrion Hypopomus occidentalis\n      0.01951\n      0.02623\n      0.02354\n      0.01076\n      0.03274\n      ...\n      0.00740\n      0.00000\n      0.00000\n      0.00090\n      0.00269\n      0.01726\n      0.00359\n      0.00404\n      0.00314\n      0.03139\n    \n    \n      12258\n      mammal\n      0\n      30521\n      14077\n      Bos grunniens\n      0.01698\n      0.01954\n      0.00789\n      0.01435\n      0.01279\n      ...\n      0.00902\n      0.01179\n      0.00980\n      0.02579\n      0.02941\n      0.02998\n      0.03467\n      0.00107\n      0.00078\n      0.00199\n    \n    \n      2177\n      virus\n      0\n      394337\n      1359\n      Tomato leaf curl New Delhi virus-[Multan;Luffa]\n      0.02134\n      0.02428\n      0.00736\n      0.01177\n      0.01545\n      ...\n      0.00736\n      0.01472\n      0.01325\n      0.02943\n      0.01840\n      0.02796\n      0.01545\n      0.00221\n      0.00147\n      0.00221\n    \n    \n      11615\n      vertebrate\n      0\n      7792\n      11800\n      Heterodontus francisci\n      0.01661\n      0.01720\n      0.00712\n      0.01025\n      0.00941\n      ...\n      0.00890\n      0.01517\n      0.01025\n      0.02017\n      0.02339\n      0.02881\n      0.02661\n      0.00068\n      0.00042\n      0.00144\n    \n    \n      3172\n      bacteria\n      0\n      320122\n      13771\n      Clostridium phage phi CD119\n      0.03544\n      0.00588\n      0.04480\n      0.01089\n      0.01343\n      ...\n      0.00029\n      0.02926\n      0.00508\n      0.04647\n      0.01017\n      0.06739\n      0.01808\n      0.00334\n      0.00182\n      0.00058\n    \n  \n\n5 rows × 69 columns\n\n\n\n\n# Describing the dataframe\ncodon.describe()\n\n\n\n\n\n  \n    \n      \n      DNAtype\n      SpeciesID\n      Ncodons\n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      CUC\n      CUA\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      count\n      13028.000000\n      13028.000000\n      1.302800e+04\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      ...\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n    \n    \n      mean\n      0.367209\n      130451.105926\n      7.960576e+04\n      0.024819\n      0.023440\n      0.020635\n      0.014104\n      0.017822\n      0.018286\n      0.019043\n      ...\n      0.005452\n      0.009929\n      0.006422\n      0.024183\n      0.021164\n      0.028292\n      0.021683\n      0.001640\n      0.000590\n      0.006178\n    \n    \n      std\n      0.688726\n      124787.086107\n      7.197010e+05\n      0.017627\n      0.011597\n      0.020709\n      0.009279\n      0.010587\n      0.014572\n      0.024251\n      ...\n      0.006601\n      0.008574\n      0.006387\n      0.013826\n      0.013038\n      0.014343\n      0.015018\n      0.001785\n      0.000882\n      0.010344\n    \n    \n      min\n      0.000000\n      7.000000\n      1.000000e+03\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.000000\n      28850.750000\n      1.602000e+03\n      0.013910\n      0.015380\n      0.005610\n      0.007108\n      0.010890\n      0.007830\n      0.005300\n      ...\n      0.001220\n      0.001690\n      0.001168\n      0.012390\n      0.011860\n      0.017360\n      0.009710\n      0.000560\n      0.000000\n      0.000410\n    \n    \n      50%\n      0.000000\n      81971.500000\n      2.927500e+03\n      0.021750\n      0.021905\n      0.015260\n      0.013360\n      0.016130\n      0.014560\n      0.009680\n      ...\n      0.003530\n      0.009270\n      0.004540\n      0.025435\n      0.019070\n      0.026085\n      0.020540\n      0.001380\n      0.000420\n      0.001130\n    \n    \n      75%\n      1.000000\n      222891.250000\n      9.120000e+03\n      0.031310\n      0.029210\n      0.029485\n      0.019803\n      0.022730\n      0.025110\n      0.017245\n      ...\n      0.007150\n      0.015922\n      0.010250\n      0.034190\n      0.027690\n      0.036800\n      0.031122\n      0.002370\n      0.000830\n      0.002890\n    \n    \n      max\n      12.000000\n      465364.000000\n      4.066258e+07\n      0.217300\n      0.091690\n      0.151330\n      0.101190\n      0.089780\n      0.100350\n      0.163920\n      ...\n      0.055540\n      0.098830\n      0.058430\n      0.185660\n      0.113840\n      0.144890\n      0.158550\n      0.045200\n      0.025610\n      0.106700\n    \n  \n\n8 rows × 67 columns\n\n\n\n\n# Setting a theme for all the visualisations\nsns.set_theme(style=\"darkgrid\")\nsns.set(rc={'figure.figsize': (11.7, 8.27)})\n\n\nPlan for EDA: - Univariate alalysis - Bivariate analysis - Check codon distribution for each Kingdom - Check codon distribution for each DNAtype"
  },
  {
    "objectID": "posts/03-Exploratory-Data-Analysis.html#univariate-analysis",
    "href": "posts/03-Exploratory-Data-Analysis.html#univariate-analysis",
    "title": "Jasmine Marzouk",
    "section": "Univariate Analysis",
    "text": "Univariate Analysis\nDue to the dataframe containing 69 columns, this will be hard to visualise without breaking up the dataframe into focus points. So, I will isolate the codon columns into a separate dataframe and the rest of the columns into another one, and then do univariate analysis.\n\nSeparating the codon only columns\n\n\njustcodon = codon.drop(\n    columns=['Kingdom', 'DNAtype', 'SpeciesID', 'Ncodons', 'SpeciesName'], axis=1)\njustcodon.sample(3)\n\n\n\n\n\n  \n    \n      \n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      CUC\n      CUA\n      CUG\n      AUU\n      AUC\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      1830\n      0.02869\n      0.01967\n      0.01742\n      0.01527\n      0.02859\n      0.01312\n      0.01199\n      0.00932\n      0.02664\n      0.01178\n      ...\n      0.00113\n      0.01445\n      0.00451\n      0.03043\n      0.02244\n      0.02613\n      0.02152\n      0.00092\n      0.00000\n      0.00020\n    \n    \n      2592\n      0.00948\n      0.02212\n      0.00316\n      0.01422\n      0.00790\n      0.01106\n      0.00790\n      0.02291\n      0.01106\n      0.02212\n      ...\n      0.00316\n      0.01422\n      0.00790\n      0.00711\n      0.02765\n      0.01896\n      0.02291\n      0.00079\n      0.00000\n      0.00000\n    \n    \n      1004\n      0.01659\n      0.02461\n      0.01495\n      0.00981\n      0.01207\n      0.01433\n      0.01130\n      0.00940\n      0.01644\n      0.02250\n      ...\n      0.00817\n      0.01521\n      0.01433\n      0.02666\n      0.02214\n      0.02764\n      0.01741\n      0.00319\n      0.00108\n      0.00057\n    \n  \n\n3 rows × 64 columns\n\n\n\n\nSeparating the non codon into dataframe\n\n\nnon_codon = codon.drop(columns=codon.columns[5:], axis=1)\nnon_codon.head(3)\n\n\n\n\n\n  \n    \n      \n      Kingdom\n      DNAtype\n      SpeciesID\n      Ncodons\n      SpeciesName\n    \n  \n  \n    \n      0\n      virus\n      0\n      100217\n      1995\n      Epizootic haematopoietic necrosis virus\n    \n    \n      1\n      virus\n      0\n      100220\n      1474\n      Bohle iridovirus\n    \n    \n      2\n      virus\n      0\n      100755\n      4862\n      Sweet potato leaf curl virus\n    \n  \n\n\n\n\n\nVisualising non_codon\n\nWhat is the distribution of the target classes for Kingdom?\nWhat is the distribution of the target classes for DNAtype?\n\nI will not be looking at SepciesID or SpeciesName as these columns won’t have a distribution that offers any noteworthy insights.\n\nfor col in non_codon[['Kingdom', 'DNAtype', 'Ncodons']]:\n    plt.figure()\n    plt.title(f'Feature: {col}')\n    plt.hist(codon[col], bins=20)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nKingdom shows the counts of the different kingdom, with bacterial and viral being the ones with the most data points in this dataset.\nDNAtype has 12 unique values, however there only seems to show 0, 1, and 2. I will have to apply a log to this graph to see the counts of the other dna types and they could be muted by the high counts from the top three.\nNcodons another distribution that is heavily skewed, I will visualise this one with a log transformation.\n\n\n# Looking at the Kingdom column distribution using log transformation:\nnon_codon['Kingdom'].value_counts().plot(kind='bar', log=True)\nplt.title('Kingdom Column Distribution')\nplt.xlabel('Kingdoms')\nplt.ylabel('Log Counts')\n\n# Proportions of the Kingdom counts:\nnon_codon['Kingdom'].value_counts(normalize=True)*100\n\nbacteria        22.413264\nvirus           21.737796\nplant           19.365981\nvertebrate      15.942585\ninvertebrate    10.323918\nmammal           4.390543\nphage            1.688671\nrodent           1.650292\nprimate          1.381640\narchaea          0.967148\nplasmid          0.138164\nName: Kingdom, dtype: float64\n\n\n\n\n\nThe dataset does contain rows for kingdoms beside the top three, however there is a high imbalance here. with some of the classes having less than 5%. I will drop plm since it is <2% (18 entries).\n\n# Looking at DNAtype column distribution with log transformation:\nnon_codon['DNAtype'].value_counts().plot(kind='bar', log=True)\nplt.title('DNAtype column distribution')\nplt.xlabel('DNA Type')\nplt.ylabel('Log Counts')\n\nprint('Proportions of DNAtype counts')\n# Proportions of the DNAtype counts:\nnon_codon['DNAtype'].value_counts(normalize=True)*100\n\nProportions of DNAtype counts\n\n\n0     71.131409\n1     22.252072\n2      6.263433\n4      0.237949\n12     0.038379\n3      0.015352\n9      0.015352\n5      0.015352\n11     0.015352\n6      0.007676\n7      0.007676\nName: DNAtype, dtype: float64\n\n\n\n\n\n\nThoughts:\n\nThe columns Kingdoms and DNAtype will be the targets (separately) that I will be aiming to classify. Looking at the distributions, and the imbalances between the classes I will aim to drop some classes and the following is my reasoning.\nIn terms of kingdoms, the Dataset contains the following: - pri: primate - rod: rodent - mam: mammalian - vrt: vertebrate - inv: invertebrate - pln: plant - bct: bacteria - vrl: virus - phg :bacteriophage - arc: archaea - plm: plasmid\nprimate ,rodent, and mammalian can be grouped and renamed under the vertebrate kingdom, because they are vertebrates and are all mammals - vrt = vrt + pri + rod + mam\nThe column will then have the following classes: - vrt: vertebrate - inv: invertebrate - pln: plant - bct: bacteria - vrl: virus - phg :bacteriophage - arc: archaea - plm: plasmid\nI will further disregard the rows that are class plm, as there are far too little entries (18 only).\nFinal classes will be:\n\nvrt: vertebrate\ninv: invertebrate\npln: plant\nbct: bacteria\nvrl: virus\nphg :bacteriophage\narc: archaea\n\nFinally I will look at the distribution after these changes.\nFor the column DNAtype, it contains the following classes: - 0: nuclear - 1: mitochondrion - 2: chloroplast - 3: cyanelle - 4: plastid - 5: nucleomorph - 6: secondary endosymbiont - 7: chromoplast - 8: leukoplast - 9: NA - 10: proplastid - 11: apicoplast - 12: kinetoplast\nAs seen in the DNAtype distribution, there is a high imbalance between these classes. I will focus here on the top three which are 0 (nuclear), 1 (mitochonrion), and 2(chloroplast). These three DNA types that are present in all of the kingdoms in our dataset, so I will go ahead with these as they are representative. I will drop all rows that do not contain 0, 1, or 2, as the are less than 1% of the dataset.\n\nKingdom Column\n\n\n# changing the value names as stated above:\nnon_codon = non_codon.replace(['primate', 'rodend', 'mammal'], 'vertebrate')\n\n# dropping rows with value plm\nnon_codon = non_codon.drop(\n    non_codon.loc[non_codon['Kingdom'] == 'plasmid'].index)\n\n\nnon_codon['Kingdom'].value_counts().plot(\n    kind='bar', log=True)  # no log applied\nplt.title('Kingdom Column Distribution')\nplt.xlabel('Kingdoms')\nplt.ylabel('Log Counts')\n\nText(0, 0.5, 'Log Counts')\n\n\n\n\n\n\nnon_codon['Kingdom'].value_counts()\n\nbacteria        2920\nvirus           2832\nvertebrate      2829\nplant           2523\ninvertebrate    1345\nphage            220\nrodent           215\narchaea          126\nName: Kingdom, dtype: int64\n\n\nAs there are still imbalances in the data, this makes sense since genome lengths differ between species, so this imbalance is inevitable. For example for archea there are only 209 species,\n\nNote: these changes have currently been made to the dataframe non_codon I will now apply these changes to the main dataframe codon\n\n\n# changing the value names as stated above:\ncodon = codon.replace(['primate', 'rodent', 'mammal'], 'vertebrate')\n\n# dropping rows with value plm\ncodon = codon.drop(codon.loc[codon['Kingdom'] == 'plasmid'].index)\n\n\nDNAtype Column\n\n\n# Dropping all rows that do not contain 0, 1, or 2 as DNAtype:\nnon_codon = non_codon.drop(non_codon.loc[non_codon['DNAtype'] > 2].index)\n\n\n# Checking distribution:\nnon_codon['DNAtype'].value_counts().plot(\n    kind='bar', log=True)  # log transformation not applied\nplt.title('DNAtype column distribution')\nplt.xlabel('DNA Type')\nplt.ylabel('Log Counts')\n\nText(0, 0.5, 'Log Counts')\n\n\n\n\n\nThe classes are highly imbalanced, but this can be adjusted later on during the modeling section.\nAgain, I will apply these changes to the original codon dataframe for consistency.\n\n# Dropping rows in DNAtype with values larger than 2:\ncodon = codon.drop(codon.loc[codon['DNAtype'] > 2].index)\n\n\n# Checking distribution in codon dataframe:\ncodon['DNAtype'].value_counts().plot(kind='bar')\n\n<AxesSubplot:>\n\n\n\n\n\n\nWhat does the distribution for the Ncodons column look like?\n\n\n# checking log transformed distribution\nplt.figure()\nplt.hist(non_codon['Ncodons'], bins=100)\nplt.show()\n\nprint('===============================================')\nnon_codon['Ncodons'] = np.log(non_codon['Ncodons']+1)\nnon_codon['Ncodons']\n\n\n\n\n===============================================\n\n\n0         7.598900\n1         7.296413\n2         8.489411\n3         7.557995\n4        10.035918\n           ...    \n13023     7.001246\n13024     7.634337\n13025     7.430707\n13026    17.520819\n13027    16.012624\nName: Ncodons, Length: 12964, dtype: float64\n\n\nNcodons is highly skewed, this is because there are more species with smaller genomes and so smaller number of codons in the sequence.\nApplying the log transformation change to the original dataframe codon\n\ncodon['Ncodons'] = np.log(codon['Ncodons']+1)\ncodon['Ncodons']\n\n0         7.598900\n1         7.296413\n2         8.489411\n3         7.557995\n4        10.035918\n           ...    \n13023     7.001246\n13024     7.634337\n13025     7.430707\n13026    17.520819\n13027    16.012624\nName: Ncodons, Length: 12964, dtype: float64\n\n\nFinally, the dataframe shape after transformation.\n\n# the dataframe containing only the non_codon columns\nprint(\n    f'The non_codon dataframe contains {non_codon.shape[0]} rows and {non_codon.shape[1]}')\n# the original codon dataframe\nprint(\n    f'The codon dataframe contains {codon.shape[0]} rows and {codon.shape[1]}')\n\nThe non_codon dataframe contains 12964 rows and 5\nThe codon dataframe contains 12964 rows and 69\n\n\n\n\n\nVisualising non_codon\n\njustcodon.describe()\n\n\n\n\n\n  \n    \n      \n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      CUC\n      CUA\n      CUG\n      AUU\n      AUC\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      count\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      ...\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n    \n    \n      mean\n      0.024819\n      0.023440\n      0.020635\n      0.014104\n      0.017822\n      0.018286\n      0.019043\n      0.018455\n      0.028354\n      0.025036\n      ...\n      0.005452\n      0.009929\n      0.006422\n      0.024183\n      0.021164\n      0.028292\n      0.021683\n      0.001640\n      0.000590\n      0.006178\n    \n    \n      std\n      0.017627\n      0.011597\n      0.020709\n      0.009279\n      0.010587\n      0.014572\n      0.024251\n      0.016583\n      0.017505\n      0.014595\n      ...\n      0.006601\n      0.008574\n      0.006387\n      0.013826\n      0.013038\n      0.014343\n      0.015018\n      0.001785\n      0.000882\n      0.010344\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.013910\n      0.015380\n      0.005610\n      0.007108\n      0.010890\n      0.007830\n      0.005300\n      0.007180\n      0.016370\n      0.015130\n      ...\n      0.001220\n      0.001690\n      0.001168\n      0.012390\n      0.011860\n      0.017360\n      0.009710\n      0.000560\n      0.000000\n      0.000410\n    \n    \n      50%\n      0.021750\n      0.021905\n      0.015260\n      0.013360\n      0.016130\n      0.014560\n      0.009680\n      0.012800\n      0.025480\n      0.021540\n      ...\n      0.003530\n      0.009270\n      0.004540\n      0.025435\n      0.019070\n      0.026085\n      0.020540\n      0.001380\n      0.000420\n      0.001130\n    \n    \n      75%\n      0.031310\n      0.029210\n      0.029485\n      0.019803\n      0.022730\n      0.025110\n      0.017245\n      0.024330\n      0.038113\n      0.031860\n      ...\n      0.007150\n      0.015922\n      0.010250\n      0.034190\n      0.027690\n      0.036800\n      0.031122\n      0.002370\n      0.000830\n      0.002890\n    \n    \n      max\n      0.217300\n      0.091690\n      0.151330\n      0.101190\n      0.089780\n      0.100350\n      0.163920\n      0.107370\n      0.154060\n      0.088600\n      ...\n      0.055540\n      0.098830\n      0.058430\n      0.185660\n      0.113840\n      0.144890\n      0.158550\n      0.045200\n      0.025610\n      0.106700\n    \n  \n\n8 rows × 64 columns\n\n\n\nThe justcodon dataframe created from the original codon dataframe contains all the codon columns which are 64 columns in total, each column representing one of the 64 codons.\nThe values in these columns are frequencies of the codon occurrence in each organism’s genome (from nuclear, mitochondrial, or chloroplast RNA)\nThe frequencies are caculated by taking a count of the total number of codons in the sequenced genome, counting the individual codon occurance, and then dividing this number by the total number of codons.\nfor example : for Enterobacteria phage P1, the total number of codons is 71879, and the number of UGU occurances is 362, so the codon frequency is 0.00503.\n\nCodon columns distributions:\n\n\nplt.figure(figsize=(30, 10))\nsns.boxplot(data=justcodon, palette='Blues')\nplt.title('Codon Distributions')\nplt.axhline(y=justcodon.median().mean(), color='r')\nplt.xlabel('Codons')\nplt.show()\n\n\n\n\nA few observations - The stop codons UAA, UAG UGA have the lowest occurance, this makes sense since stop codons are what tells the translation machinery when to stop translation, so they will not be as frequent in the RNA/DNA sequence as the rest of the codons.\n\nThe start codon AUG occurance is quite similar accross all species as the distribution is tending towards the median.\n\nUUU UUC:\n\nThese codons encode for the amino acid Phenylalanine. As this amino acid only has two codons that can encode for it, the distributions have a similar median, but UUU has slightly more recorded occurances among the different species.\n\nAUU AUC AUA:\n\nThese three codons encode for the amino acid Isoleucine. From the distributions AUU and AUC has higher occurances in the species. So they tend to be the ones found most among species to encode for the amino acid, with AUU having a higher median so it is even more favoured over AUC.\n\nACU ACC ACA ACG\n\nThese encode for the amino acid Theronine. The distributions show that ACC and ACA have a wider range of occurances among the species while ACU codon appears to be the higher occuring one for this amino acid.\n\nCAA CAG\n\nThese codons encode for the amino acid Glutamine. From the distributions it is apparent that CAA is more frequent in its occurance among species.\n\nAAU AAC\n\nThese codons encode for the amino acid Asparagine. Seeing that this amino acid only has two type of codons that can encode it, the distributions show a similar median for both however AAC tends to be favoured.\n\nThese frequencies are highly dependent on the size of the sequence, DNA length that was sequenced. Therefore, it is important to view their distributions in relation to either DNA type or Kingdom."
  },
  {
    "objectID": "posts/03-Exploratory-Data-Analysis.html#bivariate-analysis",
    "href": "posts/03-Exploratory-Data-Analysis.html#bivariate-analysis",
    "title": "Jasmine Marzouk",
    "section": "Bivariate Analysis",
    "text": "Bivariate Analysis\n\nKingdom classes:\n\nAre there codons that have a higher occurence in certain Kingdoms?\n\n\nclustergraph_kingdom = codon.drop(\n    columns=['DNAtype', 'SpeciesID', 'Ncodons', 'SpeciesName'], axis=1)\nclustergraph_kingdom.set_index('Kingdom', inplace=True)\n\n\nfig = px.imshow(clustergraph_kingdom.groupby(level='Kingdom').mean(\n),  color_continuous_scale='Cividis', origin='lower')  # or RdBu_r\nfig.update_layout()\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nFrom the heatmap it appears that CUA has a high occurance in the vertebrate kingdom. furthermore the Aspartic Acid (GAU and GAC) and Glutamic Acid (GAA and GAG) codons seem to have high occurances in all of the kingdoms but not as much in the vertebrate kingdom in comparison with the rest.\nCAG has a higher occurance for archea kingdom.\nCUA is highly favoured in vertebrates, this codon encodes for Leucine.\n\n\nDNAtype classes:\n\nAre there codons that have a higher occurance in certain DNA types?\n\n\nclustergraph_dna = codon.drop(\n    columns=['Kingdom', 'SpeciesID', 'Ncodons', 'SpeciesName'], axis=1)\nclustergraph_dna.set_index('DNAtype', inplace=True)\n\n\nfig = px.imshow(clustergraph_dna.groupby(level='DNAtype').mean(),\n                color_continuous_scale='Cividis', origin='lower')  # or RdBu_r\nfig.update_layout()\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nIn the heatmap for DNAtype codon frequencies, there are some codons that have frequencies. It appears that for DNAtype 1 which is mitochondria, there is a higher frequency of CUA which codes for the amino acid Leucine. While for Nuclear DNA there is almost an avoidance of the CUA codon. Moreover, in terms of the stop codons, it appears that mitochondrial DNA (1) seems to favour UGA stop codon over the over two.\nAnother noteworthy pattern, mitochondrial DNA shows obvious order of favouring for the Theronine amino acid encoding codons; ACG, ACA, ACC, and ACU. There is a higher occurance of ACA for this amino acid.\nNuclear DNA (0) seems to have a different pattern, in terms of frequency there are obvious codons that are more occurent for amino acids, however it isn’t as pronounced."
  },
  {
    "objectID": "posts/01-Introduction .html",
    "href": "posts/01-Introduction .html",
    "title": "Jasmine Marzouk",
    "section": "",
    "text": "01 - Introduction\n\nWhat is a codon?\n\nDNA and RNA are genetic material that contain the code to encode for proteins which are important to the various functionings of the body. Proteins are made up of sequences of amino acids, these amino acids are encoded for by codons.\nCodons are a sequence of three nucleotides from either of the four nucleotides that make up RNA U: Uracil, A: Adenine, G: Guanine, and C: Cytosine. There are 64 possible combinations of these codons, but they only encode for ~20 amino acids. This creates a redundency in the genetic code, this gives rise to Codon Bias.\nCodon bias is a term used to refer to the fact that for a known sequence of amino acids, different species will have a bias towards certain codons to encode a certain amino acid.\n\nWhy is this important?\n\nProtein synthesis is an ever important part of the Biotechnology industry, especially for drug discovery. So it is important to synthesise proteins that fold correctly to ensure, for example, inhibitor affinity to its target. This is essential to discovering and making new therapies to manage and treat various diseases.\nMany scientists will use species of bacteria to synthesise proteins for experimentation. It is useful to understand the codon bias of the species to ensure that the plasmids transferred into the bacteria contain the sequence that is faithful to that of the host’s translation and protein synthesis machinery, this ensures better folding and higher efficiency.\n\nProblem Framing:\n\nThe aim of this project is to use codon bias, rather codon frequency to determine the classification by Kingdom or DNAtype of a species.\n\n\nModelling Method:\n\nfrom IPython.display import Image\n# you can find this image in the zip folder\nImage('../notebooks/modelling_plan.png')\n# change the path accordingly.\n\n\n\n\nAs this project was attempted before by a research group who achieved an accuracy of 0.996 for their final models. I attempted to improve upon that score by employing a PCA dimensionality reduction on the data and then fit appropriate models.\n\n\nResults:\n\nKingdom classification results:\n\n\n# you can find this image in the zip folder\nImage('../notebooks/kingdom_modelling.png')\n# change the path accordingly.\n\n\n\n\n\nDNAtype classification results:\n\n\n# you can find this image in the zip folder\nImage('../notebooks/dna_modelling.png')\n# change the path accordingly.\n\n\n\n\n\n\nConclusion\nUnfortunately, I was unable to achieve an accuracy score higher than 0.996 with dimensionality reduction.\nFurther considerations would be to try and fine tune other parameters for the models, select other models like SVM model, and consider some feature engineering. However, I do not see how feature engineering would be possible on this dataset as it is not complicated and does not contain features to which any further changes could be applied.\nPerhaps, adding more data to the dataframe by scraping the CUTG website, and using species with Ncodons < 1000 in size would be an avenue to explore."
  },
  {
    "objectID": "posts/02-Data-Cleaning.html",
    "href": "posts/02-Data-Cleaning.html",
    "title": "Jasmine Marzouk",
    "section": "",
    "text": "02 - Data Cleaning\n\n\n\nData dictionary:\n\nKingdom: string: the kingdom classification of the species\n\npri: primate\nrod: rodent\nmam: mammalian\nvrt: vertebrate\ninv: invertebrate\npln: plant\nbct: bacteria\nvrl: virus\nphg :bacteriophage\narc: archaea\nplm: plasmid\n\n–\nDNAtype: int\n\n0: nuclear\n1: mitochondrion\n2: chloroplast\n3: cyanelle\n4: plastid\n5: nucleomorph\n6: secondary endosymbiont\n7: chromoplast\n8: leukoplast\n9: NA\n10: proplastid\n11: apicoplast\n12: kinetoplast\n\n–\nSpecies ID: int: the species ID on the CUTG codon database.\nNcodons: int the number of codonds in the sequence.\nSpeciesName: string Species name\nCodons: float the 64 codons, as frequencies (per thousand codons)\n\nThe dataset was found on the UCI repository for machine learning: https://archive.ics.uci.edu/ml/datasets/Codon+usage\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom scipy import stats\n\n\ndfcodon = pd.read_csv('../data/codon_usage.csv')\n\n/tmp/ipykernel_78630/713123252.py:1: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n  dfcodon = pd.read_csv('../data/codon_usage.csv')\n\n\n\n# Looking at the shape of the dataframe\ndfcodon.shape\n\n(13028, 69)\n\n\n\ndfcodon.head(10)\n\n\n\n\n\n  \n    \n      \n      Kingdom\n      DNAtype\n      SpeciesID\n      Ncodons\n      SpeciesName\n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      0\n      vrl\n      0\n      100217\n      1995\n      Epizootic haematopoietic necrosis virus\n      0.01654\n      0.01203\n      0.00050\n      0.00351\n      0.01203\n      ...\n      0.00451\n      0.01303\n      0.03559\n      0.01003\n      0.04612\n      0.01203\n      0.04361\n      0.00251\n      0.00050\n      0.00000\n    \n    \n      1\n      vrl\n      0\n      100220\n      1474\n      Bohle iridovirus\n      0.02714\n      0.01357\n      0.00068\n      0.00678\n      0.00407\n      ...\n      0.00136\n      0.01696\n      0.03596\n      0.01221\n      0.04545\n      0.01560\n      0.04410\n      0.00271\n      0.00068\n      0.00000\n    \n    \n      2\n      vrl\n      0\n      100755\n      4862\n      Sweet potato leaf curl virus\n      0.01974\n      0.0218\n      0.01357\n      0.01543\n      0.00782\n      ...\n      0.00596\n      0.01974\n      0.02489\n      0.03126\n      0.02036\n      0.02242\n      0.02468\n      0.00391\n      0.00000\n      0.00144\n    \n    \n      3\n      vrl\n      0\n      100880\n      1915\n      Northern cereal mosaic virus\n      0.01775\n      0.02245\n      0.01619\n      0.00992\n      0.01567\n      ...\n      0.00366\n      0.01410\n      0.01671\n      0.03760\n      0.01932\n      0.03029\n      0.03446\n      0.00261\n      0.00157\n      0.00000\n    \n    \n      4\n      vrl\n      0\n      100887\n      22831\n      Soil-borne cereal mosaic virus\n      0.02816\n      0.01371\n      0.00767\n      0.03679\n      0.01380\n      ...\n      0.00604\n      0.01494\n      0.01734\n      0.04148\n      0.02483\n      0.03359\n      0.03679\n      0.00000\n      0.00044\n      0.00131\n    \n    \n      5\n      vrl\n      0\n      101029\n      5274\n      Human adenovirus type 7d\n      0.02579\n      0.02218\n      0.01479\n      0.01024\n      0.02294\n      ...\n      0.00303\n      0.01593\n      0.00171\n      0.02427\n      0.02503\n      0.02825\n      0.01270\n      0.00133\n      0.00038\n      0.00209\n    \n    \n      6\n      vrl\n      0\n      101688\n      3042\n      Apple latent spherical virus\n      0.04635\n      0.01545\n      0.02005\n      0.02400\n      0.02761\n      ...\n      0.00329\n      0.01315\n      0.00822\n      0.04011\n      0.01183\n      0.02663\n      0.02663\n      0.00033\n      0.00033\n      0.00000\n    \n    \n      7\n      vrl\n      0\n      101764\n      2801\n      Aconitum latent virus\n      0.02285\n      0.02678\n      0.01214\n      0.02321\n      0.01714\n      ...\n      0.00678\n      0.01250\n      0.01107\n      0.03534\n      0.01571\n      0.03642\n      0.02785\n      0.00107\n      0.00036\n      0.00071\n    \n    \n      8\n      vrl\n      0\n      101947\n      2897\n      Pseudorabies virus Ea\n      0.01105\n      0.02106\n      0.00035\n      0.00104\n      0.00035\n      ...\n      0.02658\n      0.00207\n      0.00311\n      0.00414\n      0.04556\n      0.00449\n      0.04867\n      0.00138\n      0.00035\n      0.00138\n    \n    \n      9\n      vrl\n      0\n      10249\n      61247\n      Vaccinia virus Copenhagen\n      0.03411\n      0.0143\n      0.02771\n      0.01869\n      0.01148\n      ...\n      0.00167\n      0.02230\n      0.00411\n      0.04866\n      0.01559\n      0.03695\n      0.01412\n      0.00250\n      0.00077\n      0.00103\n    \n  \n\n10 rows × 69 columns\n\n\n\n\n\nChecking data types\n\ndfcodon.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13028 entries, 0 to 13027\nData columns (total 69 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Kingdom      13028 non-null  object \n 1   DNAtype      13028 non-null  int64  \n 2   SpeciesID    13028 non-null  int64  \n 3   Ncodons      13028 non-null  int64  \n 4   SpeciesName  13028 non-null  object \n 5   UUU          13028 non-null  object \n 6   UUC          13028 non-null  object \n 7   UUA          13028 non-null  float64\n 8   UUG          13028 non-null  float64\n 9   CUU          13028 non-null  float64\n 10  CUC          13028 non-null  float64\n 11  CUA          13028 non-null  float64\n 12  CUG          13028 non-null  float64\n 13  AUU          13028 non-null  float64\n 14  AUC          13028 non-null  float64\n 15  AUA          13028 non-null  float64\n 16  AUG          13028 non-null  float64\n 17  GUU          13028 non-null  float64\n 18  GUC          13028 non-null  float64\n 19  GUA          13028 non-null  float64\n 20  GUG          13028 non-null  float64\n 21  GCU          13028 non-null  float64\n 22  GCC          13028 non-null  float64\n 23  GCA          13028 non-null  float64\n 24  GCG          13028 non-null  float64\n 25  CCU          13028 non-null  float64\n 26  CCC          13028 non-null  float64\n 27  CCA          13028 non-null  float64\n 28  CCG          13028 non-null  float64\n 29  UGG          13028 non-null  float64\n 30  GGU          13028 non-null  float64\n 31  GGC          13028 non-null  float64\n 32  GGA          13028 non-null  float64\n 33  GGG          13028 non-null  float64\n 34  UCU          13028 non-null  float64\n 35  UCC          13028 non-null  float64\n 36  UCA          13028 non-null  float64\n 37  UCG          13028 non-null  float64\n 38  AGU          13028 non-null  float64\n 39  AGC          13028 non-null  float64\n 40  ACU          13028 non-null  float64\n 41  ACC          13028 non-null  float64\n 42  ACA          13028 non-null  float64\n 43  ACG          13028 non-null  float64\n 44  UAU          13028 non-null  float64\n 45  UAC          13028 non-null  float64\n 46  CAA          13028 non-null  float64\n 47  CAG          13028 non-null  float64\n 48  AAU          13028 non-null  float64\n 49  AAC          13028 non-null  float64\n 50  UGU          13028 non-null  float64\n 51  UGC          13028 non-null  float64\n 52  CAU          13028 non-null  float64\n 53  CAC          13028 non-null  float64\n 54  AAA          13028 non-null  float64\n 55  AAG          13028 non-null  float64\n 56  CGU          13028 non-null  float64\n 57  CGC          13028 non-null  float64\n 58  CGA          13028 non-null  float64\n 59  CGG          13028 non-null  float64\n 60  AGA          13028 non-null  float64\n 61  AGG          13028 non-null  float64\n 62  GAU          13028 non-null  float64\n 63  GAC          13028 non-null  float64\n 64  GAA          13028 non-null  float64\n 65  GAG          13028 non-null  float64\n 66  UAA          13028 non-null  float64\n 67  UAG          13028 non-null  float64\n 68  UGA          13028 non-null  float64\ndtypes: float64(62), int64(3), object(4)\nmemory usage: 6.9+ MB\n\n\nLooking at the data types there appears to be two inconsistancies in columns UUU and UUC. Since they are the codon columns and the contents of the cells in these columns are floats, the data type should reflect this. I will change the datatypes for these columns using the astype() function.\n\nfor col in dfcodon.columns[5:]:\n    try:\n        dfcodon[col] = dfcodon[col].astype(float)\n    except Exception as e:\n        print(e)\n\ncould not convert string to float: 'non-B hepatitis virus'\ncould not convert string to float: '-'\n\n\nAccording to the error message there is a string type in a couple of the column rows, this makes sense now as to why the dtype was object instead of a float type.\n\n# creating a separate dataframe to further investigate the issue of the object dtype\ndfjustcodon1 = dfcodon.drop(\n    ['Kingdom', 'DNAtype', 'SpeciesID', 'Ncodons', 'SpeciesName'], axis=1).copy()\n\n\ndef char_finder(data_frame, series_name):\n    '''\n    Function taken from the following link: https://towardsdatascience.com/data-cleaning-automatically-removing-bad-data-c4274c21e299\n\n    '''\n    cnt = 0\n    print(series_name)\n    for row in data_frame[series_name]:\n        try:\n            float(row)  # changed to float to not flag NaNs or decimals.\n            pass\n        except ValueError:\n            print(data_frame.loc[cnt, series_name], \"-> at row:\"+str(cnt))\n        cnt += 1\n\n\nfor col in dfjustcodon1:\n    char_finder(dfjustcodon1, col)\n\nUUU\nnon-B hepatitis virus -> at row:486\n12;I -> at row:5063\nUUC\n- -> at row:5063\nUUA\nUUG\nCUU\nCUC\nCUA\nCUG\nAUU\nAUC\nAUA\nAUG\nGUU\nGUC\nGUA\nGUG\nGCU\nGCC\nGCA\nGCG\nCCU\nCCC\nCCA\nCCG\nUGG\nGGU\nGGC\nGGA\nGGG\nUCU\nUCC\nUCA\nUCG\nAGU\nAGC\nACU\nACC\nACA\nACG\nUAU\nUAC\nCAA\nCAG\nAAU\nAAC\nUGU\nUGC\nCAU\nCAC\nAAA\nAAG\nCGU\nCGC\nCGA\nCGG\nAGA\nAGG\nGAU\nGAC\nGAA\nGAG\nUAA\nUAG\nUGA\n\n\nThe char_finder function has located the rows that contain the strings, I will further look at these columns and see what is going on.\n\n# row 486\ndfcodon.loc[486].head(10)\n\nKingdom                          vrl\nDNAtype                            0\nSpeciesID                      12440\nNcodons                         1238\nSpeciesName                    Non-A\nUUU            non-B hepatitis virus\nUUC                          0.04362\nUUA                            0.021\nUUG                          0.01292\nCUU                          0.01292\nName: 486, dtype: object\n\n\n\n# row 5063\ndfcodon.loc[5063].head(10)\n\nKingdom                                                  bct\nDNAtype                                                    0\nSpeciesID                                             353569\nNcodons                                                 1698\nSpeciesName    Salmonella enterica subsp. enterica serovar 4\nUUU                                                     12;I\nUUC                                                        -\nUUA                                                   0.0212\nUUG                                                  0.02356\nCUU                                                  0.01178\nName: 5063, dtype: object\n\n\nSo now I can see the issue here lies in the csv file, as it is comma seperated this has caused the name ‘Non-A, non-B hepatitis virus’ as it was recorded in the CUTG repository to offset the entire row. this issue will have to be dealt with in the CSV file by addding quotation marks around the name. The CSV file will be reuploaded into a new dataframe.\nThis should then resolve the issue with the codon columns that were object type.\n\n# reloading the fixed dataframe\ndfcodon1 = pd.read_csv('../data/codon_usage_fixed.csv')\n\n\ndfcodon1.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13028 entries, 0 to 13027\nData columns (total 69 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Kingdom      13028 non-null  object \n 1   DNAtype      13028 non-null  int64  \n 2   SpeciesID    13028 non-null  int64  \n 3   Ncodons      13028 non-null  int64  \n 4   SpeciesName  13028 non-null  object \n 5   UUU          13028 non-null  float64\n 6   UUC          13028 non-null  float64\n 7   UUA          13028 non-null  float64\n 8   UUG          13028 non-null  float64\n 9   CUU          13028 non-null  float64\n 10  CUC          13028 non-null  float64\n 11  CUA          13028 non-null  float64\n 12  CUG          13028 non-null  float64\n 13  AUU          13028 non-null  float64\n 14  AUC          13028 non-null  float64\n 15  AUA          13028 non-null  float64\n 16  AUG          13028 non-null  float64\n 17  GUU          13028 non-null  float64\n 18  GUC          13028 non-null  float64\n 19  GUA          13028 non-null  float64\n 20  GUG          13028 non-null  float64\n 21  GCU          13028 non-null  float64\n 22  GCC          13028 non-null  float64\n 23  GCA          13028 non-null  float64\n 24  GCG          13028 non-null  float64\n 25  CCU          13028 non-null  float64\n 26  CCC          13028 non-null  float64\n 27  CCA          13028 non-null  float64\n 28  CCG          13028 non-null  float64\n 29  UGG          13028 non-null  float64\n 30  GGU          13028 non-null  float64\n 31  GGC          13028 non-null  float64\n 32  GGA          13028 non-null  float64\n 33  GGG          13028 non-null  float64\n 34  UCU          13028 non-null  float64\n 35  UCC          13028 non-null  float64\n 36  UCA          13028 non-null  float64\n 37  UCG          13028 non-null  float64\n 38  AGU          13028 non-null  float64\n 39  AGC          13028 non-null  float64\n 40  ACU          13028 non-null  float64\n 41  ACC          13028 non-null  float64\n 42  ACA          13028 non-null  float64\n 43  ACG          13028 non-null  float64\n 44  UAU          13028 non-null  float64\n 45  UAC          13028 non-null  float64\n 46  CAA          13028 non-null  float64\n 47  CAG          13028 non-null  float64\n 48  AAU          13028 non-null  float64\n 49  AAC          13028 non-null  float64\n 50  UGU          13028 non-null  float64\n 51  UGC          13028 non-null  float64\n 52  CAU          13028 non-null  float64\n 53  CAC          13028 non-null  float64\n 54  AAA          13028 non-null  float64\n 55  AAG          13028 non-null  float64\n 56  CGU          13028 non-null  float64\n 57  CGC          13028 non-null  float64\n 58  CGA          13028 non-null  float64\n 59  CGG          13028 non-null  float64\n 60  AGA          13028 non-null  float64\n 61  AGG          13028 non-null  float64\n 62  GAU          13028 non-null  float64\n 63  GAC          13028 non-null  float64\n 64  GAA          13028 non-null  float64\n 65  GAG          13028 non-null  float64\n 66  UAA          13028 non-null  float64\n 67  UAG          13027 non-null  float64\n 68  UGA          13026 non-null  float64\ndtypes: float64(64), int64(3), object(2)\nmemory usage: 6.9+ MB\n\n\nUUU and UUC have now been fixed and are of dtype float.\n\n\n\nChecking for duplicates\n\ndfcodon1.duplicated().sum()\n\n0\n\n\nThere are no duplicated values in the dataframe.\n\n\n\nChecking for null values\n\npd.set_option('display.max_rows', 1000)\n\n\ndfcodon1.isnull().sum().sort_values(ascending=False)\n\nUGA            2\nUAG            1\nACG            0\nAAC            0\nAAU            0\nCAG            0\nCAA            0\nUAC            0\nUAU            0\nACA            0\nUGC            0\nACC            0\nACU            0\nAGC            0\nAGU            0\nUCG            0\nUGU            0\nCAU            0\nUCC            0\nCAC            0\nAAA            0\nAAG            0\nCGU            0\nCGC            0\nCGA            0\nCGG            0\nAGA            0\nAGG            0\nGAU            0\nGAC            0\nGAA            0\nGAG            0\nUAA            0\nUCA            0\nKingdom        0\nDNAtype        0\nCUU            0\nAUA            0\nAUC            0\nAUU            0\nCUG            0\nCUA            0\nCUC            0\nUUG            0\nGGG            0\nUUA            0\nUUC            0\nUUU            0\nSpeciesName    0\nNcodons        0\nSpeciesID      0\nAUG            0\nGUU            0\nGUC            0\nGUA            0\nGUG            0\nGCU            0\nGCC            0\nGCA            0\nGCG            0\nCCU            0\nCCC            0\nCCA            0\nCCG            0\nUGG            0\nGGU            0\nGGC            0\nGGA            0\nUCU            0\ndtype: int64\n\n\nThere are a few missing values, located in columns UGA and UAG.\n\ndfcodon1[dfcodon1['UAG'].isna()]\n\n\n\n\n\n  \n    \n      \n      Kingdom\n      DNAtype\n      SpeciesID\n      Ncodons\n      SpeciesName\n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      5063\n      bct\n      0\n      353569\n      1698\n      Salmonella enterica subsp. enterica serovar 4 ...\n      0.0212\n      0.02356\n      0.01178\n      0.01296\n      0.0106\n      ...\n      0.00707\n      0.00118\n      0.0\n      0.02945\n      0.02356\n      0.04476\n      0.02473\n      0.00118\n      NaN\n      NaN\n    \n  \n\n1 rows × 69 columns\n\n\n\n\ndfcodon1[dfcodon1['UGA'].isna()]\n\n\n\n\n\n  \n    \n      \n      Kingdom\n      DNAtype\n      SpeciesID\n      Ncodons\n      SpeciesName\n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      486\n      vrl\n      0\n      12440\n      1238\n      Non-A non-B hepatitis virus\n      0.04362\n      0.02100\n      0.01292\n      0.01292\n      0.03554\n      ...\n      0.00323\n      0.00242\n      0.00162\n      0.04443\n      0.01696\n      0.02423\n      0.02262\n      0.00162\n      0.0\n      NaN\n    \n    \n      5063\n      bct\n      0\n      353569\n      1698\n      Salmonella enterica subsp. enterica serovar 4 ...\n      0.02120\n      0.02356\n      0.01178\n      0.01296\n      0.01060\n      ...\n      0.00707\n      0.00118\n      0.00000\n      0.02945\n      0.02356\n      0.04476\n      0.02473\n      0.00118\n      NaN\n      NaN\n    \n  \n\n2 rows × 69 columns\n\n\n\nIt is easy to replace these values. I will take them from the CUTG database using the species name to find the appropriate value.\n\nFor UGA missing value in row 486, corresponding to species name Non-A non-B hepatitis virus the value is 0.0024\nFor UAG missing value in row 5063, corresponding to species name Salmonella enterica subsp. enterica serovar 4,12;I,- [gbbct]: 2 the value is 0.0\nFor UGA missing value in row 5063, corresponding to species name Salmonella enterica subsp. enterica serovar 4,12;I,- [gbbct]: 2 the value is 0.0\n\nI will fill in the missing data with these values.\n\n# filling in the missing values with the values I found from CUTG:\n\ndfcodon1.loc[486, 'UGA'] = 0.0024\ndfcodon1.loc[5063, 'UAG'] = 0.0\ndfcodon1.loc[5063, 'UGA'] = 0.0\n\n\n# Checking the missing values have been replaced:\ndfcodon1.isna().sum()\n\nKingdom        0\nDNAtype        0\nSpeciesID      0\nNcodons        0\nSpeciesName    0\nUUU            0\nUUC            0\nUUA            0\nUUG            0\nCUU            0\nCUC            0\nCUA            0\nCUG            0\nAUU            0\nAUC            0\nAUA            0\nAUG            0\nGUU            0\nGUC            0\nGUA            0\nGUG            0\nGCU            0\nGCC            0\nGCA            0\nGCG            0\nCCU            0\nCCC            0\nCCA            0\nCCG            0\nUGG            0\nGGU            0\nGGC            0\nGGA            0\nGGG            0\nUCU            0\nUCC            0\nUCA            0\nUCG            0\nAGU            0\nAGC            0\nACU            0\nACC            0\nACA            0\nACG            0\nUAU            0\nUAC            0\nCAA            0\nCAG            0\nAAU            0\nAAC            0\nUGU            0\nUGC            0\nCAU            0\nCAC            0\nAAA            0\nAAG            0\nCGU            0\nCGC            0\nCGA            0\nCGG            0\nAGA            0\nAGG            0\nGAU            0\nGAC            0\nGAA            0\nGAG            0\nUAA            0\nUAG            0\nUGA            0\ndtype: int64\n\n\n\n\n\nRenaming the Kingdom Classes:\n\ndfcodon1['Kingdom'].replace({'vrl': 'virus',\n                             'arc': 'archaea',\n                             'bct': 'bacteria',\n                             'phg': 'phage',\n                             'plm': 'plasmid',\n                             'pln': 'plant',\n                             'inv': 'invertebrate',\n                             'vrt': 'vertebrate',\n                             'mam': 'mammal',\n                             'rod': 'rodent',\n                             'pri': 'primate'\n                             }, inplace=True)\n\ndfcodon1['Kingdom']\n\n0          virus\n1          virus\n2          virus\n3          virus\n4          virus\n          ...   \n13023    primate\n13024    primate\n13025    primate\n13026    primate\n13027    primate\nName: Kingdom, Length: 13028, dtype: object\n\n\n\ndfcodon1['DNAtype'].replace({0: 'nuclear',\n                             1: 'mitochondrial',\n                             2: 'chloroplast',\n                             }, inplace=True)\n\ndfcodon1['DNAtype']\n\n0              nuclear\n1              nuclear\n2              nuclear\n3              nuclear\n4              nuclear\n             ...      \n13023          nuclear\n13024    mitochondrial\n13025    mitochondrial\n13026          nuclear\n13027    mitochondrial\nName: DNAtype, Length: 13028, dtype: object\n\n\n\n\n\nSaving Work\nThe Dataframe has been cleaned and missing values replaced. I will save this to a new dataframe to use in the following EDA section.\n\nimport joblib\n# Save data as pickle file to data folder\njoblib.dump(dfcodon1, '../data/dfcodon1.pkl')\n\n['../data/dfcodon1.pkl']"
  },
  {
    "objectID": "posts/03-Exploratory-Data-Analysis.html",
    "href": "posts/03-Exploratory-Data-Analysis.html",
    "title": "Jasmine Marzouk",
    "section": "",
    "text": "In order to explore the data set and understand what it contains I will be aiming to answer the following questions:\n\nWhat is the distribution of the target classes for Kingdom?\nWhat is the distribution of the target classes for DNAtype?\nWhat does the distribution for the Ncodons column look like?\nAre there codons that have a higher occurance in certain DNA types?\nAre there codons that have a higher occurence in certain Kingdoms?\n\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import (GridSearchCV, cross_val_score,\n                                     train_test_split)\nfrom sklearn.metrics import (accuracy_score, auc, classification_report,\n                             confusion_matrix, f1_score, plot_confusion_matrix,\n                             plot_roc_curve, precision_score, recall_score,\n                             roc_auc_score, roc_curve)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nimport warnings\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom scipy import stats\n\n%matplotlib inline\n\n\n# warnings.filterwarnings(\"ignore\")\nnp.random.seed(123)\n\n\n# importing the cleaned dataframe dfcodon1 and renaming it to codon\n\ncodon = joblib.load('../data/dfcodon1.pkl')\n\n\n# Dataframe Shape\nprint(\n    f'The dataframe contains {codon.shape[0]} rows and {codon.shape[1]} columns')\n\nThe dataframe contains 13028 rows and 69 columns\n\n\n\n# peeking at the dataframe\ncodon.sample(5)\n\n\n\n\n\n  \n    \n      \n      Kingdom\n      DNAtype\n      SpeciesID\n      Ncodons\n      SpeciesName\n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      11544\n      vertebrate\n      1\n      71258\n      4460\n      mitochondrion Hypopomus occidentalis\n      0.01951\n      0.02623\n      0.02354\n      0.01076\n      0.03274\n      ...\n      0.00740\n      0.00000\n      0.00000\n      0.00090\n      0.00269\n      0.01726\n      0.00359\n      0.00404\n      0.00314\n      0.03139\n    \n    \n      12258\n      mammal\n      0\n      30521\n      14077\n      Bos grunniens\n      0.01698\n      0.01954\n      0.00789\n      0.01435\n      0.01279\n      ...\n      0.00902\n      0.01179\n      0.00980\n      0.02579\n      0.02941\n      0.02998\n      0.03467\n      0.00107\n      0.00078\n      0.00199\n    \n    \n      2177\n      virus\n      0\n      394337\n      1359\n      Tomato leaf curl New Delhi virus-[Multan;Luffa]\n      0.02134\n      0.02428\n      0.00736\n      0.01177\n      0.01545\n      ...\n      0.00736\n      0.01472\n      0.01325\n      0.02943\n      0.01840\n      0.02796\n      0.01545\n      0.00221\n      0.00147\n      0.00221\n    \n    \n      11615\n      vertebrate\n      0\n      7792\n      11800\n      Heterodontus francisci\n      0.01661\n      0.01720\n      0.00712\n      0.01025\n      0.00941\n      ...\n      0.00890\n      0.01517\n      0.01025\n      0.02017\n      0.02339\n      0.02881\n      0.02661\n      0.00068\n      0.00042\n      0.00144\n    \n    \n      3172\n      bacteria\n      0\n      320122\n      13771\n      Clostridium phage phi CD119\n      0.03544\n      0.00588\n      0.04480\n      0.01089\n      0.01343\n      ...\n      0.00029\n      0.02926\n      0.00508\n      0.04647\n      0.01017\n      0.06739\n      0.01808\n      0.00334\n      0.00182\n      0.00058\n    \n  \n\n5 rows × 69 columns\n\n\n\n\n# Describing the dataframe\ncodon.describe()\n\n\n\n\n\n  \n    \n      \n      DNAtype\n      SpeciesID\n      Ncodons\n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      CUC\n      CUA\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      count\n      13028.000000\n      13028.000000\n      1.302800e+04\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      ...\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n    \n    \n      mean\n      0.367209\n      130451.105926\n      7.960576e+04\n      0.024819\n      0.023440\n      0.020635\n      0.014104\n      0.017822\n      0.018286\n      0.019043\n      ...\n      0.005452\n      0.009929\n      0.006422\n      0.024183\n      0.021164\n      0.028292\n      0.021683\n      0.001640\n      0.000590\n      0.006178\n    \n    \n      std\n      0.688726\n      124787.086107\n      7.197010e+05\n      0.017627\n      0.011597\n      0.020709\n      0.009279\n      0.010587\n      0.014572\n      0.024251\n      ...\n      0.006601\n      0.008574\n      0.006387\n      0.013826\n      0.013038\n      0.014343\n      0.015018\n      0.001785\n      0.000882\n      0.010344\n    \n    \n      min\n      0.000000\n      7.000000\n      1.000000e+03\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.000000\n      28850.750000\n      1.602000e+03\n      0.013910\n      0.015380\n      0.005610\n      0.007108\n      0.010890\n      0.007830\n      0.005300\n      ...\n      0.001220\n      0.001690\n      0.001168\n      0.012390\n      0.011860\n      0.017360\n      0.009710\n      0.000560\n      0.000000\n      0.000410\n    \n    \n      50%\n      0.000000\n      81971.500000\n      2.927500e+03\n      0.021750\n      0.021905\n      0.015260\n      0.013360\n      0.016130\n      0.014560\n      0.009680\n      ...\n      0.003530\n      0.009270\n      0.004540\n      0.025435\n      0.019070\n      0.026085\n      0.020540\n      0.001380\n      0.000420\n      0.001130\n    \n    \n      75%\n      1.000000\n      222891.250000\n      9.120000e+03\n      0.031310\n      0.029210\n      0.029485\n      0.019803\n      0.022730\n      0.025110\n      0.017245\n      ...\n      0.007150\n      0.015922\n      0.010250\n      0.034190\n      0.027690\n      0.036800\n      0.031122\n      0.002370\n      0.000830\n      0.002890\n    \n    \n      max\n      12.000000\n      465364.000000\n      4.066258e+07\n      0.217300\n      0.091690\n      0.151330\n      0.101190\n      0.089780\n      0.100350\n      0.163920\n      ...\n      0.055540\n      0.098830\n      0.058430\n      0.185660\n      0.113840\n      0.144890\n      0.158550\n      0.045200\n      0.025610\n      0.106700\n    \n  \n\n8 rows × 67 columns\n\n\n\n\n# Setting a theme for all the visualisations\nsns.set_theme(style=\"darkgrid\")\nsns.set(rc={'figure.figsize': (11.7, 8.27)})\n\n\nPlan for EDA: - Univariate alalysis - Bivariate analysis - Check codon distribution for each Kingdom - Check codon distribution for each DNAtype\n\n\n\nDue to the dataframe containing 69 columns, this will be hard to visualise without breaking up the dataframe into focus points. So, I will isolate the codon columns into a separate dataframe and the rest of the columns into another one, and then do univariate analysis.\n\nSeparating the codon only columns\n\n\njustcodon = codon.drop(\n    columns=['Kingdom', 'DNAtype', 'SpeciesID', 'Ncodons', 'SpeciesName'], axis=1)\njustcodon.sample(3)\n\n\n\n\n\n  \n    \n      \n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      CUC\n      CUA\n      CUG\n      AUU\n      AUC\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      1830\n      0.02869\n      0.01967\n      0.01742\n      0.01527\n      0.02859\n      0.01312\n      0.01199\n      0.00932\n      0.02664\n      0.01178\n      ...\n      0.00113\n      0.01445\n      0.00451\n      0.03043\n      0.02244\n      0.02613\n      0.02152\n      0.00092\n      0.00000\n      0.00020\n    \n    \n      2592\n      0.00948\n      0.02212\n      0.00316\n      0.01422\n      0.00790\n      0.01106\n      0.00790\n      0.02291\n      0.01106\n      0.02212\n      ...\n      0.00316\n      0.01422\n      0.00790\n      0.00711\n      0.02765\n      0.01896\n      0.02291\n      0.00079\n      0.00000\n      0.00000\n    \n    \n      1004\n      0.01659\n      0.02461\n      0.01495\n      0.00981\n      0.01207\n      0.01433\n      0.01130\n      0.00940\n      0.01644\n      0.02250\n      ...\n      0.00817\n      0.01521\n      0.01433\n      0.02666\n      0.02214\n      0.02764\n      0.01741\n      0.00319\n      0.00108\n      0.00057\n    \n  \n\n3 rows × 64 columns\n\n\n\n\nSeparating the non codon into dataframe\n\n\nnon_codon = codon.drop(columns=codon.columns[5:], axis=1)\nnon_codon.head(3)\n\n\n\n\n\n  \n    \n      \n      Kingdom\n      DNAtype\n      SpeciesID\n      Ncodons\n      SpeciesName\n    \n  \n  \n    \n      0\n      virus\n      0\n      100217\n      1995\n      Epizootic haematopoietic necrosis virus\n    \n    \n      1\n      virus\n      0\n      100220\n      1474\n      Bohle iridovirus\n    \n    \n      2\n      virus\n      0\n      100755\n      4862\n      Sweet potato leaf curl virus\n    \n  \n\n\n\n\n\n\n\nWhat is the distribution of the target classes for Kingdom?\nWhat is the distribution of the target classes for DNAtype?\n\nI will not be looking at SepciesID or SpeciesName as these columns won’t have a distribution that offers any noteworthy insights.\n\nfor col in non_codon[['Kingdom', 'DNAtype', 'Ncodons']]:\n    plt.figure()\n    plt.title(f'Feature: {col}')\n    plt.hist(codon[col], bins=20)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nKingdom shows the counts of the different kingdom, with bacterial and viral being the ones with the most data points in this dataset.\nDNAtype has 12 unique values, however there only seems to show 0, 1, and 2. I will have to apply a log to this graph to see the counts of the other dna types and they could be muted by the high counts from the top three.\nNcodons another distribution that is heavily skewed, I will visualise this one with a log transformation.\n\n\n# Looking at the Kingdom column distribution using log transformation:\nnon_codon['Kingdom'].value_counts().plot(kind='bar', log=True)\nplt.title('Kingdom Column Distribution')\nplt.xlabel('Kingdoms')\nplt.ylabel('Log Counts')\n\n# Proportions of the Kingdom counts:\nnon_codon['Kingdom'].value_counts(normalize=True)*100\n\nbacteria        22.413264\nvirus           21.737796\nplant           19.365981\nvertebrate      15.942585\ninvertebrate    10.323918\nmammal           4.390543\nphage            1.688671\nrodent           1.650292\nprimate          1.381640\narchaea          0.967148\nplasmid          0.138164\nName: Kingdom, dtype: float64\n\n\n\n\n\nThe dataset does contain rows for kingdoms beside the top three, however there is a high imbalance here. with some of the classes having less than 5%. I will drop plm since it is <2% (18 entries).\n\n# Looking at DNAtype column distribution with log transformation:\nnon_codon['DNAtype'].value_counts().plot(kind='bar', log=True)\nplt.title('DNAtype column distribution')\nplt.xlabel('DNA Type')\nplt.ylabel('Log Counts')\n\nprint('Proportions of DNAtype counts')\n# Proportions of the DNAtype counts:\nnon_codon['DNAtype'].value_counts(normalize=True)*100\n\nProportions of DNAtype counts\n\n\n0     71.131409\n1     22.252072\n2      6.263433\n4      0.237949\n12     0.038379\n3      0.015352\n9      0.015352\n5      0.015352\n11     0.015352\n6      0.007676\n7      0.007676\nName: DNAtype, dtype: float64\n\n\n\n\n\n\nThoughts:\n\nThe columns Kingdoms and DNAtype will be the targets (separately) that I will be aiming to classify. Looking at the distributions, and the imbalances between the classes I will aim to drop some classes and the following is my reasoning.\nIn terms of kingdoms, the Dataset contains the following: - pri: primate - rod: rodent - mam: mammalian - vrt: vertebrate - inv: invertebrate - pln: plant - bct: bacteria - vrl: virus - phg :bacteriophage - arc: archaea - plm: plasmid\nprimate ,rodent, and mammalian can be grouped and renamed under the vertebrate kingdom, because they are vertebrates and are all mammals - vrt = vrt + pri + rod + mam\nThe column will then have the following classes: - vrt: vertebrate - inv: invertebrate - pln: plant - bct: bacteria - vrl: virus - phg :bacteriophage - arc: archaea - plm: plasmid\nI will further disregard the rows that are class plm, as there are far too little entries (18 only).\nFinal classes will be:\n\nvrt: vertebrate\ninv: invertebrate\npln: plant\nbct: bacteria\nvrl: virus\nphg :bacteriophage\narc: archaea\n\nFinally I will look at the distribution after these changes.\nFor the column DNAtype, it contains the following classes: - 0: nuclear - 1: mitochondrion - 2: chloroplast - 3: cyanelle - 4: plastid - 5: nucleomorph - 6: secondary endosymbiont - 7: chromoplast - 8: leukoplast - 9: NA - 10: proplastid - 11: apicoplast - 12: kinetoplast\nAs seen in the DNAtype distribution, there is a high imbalance between these classes. I will focus here on the top three which are 0 (nuclear), 1 (mitochonrion), and 2(chloroplast). These three DNA types that are present in all of the kingdoms in our dataset, so I will go ahead with these as they are representative. I will drop all rows that do not contain 0, 1, or 2, as the are less than 1% of the dataset.\n\nKingdom Column\n\n\n# changing the value names as stated above:\nnon_codon = non_codon.replace(['primate', 'rodend', 'mammal'], 'vertebrate')\n\n# dropping rows with value plm\nnon_codon = non_codon.drop(\n    non_codon.loc[non_codon['Kingdom'] == 'plasmid'].index)\n\n\nnon_codon['Kingdom'].value_counts().plot(\n    kind='bar', log=True)  # no log applied\nplt.title('Kingdom Column Distribution')\nplt.xlabel('Kingdoms')\nplt.ylabel('Log Counts')\n\nText(0, 0.5, 'Log Counts')\n\n\n\n\n\n\nnon_codon['Kingdom'].value_counts()\n\nbacteria        2920\nvirus           2832\nvertebrate      2829\nplant           2523\ninvertebrate    1345\nphage            220\nrodent           215\narchaea          126\nName: Kingdom, dtype: int64\n\n\nAs there are still imbalances in the data, this makes sense since genome lengths differ between species, so this imbalance is inevitable. For example for archea there are only 209 species,\n\nNote: these changes have currently been made to the dataframe non_codon I will now apply these changes to the main dataframe codon\n\n\n# changing the value names as stated above:\ncodon = codon.replace(['primate', 'rodent', 'mammal'], 'vertebrate')\n\n# dropping rows with value plm\ncodon = codon.drop(codon.loc[codon['Kingdom'] == 'plasmid'].index)\n\n\nDNAtype Column\n\n\n# Dropping all rows that do not contain 0, 1, or 2 as DNAtype:\nnon_codon = non_codon.drop(non_codon.loc[non_codon['DNAtype'] > 2].index)\n\n\n# Checking distribution:\nnon_codon['DNAtype'].value_counts().plot(\n    kind='bar', log=True)  # log transformation not applied\nplt.title('DNAtype column distribution')\nplt.xlabel('DNA Type')\nplt.ylabel('Log Counts')\n\nText(0, 0.5, 'Log Counts')\n\n\n\n\n\nThe classes are highly imbalanced, but this can be adjusted later on during the modeling section.\nAgain, I will apply these changes to the original codon dataframe for consistency.\n\n# Dropping rows in DNAtype with values larger than 2:\ncodon = codon.drop(codon.loc[codon['DNAtype'] > 2].index)\n\n\n# Checking distribution in codon dataframe:\ncodon['DNAtype'].value_counts().plot(kind='bar')\n\n<AxesSubplot:>\n\n\n\n\n\n\nWhat does the distribution for the Ncodons column look like?\n\n\n# checking log transformed distribution\nplt.figure()\nplt.hist(non_codon['Ncodons'], bins=100)\nplt.show()\n\nprint('===============================================')\nnon_codon['Ncodons'] = np.log(non_codon['Ncodons']+1)\nnon_codon['Ncodons']\n\n\n\n\n===============================================\n\n\n0         7.598900\n1         7.296413\n2         8.489411\n3         7.557995\n4        10.035918\n           ...    \n13023     7.001246\n13024     7.634337\n13025     7.430707\n13026    17.520819\n13027    16.012624\nName: Ncodons, Length: 12964, dtype: float64\n\n\nNcodons is highly skewed, this is because there are more species with smaller genomes and so smaller number of codons in the sequence.\nApplying the log transformation change to the original dataframe codon\n\ncodon['Ncodons'] = np.log(codon['Ncodons']+1)\ncodon['Ncodons']\n\n0         7.598900\n1         7.296413\n2         8.489411\n3         7.557995\n4        10.035918\n           ...    \n13023     7.001246\n13024     7.634337\n13025     7.430707\n13026    17.520819\n13027    16.012624\nName: Ncodons, Length: 12964, dtype: float64\n\n\nFinally, the dataframe shape after transformation.\n\n# the dataframe containing only the non_codon columns\nprint(\n    f'The non_codon dataframe contains {non_codon.shape[0]} rows and {non_codon.shape[1]}')\n# the original codon dataframe\nprint(\n    f'The codon dataframe contains {codon.shape[0]} rows and {codon.shape[1]}')\n\nThe non_codon dataframe contains 12964 rows and 5\nThe codon dataframe contains 12964 rows and 69\n\n\n\n\n\n\n\njustcodon.describe()\n\n\n\n\n\n  \n    \n      \n      UUU\n      UUC\n      UUA\n      UUG\n      CUU\n      CUC\n      CUA\n      CUG\n      AUU\n      AUC\n      ...\n      CGG\n      AGA\n      AGG\n      GAU\n      GAC\n      GAA\n      GAG\n      UAA\n      UAG\n      UGA\n    \n  \n  \n    \n      count\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      ...\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n      13028.000000\n    \n    \n      mean\n      0.024819\n      0.023440\n      0.020635\n      0.014104\n      0.017822\n      0.018286\n      0.019043\n      0.018455\n      0.028354\n      0.025036\n      ...\n      0.005452\n      0.009929\n      0.006422\n      0.024183\n      0.021164\n      0.028292\n      0.021683\n      0.001640\n      0.000590\n      0.006178\n    \n    \n      std\n      0.017627\n      0.011597\n      0.020709\n      0.009279\n      0.010587\n      0.014572\n      0.024251\n      0.016583\n      0.017505\n      0.014595\n      ...\n      0.006601\n      0.008574\n      0.006387\n      0.013826\n      0.013038\n      0.014343\n      0.015018\n      0.001785\n      0.000882\n      0.010344\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.013910\n      0.015380\n      0.005610\n      0.007108\n      0.010890\n      0.007830\n      0.005300\n      0.007180\n      0.016370\n      0.015130\n      ...\n      0.001220\n      0.001690\n      0.001168\n      0.012390\n      0.011860\n      0.017360\n      0.009710\n      0.000560\n      0.000000\n      0.000410\n    \n    \n      50%\n      0.021750\n      0.021905\n      0.015260\n      0.013360\n      0.016130\n      0.014560\n      0.009680\n      0.012800\n      0.025480\n      0.021540\n      ...\n      0.003530\n      0.009270\n      0.004540\n      0.025435\n      0.019070\n      0.026085\n      0.020540\n      0.001380\n      0.000420\n      0.001130\n    \n    \n      75%\n      0.031310\n      0.029210\n      0.029485\n      0.019803\n      0.022730\n      0.025110\n      0.017245\n      0.024330\n      0.038113\n      0.031860\n      ...\n      0.007150\n      0.015922\n      0.010250\n      0.034190\n      0.027690\n      0.036800\n      0.031122\n      0.002370\n      0.000830\n      0.002890\n    \n    \n      max\n      0.217300\n      0.091690\n      0.151330\n      0.101190\n      0.089780\n      0.100350\n      0.163920\n      0.107370\n      0.154060\n      0.088600\n      ...\n      0.055540\n      0.098830\n      0.058430\n      0.185660\n      0.113840\n      0.144890\n      0.158550\n      0.045200\n      0.025610\n      0.106700\n    \n  \n\n8 rows × 64 columns\n\n\n\nThe justcodon dataframe created from the original codon dataframe contains all the codon columns which are 64 columns in total, each column representing one of the 64 codons.\nThe values in these columns are frequencies of the codon occurrence in each organism’s genome (from nuclear, mitochondrial, or chloroplast RNA)\nThe frequencies are caculated by taking a count of the total number of codons in the sequenced genome, counting the individual codon occurance, and then dividing this number by the total number of codons.\nfor example : for Enterobacteria phage P1, the total number of codons is 71879, and the number of UGU occurances is 362, so the codon frequency is 0.00503.\n\n\n\n\nplt.figure(figsize=(30, 10))\nsns.boxplot(data=justcodon, palette='Blues')\nplt.title('Codon Distributions')\nplt.axhline(y=justcodon.median().mean(), color='r')\nplt.xlabel('Codons')\nplt.show()\n\n\n\n\nA few observations - The stop codons UAA, UAG UGA have the lowest occurance, this makes sense since stop codons are what tells the translation machinery when to stop translation, so they will not be as frequent in the RNA/DNA sequence as the rest of the codons.\n\nThe start codon AUG occurance is quite similar accross all species as the distribution is tending towards the median.\n\nUUU UUC:\n\nThese codons encode for the amino acid Phenylalanine. As this amino acid only has two codons that can encode for it, the distributions have a similar median, but UUU has slightly more recorded occurances among the different species.\n\nAUU AUC AUA:\n\nThese three codons encode for the amino acid Isoleucine. From the distributions AUU and AUC has higher occurances in the species. So they tend to be the ones found most among species to encode for the amino acid, with AUU having a higher median so it is even more favoured over AUC.\n\nACU ACC ACA ACG\n\nThese encode for the amino acid Theronine. The distributions show that ACC and ACA have a wider range of occurances among the species while ACU codon appears to be the higher occuring one for this amino acid.\n\nCAA CAG\n\nThese codons encode for the amino acid Glutamine. From the distributions it is apparent that CAA is more frequent in its occurance among species.\n\nAAU AAC\n\nThese codons encode for the amino acid Asparagine. Seeing that this amino acid only has two type of codons that can encode it, the distributions show a similar median for both however AAC tends to be favoured.\n\nThese frequencies are highly dependent on the size of the sequence, DNA length that was sequenced. Therefore, it is important to view their distributions in relation to either DNA type or Kingdom.\n\n\n\n\n\n\n\n\nAre there codons that have a higher occurence in certain Kingdoms?\n\n\nclustergraph_kingdom = codon.drop(\n    columns=['DNAtype', 'SpeciesID', 'Ncodons', 'SpeciesName'], axis=1)\nclustergraph_kingdom.set_index('Kingdom', inplace=True)\n\n\nfig = px.imshow(clustergraph_kingdom.groupby(level='Kingdom').mean(\n),  color_continuous_scale='Cividis', origin='lower')  # or RdBu_r\nfig.update_layout()\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nFrom the heatmap it appears that CUA has a high occurance in the vertebrate kingdom. furthermore the Aspartic Acid (GAU and GAC) and Glutamic Acid (GAA and GAG) codons seem to have high occurances in all of the kingdoms but not as much in the vertebrate kingdom in comparison with the rest.\nCAG has a higher occurance for archea kingdom.\nCUA is highly favoured in vertebrates, this codon encodes for Leucine.\n\n\n\n\nAre there codons that have a higher occurance in certain DNA types?\n\n\nclustergraph_dna = codon.drop(\n    columns=['Kingdom', 'SpeciesID', 'Ncodons', 'SpeciesName'], axis=1)\nclustergraph_dna.set_index('DNAtype', inplace=True)\n\n\nfig = px.imshow(clustergraph_dna.groupby(level='DNAtype').mean(),\n                color_continuous_scale='Cividis', origin='lower')  # or RdBu_r\nfig.update_layout()\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nIn the heatmap for DNAtype codon frequencies, there are some codons that have frequencies. It appears that for DNAtype 1 which is mitochondria, there is a higher frequency of CUA which codes for the amino acid Leucine. While for Nuclear DNA there is almost an avoidance of the CUA codon. Moreover, in terms of the stop codons, it appears that mitochondrial DNA (1) seems to favour UGA stop codon over the over two.\nAnother noteworthy pattern, mitochondrial DNA shows obvious order of favouring for the Theronine amino acid encoding codons; ACG, ACA, ACC, and ACU. There is a higher occurance of ACA for this amino acid.\nNuclear DNA (0) seems to have a different pattern, in terms of frequency there are obvious codons that are more occurent for amino acids, however it isn’t as pronounced."
  }
]